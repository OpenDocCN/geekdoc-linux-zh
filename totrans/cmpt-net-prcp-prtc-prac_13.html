<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Sharing resources#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Sharing resources#</h1>
<blockquote>原文：<a href="https://4ed.computer-networking.info/syllabus/default/networks/sharing.html">https://4ed.computer-networking.info/syllabus/default/networks/sharing.html</a></blockquote>
<span id="index-0"/>
<p>A network is designed to support a potentially large number of users that exchange information with each other. These users produce and consume information which is exchanged through the network. To support its users, a network uses several types of resources. It is important to keep in mind the different resources that are shared inside the network.</p>
<p>The first and more important resource inside a network is the link bandwidth. There are two situations where link bandwidth needs to be shared between different users. The first situation is when several hosts are attached to the same physical link. This situation mainly occurs in Local Area Networks (LAN). A LAN is a network that efficiently interconnects several hosts (usually a few dozens to a few hundreds) in the same room, building or campus. Consider for example a network with five hosts. Any of these hosts needs to be able to exchange information with any of the other hosts. A first organization for this LAN is the full-mesh.</p>
<blockquote>
<div><div class="figure" id="id76" style="text-align: center"><p><img src="../Images/4ef12a85b0fb083d06f7f0e6539c8f58.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-7460b6f809ff9496cd4775752cf8ae5c788a688b.png"/></p>
<p><span class="caption-number">Fig. 187 </span><span class="caption-text">A full-mesh network</span></p>
</div></div></blockquote>
<p>The full-mesh is the most reliable and highest performing network to interconnect these five hosts. However, this network organization has two important drawbacks. First, if a network contains <cite>n</cite> hosts, then <span class="math notranslate nohighlight">\(\frac{n\times(n-1)}{2}\)</span> links are required. If the network contains more than a few hosts, it becomes impossible to lay down the required physical links. Second, if the network contains <cite>n</cite> hosts, then each host must have <span class="math notranslate nohighlight">\(n-1\)</span> interfaces to terminate <span class="math notranslate nohighlight">\(n-1\)</span> links. This is beyond the capabilities of most hosts. Furthermore, if a new host is added to the network, new links have to be laid down and one interface has to be added to each participating host. However, full-mesh has the advantage of providing the lowest delay between the hosts and the best resiliency against link failures. In practice, full-mesh networks are rarely used except when there are few network nodes and resiliency is key.</p>
<p>The second possible physical organization, which is also used inside computers to connect different extension cards, is the bus. In a bus network, all hosts are attached to a shared medium, usually a cable through a single interface. When one host sends an electrical signal on the bus, the signal is received by all hosts attached to the bus. A drawback of bus-based networks is that if the bus is physically cut, then the network is split into two isolated networks.  For this reason, bus-based networks are sometimes considered to be difficult to operate and maintain, especially when the cable is long and there are many places where it can break. Such a bus-based topology was used in early Ethernet networks.</p>
<blockquote>
<div><div class="figure" id="id77" style="text-align: center"><p><img src="../Images/3a69ecd6632515f93fca7a7a4e043988.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-fd53ee25cc3a189af4b91d435c88362566eeb604.png"/></p>
<p><span class="caption-number">Fig. 188 </span><span class="caption-text">A network organized as a bus</span></p>
</div></div></blockquote>
<p>A third organization of a computer network is a star topology. In such networks, hosts have a single physical interface and there is one physical link between each host and the center of the star. The node at the center of the star can be either a piece of equipment that amplifies an electrical signal, or an active device, such as a piece of equipment that understands the format of the messages exchanged through the network. Of course, the failure of the central node implies the failure of the network. However, if one physical link fails (e.g. because the cable has been cut), then only one node is disconnected from the network. In practice, star-shaped networks are easier to operate and maintain than bus-shaped networks. Many network administrators also appreciate the fact that they can control the network from a central point. Administered from a Web interface, or through a console-like connection, the center of the star is a useful point of control (enabling or disabling devices) and an excellent observation point (usage statistics).</p>
<blockquote>
<div><div class="figure" id="id78" style="text-align: center"><p><img src="../Images/7716555a16a5fea52c47b90cf0272f73.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-ee2eccac1658d2f738e770e8778cf8d7f410101a.png"/></p>
<p><span class="caption-number">Fig. 189 </span><span class="caption-text">A network organized as a star</span></p>
</div></div></blockquote>
<p>A fourth physical organization of a network is the ring topology. Like the bus organization, each host has a single physical interface connecting it to the ring. Any signal sent by a host on the ring will be received by all hosts attached to the ring. From a redundancy point of view, a single ring is not the best solution, as the signal only travels in one direction on the ring; thus if one of the links composing the ring is cut, the entire network fails. In practice, such rings have been used in local area networks, but are now often replaced by star-shaped networks. In metropolitan networks, rings are often used to interconnect multiple locations. In this case, two parallel links, composed of different cables, are often used for redundancy. With such a dual ring, when one ring fails all the traffic can be quickly switched to the other ring.</p>
<blockquote>
<div><div class="figure" id="id79" style="text-align: center"><p><img src="../Images/02439986e682d8116eb77f12b88154ac.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-0e062914d5fb5866aa9a05190d3aee03f3992498.png"/></p>
<p><span class="caption-number">Fig. 190 </span><span class="caption-text">A network organized as a ring</span></p>
</div></div></blockquote>
<p>A fifth physical organization of a network is the tree. Such networks are typically used when a large number of customers must be connected in a very cost-effective manner. Cable TV networks are often organized as trees.</p>
<blockquote>
<div><div class="figure" id="id80" style="text-align: center"><p><img src="../Images/1ef1c6253ef9a140ed3a946c46df4dee.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-40d080f9bc48fd186b3ff0e71e916506a651bcde.png"/></p>
<p><span class="caption-number">Fig. 191 </span><span class="caption-text">A network organized as a tree</span></p>
</div></div></blockquote>
<section id="sharing-bandwidth">
<h2>Sharing bandwidth<a class="headerlink" href="#sharing-bandwidth" title="Link to this heading">#</a></h2>
<p>In all these networks, except the full-mesh, the link bandwidth is shared among all connected hosts. Various algorithms have been proposed and are used to efficiently share the access to this resource. We explain several of them in the Medium Access Control section below.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Fairness in computer networks</p>
<p>Sharing resources is important to ensure that the network efficiently serves its user. In practice, there are many ways to share resources. Some resource sharing schemes consider that some users are more important than others and should obtain more resources. For example, on the roads, police cars and ambulances have priority. In some cities, traffic lanes are reserved for buses to promote public services, … In computer networks, the same problem arise. Given that resources are limited, the network needs to enable users to efficiently share them. Before designing an efficient resource sharing scheme, one needs to first formalize its objectives. In computer networks, the most popular objective for resource sharing schemes is that they must be <cite>fair</cite>. In a simple situation, for example two hosts using a shared 2 Mbps link, the sharing scheme should allocate the same bandwidth to each user, in this case 1 Mbps. However, in a large networks, simply dividing the available resources by the number of users is not sufficient. Consider the network shown in the figure below where <cite>A1</cite> sends data to <cite>A2</cite>, <cite>B1</cite> to <cite>B2</cite>, … In this network, how should we divide the bandwidth among the different flows ? A first approach would be to allocate the same bandwidth to each flow. In this case, each flow would obtain 5 Mbps and the link between <cite>R2</cite> and <cite>R3</cite> would not be fully loaded. Another approach would be to allocate 10 Mbps to <cite>A1-A2</cite>, 20 Mbps to <cite>C1-C2</cite> and nothing to <cite>B1-B2</cite>. This is clearly unfair.</p>
<blockquote>
<div><div class="figure" id="id81" style="text-align: center"><p><img src="../Images/a92629ffc43cb0912e0548fff34a5038.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-5a1986707352b87a193f895c2ce30216e6ca221c.png"/></p>
<p><span class="caption-number">Fig. 192 </span><span class="caption-text">A small network</span></p>
</div></div></blockquote>
<p id="index-1">In large networks, fairness is always a compromise. The most widely used definition of fairness is the <cite>max-min fairness</cite>. A bandwidth allocation in a network is said to be <cite>max-min fair</cite> if it is such that it is impossible to allocate more bandwidth to one of the flows without reducing the bandwidth of a flow that already has a smaller allocation than the flow that we want to increase. If the network is completely known, it is possible to derive a <cite>max-min fair</cite> allocation as follows. Initially, all flows have a null bandwidth and they are placed in the candidate set. The bandwidth allocation of all flows in the candidate set is increased until one link becomes congested. At this point, the flows that use the congested link have reached their maximum allocation. They are removed from the candidate set and the process continues until the candidate set becomes empty.</p>
<p>In the above network, the allocation of all flows would grow until <cite>A1-A2</cite> and <cite>B1-B2</cite> reach 5 Mbps. At this point, link <cite>R1-R2</cite> becomes congested and these two flows have reached their maximum. The allocation for flow <cite>C1-C2</cite> can increase until reaching 15 Mbps. At this point, link <cite>R2-R3</cite> is congested. To increase the bandwidth allocated to <cite>C1-C2</cite>, one would need to reduce the allocation to flow <cite>B1-B2</cite>. Similarly, the only way to increase the allocation to flow <cite>B1-B2</cite> would require a decrease of the allocation to <cite>A1-A2</cite>.</p>
</div>
</section>
<section id="network-congestion">
<h2>Network congestion<a class="headerlink" href="#network-congestion" title="Link to this heading">#</a></h2>
<p>Sharing bandwidth among the hosts directly attached to a link is not the only sharing problem that occurs in computer networks. To understand the general problem, let us consider a very simple network which contains only point-to-point links. This network contains three hosts and two routers. All the links inside the network have the same capacity. For example, let us assume that all links have a bandwidth of 1000 bits per second and that the hosts send packets containing exactly one thousand bits.</p>
<blockquote>
<div><div class="figure" id="id82" style="text-align: center"><p><img src="../Images/f79da50c17973252bf5368a3df95f186.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-85b7450b921769000c252aef39a6e4c58c692cd7.png"/></p>
<p><span class="caption-number">Fig. 193 </span><span class="caption-text">A small network</span></p>
</div></div></blockquote>
<p>In the network above, consider the case where host <cite>A</cite> is transmitting packets to destination <cite>C</cite>. <cite>A</cite> can send one packet per second and its packets will be delivered to <cite>C</cite>. Now, let us explore what happens when host <cite>B</cite> also starts to transmit a packet. Node <cite>R1</cite> will receive two packets that must be forwarded to <cite>R2</cite>. Unfortunately, due to the limited bandwidth on the <cite>R1-R2</cite> link, only one of these two packets can be transmitted. The outcome of the second packet will depend on the available buffers on <cite>R1</cite>. If <cite>R1</cite> has one available buffer, it could store the packet that has not been transmitted on the <cite>R1-R2</cite> link until the link becomes available. If <cite>R1</cite> does not have available buffers, then the packet needs to be discarded.</p>
<p id="index-2">Besides the link bandwidth, the buffers on the network nodes are the second type of resource that needs to be shared inside the network. The node buffers play an important role in the operation of the network because that can be used to absorb transient traffic peaks. Consider again the example above. Assume that on average host <cite>A</cite> and host <cite>B</cite> send a group of three packets every ten seconds. Their combined transmission rate (0.6 packets per second) is, on average, lower than the network capacity (1 packet per second). However, if they both start to transmit at the same time, node <cite>R1</cite> will have to absorb a burst of packets. This burst of packets is a small <cite>network congestion</cite>. We will say that a network is congested, when the sum of the traffic demand from the hosts is larger than the network capacity <span class="math notranslate nohighlight">\(\sum{demand}&gt;capacity\)</span>. This <cite>network congestion</cite> problem is one of the most difficult resource sharing problem in computer networks. <cite>Congestion</cite> occurs in almost all networks. Minimizing the amount of congestion is a key objective for many network operators. In most cases, they will have to accept transient congestion, i.e. congestion lasting a few seconds or perhaps minutes, but will want to prevent congestion that lasts days or months. For this, they can rely on a wide range of solutions. We briefly present some of these in the paragraphs below.</p>
<p id="index-3">If <cite>R1</cite> has enough buffers, it will be able to absorb the load without having to discard packets. The packets sent by hosts <cite>A</cite> and <cite>B</cite> will reach their final destination <cite>C</cite>, but will experience a longer delay than when they are transmitting alone. The amount of buffering on the network node is the first parameter that a network operator can tune to control congestion inside his network. Given the decreasing cost of memory, one could be tempted to put as many buffers <a class="footnote-reference brackets" href="#fbufferbloat" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> as possible on the network nodes. Let us consider this case in the network above and assume that <cite>R1</cite> has infinite buffers. Assume now that hosts <cite>A</cite> and <cite>B</cite> try to transmit a file that corresponds to one thousand packets each. Both are using a reliable protocol that relies on go-back-n to recover from transmission errors. The transmission starts and packets start to accumulate in <cite>R1</cite>’s buffers. The presence of these packets in the buffers increases the delay between the transmission of a packet by <cite>A</cite> and the return of the corresponding acknowledgment. Given the increasing delay, host <cite>A</cite> (and <cite>B</cite> as well) will consider that some of the packets that it sent have been lost. These packets will be retransmitted and will enter the buffers of <cite>R1</cite>. The occupancy of the buffers of <cite>R1</cite> will continue to increase and the delays as well. This will cause new retransmissions, … In the end, only one file will be delivered (very slowly) to the destination, but the link <cite>R1-R2</cite> will transfer much more bytes than the size of the file due to the multiple copies of the same packets. This is known as the <cite>congestion collapse</cite> problem <span class="target" id="index-4"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc896.html"><strong>RFC 896</strong></a>. Congestion collapse is the nightmare for network operators. When it happens, the network carries packets without delivering useful data to the end users.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Congestion collapse on the Internet</p>
<p>Congestion collapse is unfortunately not only an academic experience. Van Jacobson reports in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id2"><span>[Jacobson1988]</span></a> one of these events that affected him while he was working at the Lawrence Berkeley Laboratory (LBL). LBL was two network nodes away from the University of California in Berkeley. At that time, the link between the two sites had a bandwidth of 32 Kbps, but some hosts were already attached to 10 Mbps LANs. “In October 1986,  the data throughput from LBL to UC Berkeley … dropped from 32 Kbps to 40 bps. We were fascinated by this sudden factor-of-thousand drop in bandwidth and embarked on an investigation of why things had gotten so bad.” This work lead to the development of various congestion control techniques that have allowed the Internet to continue to grow without experiencing widespread congestion collapse events.</p>
</div>
<p>Besides bandwidth and memory, a third resource that needs to be shared inside a network is the (packet) processing capacity. To forward a packet, a router needs bandwidth on the outgoing link, but it also needs to analyze the packet header to perform a lookup inside its forwarding table. Performing these lookup operations require resources such as CPU cycles or memory accesses. Routers are usually designed to be able to sustain a given packet processing rate, measured in packets per second <a class="footnote-reference brackets" href="#fpps" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Packets per second versus bits per second</p>
<p>The performance of network nodes (either routers or switches) can be characterized by two key metrics :</p>
<blockquote>
<div><ul class="simple">
<li><p>the node’s capacity measured in bits per second</p></li>
<li><p>the node’s lookup performance measured in packets per second</p></li>
</ul>
</div></blockquote>
<p>The node’s capacity in bits per second mainly depends on the physical interfaces that it uses and also on the capacity of the internal interconnection (bus, crossbar switch, …) between the different interfaces inside the node. Many vendors, in particular for low-end devices will use the sum of the bandwidth of the nodes’ interfaces as the node capacity in bits per second. Measurements do not always match this maximum theoretical capacity. A well designed network node will usually have a capacity in bits per second larger than the sum of its link capacities. Such nodes will usually reach this maximum capacity when forwarding large packets.</p>
<p>When a network node forwards small packets, its performance is usually limited by the number of lookup operations that it can perform every second. This lookup performance is measured in packets per second. The performance may depend on the length of the forwarded packets. The key performance factor is the number of minimal size packets that are forwarded by the node every second. This rate can lead to a capacity in bits per second which is much lower than the sum of the bandwidth of the node’s links.</p>
</div>
<p id="index-5">Let us now try to present a broad overview of the congestion problem in networks. We will assume that the network is composed of dedicated links having a fixed bandwidth <a class="footnote-reference brackets" href="#fadjust" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>. A network contains hosts that generate and receive packets and nodes (routers and switches) that forward packets. Assuming that each host is connected via a single link to the network, the largest demand is <span class="math notranslate nohighlight">\(\sum{Access Links}\)</span>. In practice, this largest demand is never reached and the network will be engineered to sustain a much lower traffic demand. The difference between the worst-case traffic demand and the sustainable traffic demand can be large, up to several orders of magnitude. Fortunately, the hosts are not completely dumb and they can adapt their traffic demand to the current state of the network and the available bandwidth. For this, the hosts need to <cite>sense</cite> the current level of congestion and adjust their own traffic demand based on the estimated congestion. Network nodes can react in different ways to network congestion and hosts can sense the level of congestion in different ways.</p>
<p>Let us first explore which mechanisms can be used inside a network to control congestion and how these mechanisms can influence the behavior of the end hosts.</p>
<p>As explained earlier, one of the first manifestation of congestion on network nodes is the saturation of the network links that leads to a growth in the occupancy of the buffers of the node. This growth of the buffer occupancy implies that some packets will spend more time in the buffer and thus in the network. If hosts measure the network delays (e.g. by measuring the round-trip-time between the transmission of a packet and the return of the corresponding acknowledgment) they could start to sense congestion. On low bandwidth links, a growth in the buffer occupancy can lead to an increase of the delays which can be easily measured by the end hosts. On high bandwidth links, a few packets inside the buffer will cause a small variation in the delay which may not necessarily be larger that the natural fluctuations of the delay measurements.</p>
<p>If the buffer’s occupancy continues to grow, it will overflow and packets will need to be discarded. Discarding packets during congestion is the second possible reaction of a network node to congestion. Before looking at how a node can discard packets, it is interesting to discuss qualitatively the impact of the buffer occupancy on the reliable delivery of data through a network. This is illustrated by figure <a class="reference internal" href="#fig-congestion-jain"><span class="std std-numref">Fig. 194</span></a>, adapted from <a class="reference internal" href="../bibliography.html#jain1990" id="id5"><span>[Jain1990]</span></a>.</p>
<figure class="align-center" id="fig-congestion-jain">
<img alt="../_images/jain.png" src="../Images/b9e0fdb8cefccc00202ebacd9b171691.png" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/jain.png"/>
<figcaption>
<p><span class="caption-number">Fig. 194 </span><span class="caption-text">Network congestion</span><a class="headerlink" href="#fig-congestion-jain" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>When the network load is low, buffer occupancy and link utilization are low. The buffers on the network nodes are mainly used to absorb very short bursts of packets, but on average the traffic demand is lower than the network capacity. If the demand increases, the average buffer occupancy will increase as well. Measurements have shown that the total throughput increases as well. If the buffer occupancy is zero or very low, transmission opportunities on network links can be missed. This is not the case when the buffer occupancy is small but non zero. However, if the buffer occupancy continues to increase, the buffer becomes overloaded and the throughput does not increase anymore. When the buffer occupancy is close to the maximum, the throughput may decrease. This drop in throughput can be caused by excessive retransmissions of reliable protocols that incorrectly assume that previously sent packets have been lost while they are still waiting in the buffer. The network delay on the other hand increases with the buffer occupancy. In practice, a good operating point for a network buffer is a low occupancy to achieve high link utilization and also low delay for interactive applications.</p>
<p id="index-6">Discarding packets is one of the signals that the network nodes can use to inform the hosts of the current level of congestion. Buffers on network nodes are usually used as FIFO queues to preserve packet ordering. Several <cite>packet discard mechanisms</cite> have been proposed for network nodes. These techniques basically answer two different questions :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>What triggers a packet to be discarded ?</cite> What are the conditions that lead a network node to decide to discard a packet? The simplest answer to this question is : <cite>When the buffer is full</cite>. Although this is a good congestion indication, it is probably not the best one from a performance viewpoint. An alternative is to discard packets when the buffer occupancy grows too much. In this case, it is likely that the buffer will become full shortly. Since packet discarding is an information that allows hosts to adapt their transmission rate, discarding packets early could allow hosts to react earlier and thus prevent congestion from happening.</p></li>
<li><p><cite>Which packet(s) should be discarded ?</cite> Once the network node has decided to discard packets, it needs to actually discard real packets.</p></li>
</ul>
</div></blockquote>
<p>By combining different answers to these questions, network researchers have developed different packet discard mechanisms.</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>Tail drop</cite> is the simplest packet discard technique. When a buffer is full, the arriving packet is discarded. <cite>Tail drop</cite> can be easily implemented. This is, by far, the most widely used packet discard mechanism. However, it suffers from two important drawbacks. First, since <cite>tail drop</cite> discards packets only when the buffer is full, buffers tend to be congested and real-time applications may suffer from increased delays. Second, <cite>tail drop</cite> is blind when it discards a packet. It may discard a packet from a low bandwidth interactive flow while most of the buffer is used by large file transfers.</p></li>
<li><p><cite>Drop from front</cite> is an alternative packet discard technique. Instead of removing the arriving packet, it removes the packet that was at the head of the queue. Discarding this packet instead of the arriving one can have two advantages. First, it already stayed a long time in the buffer. Second, hosts should be able to detect the loss (and thus the congestion) earlier.</p></li>
<li><p><cite>Probabilistic drop</cite>. Various random drop techniques have been proposed. A frequently cited technique is <cite>Random Early Discard</cite> (RED) <a class="reference internal" href="../bibliography.html#fj1993" id="id6"><span>[FJ1993]</span></a>. RED measures the average buffer occupancy and discards packets with a given probability when this average occupancy is too high. Compared to <cite>tail drop</cite> and <cite>drop from front</cite>, an advantage of <cite>RED</cite> is that thanks to the probabilistic drops, packets should be discarded from different flows in proportion of their bandwidth.</p></li>
</ul>
</div></blockquote>
<p>Discarding packets is a frequent reaction to network congestion. Unfortunately, discarding packets is not optimal since a packet which is discarded on a network node has already consumed resources on the upstream nodes. There are other ways for the network to inform the end hosts of the current congestion level. A first solution is to mark the packets when a node is congested. Several networking technologies have relied on this kind of packet marking.</p>
<p id="index-7">In datagram networks, <cite>Forward Explicit Congestion Notification</cite> (FECN) can be used. One field of the packet header, typically one bit, is used to indicate congestion. When a host sends a packet, the congestion bit is unset. If the packet passes through a congested node, the congestion bit is set. The destination can then determine the current congestion level by measuring the fraction of the packets that it received with the congestion bit set. It may then return this information to the sending host to allow it to adapt its retransmission rate. Compared to packet discarding, the main advantage of FECN is that hosts can detect congestion explicitly without having to rely on packet losses.</p>
<p>In virtual circuit networks, packet marking can be improved if the return packets follow the reverse path of the forward packets. It this case, a network node can detect congestion on the forward path (e.g. due to the size of its buffer), but mark the packets on the return path. Marking the return packets (e.g. the acknowledgments used by reliable protocols) provides a faster feedback to the sending hosts compared to FECN. This technique is usually called <cite>Backward Explicit Congestion Notification (BECN)</cite>.</p>
<p>If the packet header does not contain any bit in the header to represent the current congestion level, an alternative is to allow the network nodes to send a control packet to the source to indicate the current congestion level. Some networking technologies use such control packets to explicitly regulate the transmission rate of sources. However, their usage is mainly restricted to small networks. In large networks, network nodes usually avoid using such control packets. These control packets are even considered to be dangerous in some networks. First, using them increases the network load when the network is congested. Second, while network nodes are optimized to forward packets, they are usually pretty slow at creating new packets.</p>
<p id="index-8">Dropping and marking packets is not the only possible reaction of a router that becomes congested. A router could also selectively delay packets belonging to some flows. There are different algorithms that can be used by a router to delay packets. If the objective of the router is to fairly distribute to bandwidth of an output link among competing flows, one possibility is to organize the buffers of the router as a set of queues. For simplicity, let us assume that the router is capable of supporting a fixed number of concurrent flows, say <cite>N</cite>. One of the queues of the router is associated to each flow and when a packet arrives, it is placed at the tail of the corresponding queue. All the queues are controlled by a <cite>scheduler</cite>. A <cite>scheduler</cite> is an algorithm that is run each time there is an opportunity to transmit a packet on the outgoing link. Various schedulers have been proposed in the scientific literature and some are used in real routers.</p>
<blockquote>
<div><div class="figure" id="id83" style="text-align: center"><p><img src="../Images/e6916b3dd773fa746d317a93e0a077db.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-1ab27e7b575b28dcf1d5296681e54c7f6463c119.png"/></p>
<p><span class="caption-number">Fig. 195 </span><span class="caption-text">A round-robin scheduler, where N = 5</span></p>
</div></div></blockquote>
<p>A very simple scheduler is the <cite>round-robin scheduler</cite>. This scheduler serves all the queues in a round-robin fashion. If all flows send packets of the same size, then the round-robin scheduler fairly allocates the bandwidth among the different flows. Otherwise, it favors flows that are using larger packets. Extensions to the <cite>round-robin scheduler</cite> have been proposed to provide a fair distribution of the bandwidth with variable-length packets <a class="reference internal" href="../bibliography.html#sv1995" id="id7"><span>[SV1995]</span></a> but these are outside the scope of this chapter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># N queues</span>
<span class="c1"># state variable : next_queue</span>
<span class="n">next_queue</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">isEmpty</span><span class="p">(</span><span class="n">buffer</span><span class="p">):</span>
        <span class="c1"># Wait for next packet in buffer</span>
        <span class="n">wait</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isEmpty</span><span class="p">(</span><span class="n">queue</span><span class="p">[</span><span class="n">next_queue</span><span class="p">])):</span>
        <span class="c1"># Send packet at head of next_queue</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">remove_packet</span><span class="p">(</span><span class="n">queue</span><span class="p">[</span><span class="n">next_queue</span><span class="p">])</span>
        <span class="n">send</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">next_queue</span><span class="o">=</span><span class="p">(</span><span class="n">next_queue</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">N</span>
<span class="c1"># end while</span>
</pre></div>
</div>
</section>
<section id="distributing-the-load-across-the-network">
<h2>Distributing the load across the network<a class="headerlink" href="#distributing-the-load-across-the-network" title="Link to this heading">#</a></h2>
<p>Delays, packet discards, packet markings and control packets are the main types of information that the network can exchange with the end hosts. Discarding packets is the main action that a network node can perform if the congestion is too severe. Besides tackling congestion at each node, it is also possible to divert some traffic flows from heavily loaded links to reduce congestion. Early routing algorithms <a class="reference internal" href="../bibliography.html#mrr1980" id="id8"><span>[MRR1980]</span></a> have used delay measurements to detect congestion between network nodes and update the link weights dynamically. By reflecting the delay perceived by applications in the link weights used for the shortest paths computation, these routing algorithms managed to dynamically change the forwarding paths in reaction to congestion. However, deployment experience showed that these dynamic routing algorithms could cause oscillations and did not necessarily lower congestion. Deployed datagram networks rarely use dynamic routing algorithms, except in some wireless networks. In datagram networks, the state of the art reaction to long term congestion, i.e. congestion lasting hours, days or more, is to measure the traffic demand and then select the link weights <a class="reference internal" href="../bibliography.html#frt2002" id="id9"><span>[FRT2002]</span></a> that allow minimizing the maximum link loads. If the congestion lasts longer, changing the weights is not sufficient anymore and the network needs to be upgraded with additional or faster links. However, in Wide Area Networks, adding new links can take months.</p>
<p>In virtual circuit networks, another way to manage or prevent congestion is to limit the number of circuits that use the network at any time. This technique is usually called <cite>connection admission control</cite>. When a host requests the creation of a new circuit in the network, it specifies the destination and in some networking technologies the required bandwidth. With this information, the network can check whether there are enough resources available to reach this particular destination. If yes, the circuit is established. If not, the request is denied and the host will have to defer the creation of its virtual circuit. <cite>Connection admission control</cite> schemes are widely used in the telephone networks. In these networks, a busy tone corresponds to an unavailable destination or a congested network.</p>
<p>In datagram networks, this technique cannot be easily used since the basic assumption of such a network is that a host can send any packet towards any destination at any time. A host does not need to request the authorization of the network to send packets towards a particular destination.</p>
<p>Based on the feedback received from the network, the hosts can adjust their transmission rate. We discuss in section <cite>Congestion control</cite> some techniques that allow hosts to react to congestion.</p>
<p>Another way to share the network resources is to distribute the load across multiple links. Many techniques have been designed to spread the load over the network. As an illustration, let us briefly consider how the load can be shared when accessing some content. Consider a large and popular file such as the image of a Linux distribution or the upgrade of a commercial operating system that will be downloaded by many users. There are many ways to distribute this large file. A naive solution is to place one copy of the file on a server and allow all users to download this file from the server. If the file is popular and millions of users want to download it, the server will quickly become overloaded. There are two classes of solutions that can be used to serve a large number of users. A first approach is to store the file on servers whose name is known by the clients. Before retrieving the file, each client will query the name service to obtain the address of the server. If the file is available from many servers, the name service can provide different addresses to different clients. This will automatically spread the load since different clients will download the file from different servers. Most large content providers use such a solution to distribute large files or videos.</p>
<p>There is another solution that allows spreading the load among many sources without relying on the name service. The popular <a class="reference external" href="https://www.bittorrent.com">bittorent</a> service
is an example of this approach. With this solution, each file is divided in blocks of fixed size. To retrieve a file, a client needs to retrieve all the blocks that compose the file. However, nothing forces the client to retrieve all the blocks in sequence and from the same server. Each file is associated with metadata that indicates for each block a list of addresses of hosts that store this block. To retrieve a complete file, a client first downloads the metadata. Then, it tries to retrieve each block from one of the hosts that store the block. In practice, implementations often try to download several blocks in parallel. Once one block has been successfully downloaded, the next block can be requested. If a host is slow to provide one block or becomes unavailable, the client can contact another host listed in the metadata. Most deployments of bittorrent allow the clients to participate to the distribution of blocks. Once a client has downloaded one block, it contacts the server which stores the metadata to indicate that it can also provide this block. With this scheme, when a file is popular, its blocks are downloaded by many hosts that automatically participate in the distribution of the blocks. Thus, the number of <cite>servers</cite> that are capable of providing blocks from a popular file automatically increases with the file’s popularity.</p>
<p>Now that we have provided a broad overview of the techniques that can be used to spread the load and allocate resources in the network, let us analyze two techniques in more details : Medium Access Control and Congestion control.</p>
</section>
<section id="medium-access-control-algorithms">
<h2>Medium Access Control algorithms<a class="headerlink" href="#medium-access-control-algorithms" title="Link to this heading">#</a></h2>
<p id="index-9">The common problem among Local Area Networks is how to efficiently share the available bandwidth. If two devices send a frame at the same time, the two electrical, optical or radio signals that correspond to these frames will appear at the same time on the transmission medium and a receiver will not be able to decode either frame. Such simultaneous transmissions are called <cite>collisions</cite>. A <cite>collision</cite> may involve frames transmitted by two or more devices attached to the Local Area Network. Collisions are the main cause of errors in wired Local Area Networks.</p>
<p>All Local Area Network technologies rely on a <cite>Medium Access Control</cite> algorithm to regulate the transmissions to either minimize or avoid collisions. There are two broad families of <cite>Medium Access Control</cite> algorithms :</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><cite>Deterministic</cite> or <cite>pessimistic</cite> MAC algorithms. These algorithms assume that collisions are a very severe problem and that they must be completely avoided. These algorithms ensure that at any time, at most one device is allowed to send a frame on the LAN. This is usually achieved by using a distributed protocol which elects one device that is allowed to transmit at each time. A deterministic MAC algorithm ensures that no collision will happen, but there is some overhead in regulating the transmission of all the devices attached to the LAN.</p></li>
<li><p><cite>Stochastic</cite> or <cite>optimistic</cite> MAC algorithms. These algorithms assume that collisions are part of the normal operation of a Local Area Network. They aim to minimize the number of collisions, but they do not try to avoid all collisions. Stochastic algorithms are usually easier to implement than deterministic ones.</p></li>
</ol>
</div></blockquote>
<p>We first discuss a simple deterministic MAC algorithm and then we describe several important optimistic algorithms, before coming back to a distributed and deterministic MAC algorithm.</p>
<section id="static-allocation-methods">
<h3>Static allocation methods<a class="headerlink" href="#static-allocation-methods" title="Link to this heading">#</a></h3>
<p>A first solution to share the available resources among all the devices attached to one Local Area Network is to define, <cite>a priori</cite>, the distribution of the transmission resources among the different devices. If <cite>N</cite> devices need to share the transmission capacities of a LAN operating at <cite>b</cite> Mbps, each device could be allocated a bandwidth of <span class="math notranslate nohighlight">\(\frac{b}{N}\)</span> Mbps.</p>
<p id="index-10">Limited resources need to be shared in other environments than Local Area Networks. Since the first radio transmissions by <a class="reference external" href="http://en.wikipedia.org/wiki/Guglielmo_Marconi">Marconi</a> more than one century ago, many applications that exchange information through radio signals have been developed. Each radio signal is an electromagnetic wave whose power is centered around a given frequency. The radio spectrum corresponds to frequencies ranging between roughly 3 KHz and 300 GHz. Frequency allocation plans negotiated among governments reserve most frequency ranges for specific applications such as broadcast radio, broadcast television, mobile communications, aeronautical radio navigation, amateur radio, satellite, etc. Each frequency range is then subdivided into channels and each channel can be reserved for a given application, e.g. a radio broadcaster in a given region.</p>
<p id="index-11"><cite>Frequency Division Multiplexing</cite> (FDM) is a static allocation scheme in which a frequency is allocated to each device attached to the shared medium. As each device uses a different transmission frequency, collisions cannot occur. In optical networks, a variant of FDM called <cite>Wavelength Division Multiplexing</cite> (WDM) can be used. An optical fiber can transport light at different wavelengths without interference. With WDM, a different wavelength is allocated to each of the devices that share the same optical fiber.</p>
<p id="index-12"><cite>Time Division Multiplexing</cite> (TDM) is a static bandwidth allocation method that was initially defined for the telephone network. In the fixed telephone network, a voice conversation is usually transmitted as a 64 Kbps signal. Thus, a telephone conservation generates 8 KBytes per second or one byte every 125 microseconds. Telephone conversations often need to be multiplexed together on a single line. For example, in Europe, thirty 64 Kbps voice signals are multiplexed over a single 2 Mbps (E1) line. This is done by using  <cite>Time Division Multiplexing</cite> (TDM). TDM divides the transmission opportunities into slots. In the telephone network, a slot corresponds to 125 microseconds. A position inside each slot is reserved for each voice signal. The figure below illustrates TDM on a link that is used to carry four voice conversations. The vertical lines represent the slot boundaries and the letters the different voice conversations. One byte from each voice conversation is sent during each 125 microseconds slot. The byte corresponding to a given conversation is always sent at the same position in each slot.</p>
<blockquote>
<div><div class="figure" id="id84" style="text-align: center"><p><img src="../Images/a5ce7017001779fb85df0a821c9dfa33.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-d77b364fc4d8103f3e76051b25f626c2864e0059.png"/></p>
<p><span class="caption-number">Fig. 196 </span><span class="caption-text">Time-division multiplexing</span></p>
</div></div></blockquote>
<p>TDM as shown above can be completely static, i.e. the same conversations always share the link, or dynamic. In the latter case, the two endpoints of the link must exchange messages specifying which conversation uses which byte inside each slot. Thanks to these control messages, it is possible to dynamically add and remove voice conversations from a given link.</p>
<p>TDM and FDM are widely used in telephone networks to support fixed bandwidth conversations. Using them in Local Area Networks that support computers would probably be inefficient. Computers usually do not send information at a fixed rate. Instead, they often have an on-off behavior. During the on period, the computer tries to send at the highest possible rate, e.g. to transfer a file. During the off period, which is often much longer than the on period, the computer does not transmit any packet. Using a static allocation scheme for computers attached to a LAN would lead to huge inefficiencies, as they would only be able to transmit at <span class="math notranslate nohighlight">\(\frac{1}{N}\)</span> of the total bandwidth during their on period, despite the fact that the other computers are in their off period and thus do not need to transmit any information. The dynamic MAC algorithms discussed in the remainder of this chapter aim to solve this problem.</p>
</section>
<section id="aloha">
<h3>ALOHA<a class="headerlink" href="#aloha" title="Link to this heading">#</a></h3>
<p id="index-13">In the 1960s, computers were mainly mainframes with a few dozen terminals attached to them. These terminals were usually in the same building as the mainframe and were directly connected to it. In some cases, the terminals were installed in remote locations and connected through a <a class="reference internal" href="../glossary.html#term-modem"><span class="xref std std-term">modem</span></a> attached to a <a class="reference internal" href="../glossary.html#term-dial-up-line"><span class="xref std std-term">dial-up  line</span></a>. The university of Hawaii chose a different organization. Instead of using telephone lines to connect the distant terminals, they developed the first <cite>packet radio</cite> technology <a class="reference internal" href="../bibliography.html#abramson1970" id="id10"><span>[Abramson1970]</span></a>. Until then, computer networks were built on top of either the telephone network or physical cables. ALOHANet showed that it is possible to use radio signals to interconnect computers.</p>
<p id="index-14">The first version of ALOHANet, described in <a class="reference internal" href="../bibliography.html#abramson1970" id="id11"><span>[Abramson1970]</span></a>, operated as follows. First, the terminals and the mainframe exchanged fixed-length frames composed of 704 bits. Each frame contained 80 8-bit characters, some control bits and parity information to detect transmission errors. Two channels in the 400 MHz range were reserved for the operation of ALOHANet. The first channel was used by the mainframe to send frames to all terminals. The second channel was shared among all terminals to send frames to the mainframe. As all terminals share the same transmission channel, there is a risk of collision. To deal with this problem as well as transmission errors, the mainframe verified the parity bits of the received frame and sent an acknowledgment on its channel for each correctly received frame. The terminals on the other hand had to retransmit the unacknowledged frames. As for TCP, retransmitting these frames immediately upon expiration of a fixed timeout is not a good approach as several terminals may retransmit their frames at the same time leading to a network collapse. A better approach, but still far from perfect, is for each terminal to wait a random amount of time after the expiration of its retransmission timeout. This avoids synchronization among multiple retransmitting terminals.</p>
<p>The pseudo-code below shows the operation of an ALOHANet terminal. We use this python syntax for all Medium Access Control algorithms described in this chapter. The algorithm is applied to each new frame that needs to be transmitted. It attempts to transmit a frame at most <cite>max</cite> times (<cite>while loop</cite>). Each transmission attempt is performed as follows. First, the frame is sent. Each frame is protected by a timeout. Then, the terminal waits for either a valid acknowledgment frame or the expiration of its timeout. If the terminal receives an acknowledgment, the frame has been delivered correctly and the algorithm terminates. Otherwise, the terminal waits for a random time and attempts to retransmit the frame.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># ALOHA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack_on_return_channel</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ack_on_return_channel</span><span class="p">):</span>
       <span class="k">break</span>  <span class="c1"># transmission was successful</span>
    <span class="k">else</span><span class="p">:</span>
             <span class="c1"># timeout</span>
             <span class="n">wait</span><span class="p">(</span><span class="n">random_time</span><span class="p">)</span>
             <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p><a class="reference internal" href="../bibliography.html#abramson1970" id="id12"><span>[Abramson1970]</span></a> analyzed the performance of ALOHANet under particular assumptions and found that ALOHANet worked well when the channel was lightly loaded. In this case, the frames are rarely retransmitted and the <cite>channel traffic</cite>, i.e. the total number of (correct and retransmitted) frames transmitted per unit of time is close to the <cite>channel utilization</cite>, i.e. the number of correctly transmitted frames per unit of time. Unfortunately, the analysis also reveals that the <cite>channel utilization</cite> reaches its maximum at <span class="math notranslate nohighlight">\(\frac{1}{2 \times e}=0.186\)</span> times the channel bandwidth. At higher utilization, ALOHANet becomes unstable and the network collapses due to collided retransmissions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Amateur packet radio</p>
<p>Packet radio technologies have evolved in various directions since the first experiments performed at the University of Hawaii. The Amateur packet radio service developed by amateur radio operators is one of the descendants ALOHANet. Many amateur radio operators are very interested in new technologies and they often spend countless hours developing new antennas or transceivers. When the first personal computers appeared, several amateur radio operators designed radio modems and their own datalink layer protocols <a class="reference internal" href="../bibliography.html#kpd1985" id="id13"><span>[KPD1985]</span></a> <a class="reference internal" href="../bibliography.html#bnt1997" id="id14"><span>[BNT1997]</span></a>. This network grew and it was possible to connect to servers in several European countries by only using packet radio relays. Some amateur radio operators also developed TCP/IP protocol stacks that were used over the packet radio service. Some parts of the <a class="reference external" href="http://www.ampr.org/">amateur packet radio network</a> are connected to the global Internet and use the <cite>44.0.0.0/8</cite> IPv4 prefix.</p>
</div>
<p id="index-15">Many improvements to ALOHANet have been proposed since the publication of <a class="reference internal" href="../bibliography.html#abramson1970" id="id15"><span>[Abramson1970]</span></a>, and this technique, or some of its variants, are still found in wireless networks today. The slotted technique proposed in <a class="reference internal" href="../bibliography.html#roberts1975" id="id16"><span>[Roberts1975]</span></a> is important because it shows that a simple modification can significantly improve channel utilization. Instead of allowing all terminals to transmit at any time, <a class="reference internal" href="../bibliography.html#roberts1975" id="id17"><span>[Roberts1975]</span></a> proposed to divide time into slots and allow terminals to transmit only at the beginning of each slot. Each slot corresponds to the time required to transmit one fixed size frame. In practice, these slots can be imposed by a single clock that is received by all terminals. In ALOHANet, it could have been located on the central mainframe. The analysis in <a class="reference internal" href="../bibliography.html#roberts1975" id="id18"><span>[Roberts1975]</span></a> reveals that this simple modification improves the channel utilization by a factor of two.</p>
</section>
<section id="carrier-sense-multiple-access">
<span id="index-16"/><h3>Carrier Sense Multiple Access<a class="headerlink" href="#carrier-sense-multiple-access" title="Link to this heading">#</a></h3>
<p>ALOHA and slotted ALOHA can easily be implemented, but unfortunately, they can only be used in networks that are very lightly loaded. Designing a network for a very low utilization is possible, but it clearly increases the cost of the network. To overcome the problems of ALOHA, many Medium Access Control mechanisms have been proposed which improve channel utilization. Carrier Sense Multiple Access (CSMA) is a significant improvement compared to ALOHA. CSMA requires all nodes to listen to the transmission channel to verify that it is free before transmitting a frame <a class="reference internal" href="../bibliography.html#kt1975" id="id19"><span>[KT1975]</span></a>. When a node senses the channel to be busy, it defers its transmission until the channel becomes free again. The pseudo-code below provides a more detailed description of the operation of CSMA.</p>
<div class="highlight-python notranslate" id="index-17"><div class="highlight"><pre><span/><span class="c1"># persistent CSMA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel_becomes_free</span><span class="p">)</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ack</span><span class="p">:</span>
       <span class="k">break</span>  <span class="c1"># transmission was successful</span>
    <span class="k">else</span><span class="p">:</span>
             <span class="c1"># timeout</span>
             <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p>The above pseudo-code is often called <cite>persistent CSMA</cite> <a class="reference internal" href="../bibliography.html#kt1975" id="id20"><span>[KT1975]</span></a> as the terminal will continuously listen to the channel and transmit its frame as soon as the channel becomes free. Another important variant of CSMA is the <cite>non-persistent CSMA</cite> <a class="reference internal" href="../bibliography.html#kt1975" id="id21"><span>[KT1975]</span></a>. The main difference between persistent and non-persistent CSMA described in the pseudo-code below is that a non-persistent CSMA node does not continuously listen to the channel to determine when it becomes free. When a non-persistent CSMA terminal senses the transmission channel to be busy, it waits for a random time before sensing the channel again. This improves channel utilization compared to persistent CSMA. With persistent CSMA, when two terminals sense the channel to be busy, they will both transmit (and thus cause a collision) as soon as the channel becomes free. With non-persistent CSMA, this synchronization does not occur, as the terminals wait a random time after having sensed the transmission channel. However, the higher channel utilization achieved by non-persistent CSMA comes at the expense of a slightly higher waiting time in the terminals when the network is lightly loaded.</p>
<div class="highlight-python notranslate" id="index-18"><div class="highlight"><pre><span/><span class="c1"># Non persistent CSMA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">listen</span><span class="p">(</span><span class="n">channel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">free</span><span class="p">(</span><span class="n">channel</span><span class="p">):</span>
        <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">received</span><span class="p">(</span><span class="n">ack</span><span class="p">):</span>
           <span class="k">break</span>  <span class="c1"># transmission was successful</span>
        <span class="k">else</span><span class="p">:</span>
                 <span class="c1"># timeout</span>
                 <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">random_time</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p><a class="reference internal" href="../bibliography.html#kt1975" id="id22"><span>[KT1975]</span></a> analyzes in detail the performance of several CSMA variants. Under some assumptions about the transmission channel and the traffic, the analysis compares ALOHA, slotted ALOHA, persistent and non-persistent CSMA. Under these assumptions, ALOHA achieves a channel utilization of only 18.4% of the channel capacity. Slotted ALOHA is able to use 36.6% of this capacity. Persistent CSMA improves the utilization by reaching 52.9% of the capacity while non-persistent CSMA achieves 81.5% of the channel capacity.</p>
</section>
<section id="carrier-sense-multiple-access-with-collision-detection">
<span id="index-19"/><h3>Carrier Sense Multiple Access with Collision Detection<a class="headerlink" href="#carrier-sense-multiple-access-with-collision-detection" title="Link to this heading">#</a></h3>
<p id="index-20">CSMA improves channel utilization compared to ALOHA. However, the performance can still be improved, especially in wired networks. Consider the situation of two terminals that are connected to the same cable. This cable could, for example, be a coaxial cable as in the early days of Ethernet <a class="reference internal" href="../bibliography.html#metcalfe1976" id="id23"><span>[Metcalfe1976]</span></a>. It could also be built with twisted pairs. Before extending CSMA, it is useful to understand, more intuitively, how frames are transmitted in such a network and how collisions can occur. The figure below illustrates the physical transmission of a frame on such a cable. To transmit its frame, host A must send an electrical signal on the shared medium. The first step is thus to begin the transmission of the electrical signal. This is point <cite>(1)</cite> in the figure below. This electrical signal will travel along the cable. Although electrical signals travel fast, we know that information cannot travel faster than the speed of light (i.e. 300.000 kilometers/second). On a coaxial cable, an electrical signal is slightly slower than the speed of light and 200.000 kilometers per second is a reasonable estimation. This implies that if the cable has a length of one kilometer, the electrical signal will need 5 microseconds to travel from one end of the cable to the other. The ends of coaxial cables are equipped with termination points that ensure that the electrical signal is not reflected back to its source. This is illustrated at point <cite>(3)</cite> in the figure, where the electrical signal has reached the left endpoint and host B. At this point, B starts to receive the frame being transmitted by A. Notice that there is a delay between the transmission of a bit on host A and its reception by host B. If there were other hosts attached to the cable, they would receive the first bit of the frame at slightly different times. As we will see later, this timing difference is a key problem for MAC algorithms. At point <cite>(4)</cite>, the electrical signal has reached both ends of the cable and occupies it completely. Host A continues to transmit the electrical signal until the end of the frame. As shown at point <cite>(5)</cite>, when the sending host stops its transmission, the electrical signal corresponding to the end of the frame leaves the coaxial cable. The channel becomes empty again once the entire electrical signal has been removed from the cable.</p>
<figure class="align-center" id="id85">
<a class="reference internal image-reference" href="../_images/frame-bus.png"><img alt="../_images/frame-bus.png" src="../Images/ab89b8b4019e54d32dc8e8a85f104ba1.png" style="width: 452.2px; height: 354.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-bus.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 197 </span><span class="caption-text">Frame transmission on a shared bus</span><a class="headerlink" href="#id85" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Now that we have looked at how a frame is actually transmitted as an electrical signal on a shared bus, it is interesting to look in more detail at what happens when two hosts transmit a frame at almost the same time. This is illustrated in the figure below, where hosts A and B start their transmission at the same time (point <cite>(1)</cite>). At this time, if host C senses the channel, it will consider it to be free. This will not last a long time and at point <cite>(2)</cite> the electrical signals from both host A and host B reach host C. The combined electrical signal (shown graphically as the superposition of the two curves in the figure) cannot be decoded by host C. Host C detects a collision, as it receives a signal that it cannot decode. Since host C cannot decode the frames, it cannot determine which hosts are sending the colliding frames. Note that host A (and host B) will detect the collision after host C (point <cite>(3)</cite> in figure <a class="reference internal" href="#fig-collision-bus"><span class="std std-numref">Fig. 198</span></a>).</p>
<figure class="align-center" id="fig-collision-bus">
<a class="reference internal image-reference" href="../_images/frame-collision.png"><img alt="../_images/frame-collision.png" src="../Images/7b88fb2c9cc28c5857f616b145a56cb6.png" style="width: 446.59999999999997px; height: 298.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 198 </span><span class="caption-text">Frame collision on a shared bus</span><a class="headerlink" href="#fig-collision-bus" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-21">As shown above, hosts detect collisions when they receive an electrical signal that they cannot decode. In a wired network, a host is able to detect such a collision both while it is listening (e.g. like host C in the figure above) and also while it is sending its own frame. When a host transmits a frame, it can compare the electrical signal that it transmits with the electrical signal that it senses on the wire. At points <cite>(1)</cite> and <cite>(2)</cite> in the figure above, host A senses only its own signal. At point <cite>(3)</cite>, it senses an electrical signal that differs from its own signal and can thus detects the collision. At this point, its frame is corrupted and it can stop its transmission. The ability to detect collisions while transmitting is the starting point for the <cite>Carrier Sense Multiple Access with Collision Detection (CSMA/CD)</cite> Medium Access Control algorithm, which is used in Ethernet networks <a class="reference internal" href="../bibliography.html#metcalfe1976" id="id24"><span>[Metcalfe1976]</span></a> <a class="reference internal" href="../bibliography.html#ieee802-3" id="id25"><span>[IEEE802.3]</span></a> . When an Ethernet host detects a collision while it is transmitting, it immediately stops its transmission. Compared with pure CSMA, CSMA/CD is an important improvement since when collisions occur, they only last until colliding hosts have detected it and stopped their transmission. In practice, when a host detects a collision, it sends a special jamming signal on the cable to ensure that all hosts have detected the collision.</p>
<p>To better understand these collisions, it is useful to analyze what would be the worst collision on a shared bus network. Let us consider a wire with two hosts attached at both ends, as shown in the figure below. Host A starts to transmit its frame and its electrical signal is propagated on the cable. Its propagation time depends on the physical length of the cable and the speed of the electrical signal. Let us use <span class="math notranslate nohighlight">\(\tau\)</span> to represent this propagation delay in seconds. Slightly less than <span class="math notranslate nohighlight">\(\tau\)</span> seconds after the beginning of the transmission of A’s frame, B decides to start transmitting its own frame. After <span class="math notranslate nohighlight">\(\epsilon\)</span> seconds, B senses A’s frame, detects the collision and stops transmitting. The beginning of B’s frame travels on the cable until it reaches host A. Host A can thus detect the collision at time <span class="math notranslate nohighlight">\(\tau-\epsilon+\tau \approx 2\times\tau\)</span>. An important point to note is that a collision can only occur during the first <span class="math notranslate nohighlight">\(2\times\tau\)</span> seconds of its transmission. If a collision did not occur during this period, it cannot occur afterwards since the transmission channel is busy after <span class="math notranslate nohighlight">\(\tau\)</span> seconds and CSMA/CD hosts sense the transmission channel before transmitting their frame.</p>
<figure class="align-center" id="id86">
<a class="reference internal image-reference" href="../_images/frame-collision-worst.png"><img alt="../_images/frame-collision-worst.png" src="../Images/cb963f4bef31245667608aad2e590553.png" style="width: 448.7px; height: 208.6px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision-worst.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 199 </span><span class="caption-text">The worst collision on a shared bus</span><a class="headerlink" href="#id86" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Furthermore, on the wired networks where CSMA/CD is used, collisions are almost the only cause of transmission errors that affect frames. Transmission errors that only affect a few bits inside a frame seldom occur in these wired networks. For this reason, the designers of CSMA/CD chose to completely remove the acknowledgment frames in the datalink layer. When a host transmits a frame, it verifies whether its transmission has been affected by a collision. If not, given the negligible Bit Error Ratio of the underlying network, it assumes that the frame was received correctly by its destination. Otherwise the frame is retransmitted after some delay.</p>
<p>Removing acknowledgments is an interesting optimization as it reduces the number of frames that are exchanged on the network and the number of frames that need to be processed by the hosts. However, to use this optimization, we must ensure that all hosts will be able to detect all the collisions that affect their frames. The problem is important for short frames. Let us consider two hosts, A and B, that are sending a small frame to host C as illustrated in the figure below. If the frames sent by A and B are very short, the situation illustrated below may occur. Hosts A and B send their frame and stop transmitting (point <cite>(1)</cite>). When the two short frames arrive at the location of host C, they collide and host C cannot decode them (point <cite>(2)</cite>). The two frames are absorbed by the ends of the wire. Neither host A nor host B have detected the collision. They both consider their frame to have been received correctly by its destination.</p>
<figure class="align-center" id="id87">
<a class="reference internal image-reference" href="../_images/frame-collision-short.png"><img alt="../_images/frame-collision-short.png" src="../Images/9a5d6d727ec45fd15b48e2c3fdc820d7.png" style="width: 438.9px; height: 298.9px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision-short.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 200 </span><span class="caption-text">The short-frame collision problem</span><a class="headerlink" href="#id87" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-22">To solve this problem, networks using CSMA/CD require hosts to transmit for at least <span class="math notranslate nohighlight">\(2\times\tau\)</span> seconds. Since the network transmission speed is fixed for a given network technology, this implies that a technology that uses CSMA/CD enforces a minimum frame size. In the most popular CSMA/CD technology, Ethernet, <span class="math notranslate nohighlight">\(2\times\tau\)</span> is called the <cite>slot time</cite> <a class="footnote-reference brackets" href="#fslottime" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.</p>
<p id="index-23">The last innovation introduced by CSMA/CD is the computation of the retransmission timeout. As for ALOHA, this timeout cannot be fixed, otherwise hosts could become synchronized and always retransmit at the same time. Setting such a timeout is always a compromise between the network access delay and the amount of collisions. A short timeout would lead to a low network access delay but with a higher risk of collisions. On the other hand, a long timeout would cause a long network access delay but a lower risk of collisions. The <cite>binary exponential back-off</cite> algorithm was introduced in CSMA/CD networks to solve this problem.</p>
<p>To understand <cite>binary exponential back-off</cite>, let us consider a collision caused by exactly two hosts. Once it has detected the collision, a host can either retransmit its frame immediately or defer its transmission for some time. If each colliding host flips a coin to decide whether to retransmit immediately or to defer its retransmission, four cases are possible :</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Both hosts retransmit immediately and a new collision occurs</p></li>
<li><p>The first host retransmits immediately and the second defers its retransmission</p></li>
<li><p>The second host retransmits immediately and the first defers its retransmission</p></li>
<li><p>Both hosts defer their retransmission and a new collision occurs</p></li>
</ol>
</div></blockquote>
<p>In the second and third cases, both hosts have flipped different coins. The delay chosen by the host that defers its retransmission should be long enough to ensure that its retransmission will not collide with the immediate retransmission of the other host. However the delay should not be longer than the time necessary to avoid the collision, because if both hosts decide to defer their transmission, the network will be idle during this delay. The <cite>slot time</cite> is the optimal delay since it is the shortest delay that ensures that the first host will be able to retransmit its frame completely without any collision.</p>
<p>If two hosts are competing, the algorithm above will avoid a second collision 50% of the time. However, if the network is heavily loaded, several hosts may be competing at the same time. In this case, the hosts should be able to automatically adapt their retransmission delay. The <cite>binary exponential back-off</cite> performs this adaptation based on the number of collisions that have affected a frame. After the first collision, the host flips a coin and waits 0 or 1 <cite>slot time</cite>. After the second collision, it generates a random number and waits 0, 1, 2 or 3 <cite>slot times</cite>, etc. The duration of the waiting time is doubled after each collision. The complete pseudo-code for the CSMA/CD algorithm is shown in the figure below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># CSMA/CD pseudo-code</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel_becomes_free</span><span class="p">)</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait_until</span> <span class="p">(</span><span class="n">end_of_frame</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">collision</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">collision</span> <span class="n">detected</span><span class="p">:</span>
        <span class="n">stop_transmitting</span><span class="p">()</span>
        <span class="n">send</span><span class="p">(</span><span class="n">jamming</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">random</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">r</span> <span class="o">*</span> <span class="n">slotTime</span><span class="p">)</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">inter</span><span class="o">-</span><span class="n">frame_delay</span><span class="p">)</span>
        <span class="k">break</span>  <span class="c1"># transmission was successful</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p>The inter-frame delay used in this pseudo-code is a short delay corresponding to the time required by a network adapter to switch from transmit to receive mode. It is also used to prevent a host from sending a continuous stream of frames without leaving any transmission opportunities for other hosts on the network. This contributes to the fairness of CSMA/CD. Despite this delay, there are still conditions where CSMA/CD is not completely fair <a class="reference internal" href="../bibliography.html#ry1994" id="id27"><span>[RY1994]</span></a>. Consider for example a network with two hosts : a server sending long frames and a client sending acknowledgments. Measurements reported in <a class="reference internal" href="../bibliography.html#ry1994" id="id28"><span>[RY1994]</span></a> have shown that there are situations where the client could suffer from repeated collisions that lead it to wait for long periods of time due to the exponential back-off algorithm.</p>
</section>
<section id="carrier-sense-multiple-access-with-collision-avoidance">
<span id="index-24"/><h3>Carrier Sense Multiple Access with Collision Avoidance<a class="headerlink" href="#carrier-sense-multiple-access-with-collision-avoidance" title="Link to this heading">#</a></h3>
<p>The <cite>Carrier Sense Multiple Access with Collision Avoidance</cite> (CSMA/CA) Medium Access Control algorithm was designed for the popular WiFi wireless network technology <a class="reference internal" href="../bibliography.html#ieee802-11" id="id29"><span>[IEEE802.11]</span></a>. CSMA/CA also senses the transmission channel before transmitting a frame. Furthermore, CSMA/CA tries to avoid collisions by carefully tuning the timers used by CSMA/CA devices.</p>
<p id="index-25">CSMA/CA uses acknowledgments like CSMA. Each frame contains a sequence number and a CRC. The CRC is used to detect transmission errors while the sequence number is used to avoid frame duplication. When a device receives a correct frame, it returns a special acknowledgment frame to the sender. CSMA/CA introduces a small delay, named <cite>Short Inter Frame Spacing</cite>  (SIFS), between the reception of a frame and the transmission of the acknowledgment frame. This delay corresponds to the time that is required to switch the radio of a device between the reception and transmission modes.</p>
<p id="index-26">Compared to CSMA, CSMA/CA defines more precisely when a device is allowed to send a frame. First, CSMA/CA defines two delays : <cite>DIFS</cite> and <cite>EIFS</cite>. To send a frame, a device must first wait until the channel has been idle for at least the <cite>Distributed Coordination Function Inter Frame Space</cite> (DIFS) if the previous frame was received correctly. However, if the previously received frame was corrupted, this indicates that there are collisions and the device must sense the channel idle for at least the <cite>Extended Inter Frame Space</cite> (EIFS), with <span class="math notranslate nohighlight">\(SIFS&lt;DIFS&lt;EIFS\)</span>. The exact values for SIFS, DIFS and EIFS depend on the underlying physical layer <a class="reference internal" href="../bibliography.html#ieee802-11" id="id30"><span>[IEEE802.11]</span></a>.</p>
<p>The figure below shows the basic operation of CSMA/CA devices. Before transmitting, host <cite>A</cite> verifies that the channel is empty for a long enough period. Then, its sends its data frame. After checking the validity of the received frame, the recipient sends an acknowledgment frame after a short SIFS delay. Host <cite>C</cite>, which does not participate in the frame exchange, senses the channel to be busy at the beginning of the data frame. Host <cite>C</cite> can use this information to determine how long the channel will be busy for. Note that as <span class="math notranslate nohighlight">\(SIFS&lt;DIFS&lt;EIFS\)</span>, even a device that would start to sense the channel immediately after the last bit of the data frame could not decide to transmit its own frame during the transmission of the acknowledgment frame.</p>
<figure class="align-center" id="id88">
<a class="reference internal image-reference" href="../_images/csmaca-1.png"><img alt="../_images/csmaca-1.png" src="../Images/a384b3211b0772e04aaf1c46baf44b77.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-1.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 201 </span><span class="caption-text">Operation of a CSMA/CA device</span><a class="headerlink" href="#id88" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-27">The main difficulty with CSMA/CA is when two or more devices transmit at the same time and cause collisions. This is illustrated in the figure below, assuming a fixed timeout after the transmission of a data frame. With CSMA/CA, the timeout after the transmission of a data frame is very small, since it corresponds to the SIFS plus the time required to transmit the acknowledgment frame.</p>
<figure class="align-center" id="id89">
<a class="reference internal image-reference" href="../_images/csmaca-2.png"><img alt="../_images/csmaca-2.png" src="../Images/0dbf88b43018a324dcb149e4a1fe2cae.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-2.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 202 </span><span class="caption-text">Collisions with CSMA/CA</span><a class="headerlink" href="#id89" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To deal with this problem, CSMA/CA relies on a backoff timer. This backoff timer is a random delay that is chosen by each device in a range that depends on the number of retransmissions for the current frame. The range grows exponentially with the retransmissions as in CSMA/CD. The minimum range for the backoff timer is <span class="math notranslate nohighlight">\([0,7*slotTime]\)</span> where the <cite>slotTime</cite> is a parameter that depends on the underlying physical layer. Compared to CSMA/CD’s exponential backoff, there are two important differences to notice. First, the initial range for the backoff timer is seven times larger. This is because it is impossible in CSMA/CA to detect collisions as they happen. With CSMA/CA, a collision may affect the entire frame while with CSMA/CD it can only affect the beginning of the frame. Second, a CSMA/CA device must regularly sense the transmission channel during its back off timer. If the channel becomes busy (i.e. because another device is transmitting), then the back off timer must be frozen until the channel becomes free again. Once the channel becomes free, the back off timer is restarted. This is in contrast with CSMA/CD where the back off is recomputed after each collision. This is illustrated in the figure below. Host <cite>A</cite> chooses a smaller backoff than host <cite>C</cite>. When <cite>C</cite> senses the channel to be busy, it freezes its backoff timer and only restarts it once the channel is free again.</p>
<figure class="align-center" id="id90">
<a class="reference internal image-reference" href="../_images/csmaca-3.png"><img alt="../_images/csmaca-3.png" src="../Images/9634d388481fa8d42a5b4b7de8b205c0.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-3.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 203 </span><span class="caption-text">Detailed example with CSMA/CA</span><a class="headerlink" href="#id90" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The pseudo-code below summarizes the operation of a CSMA/CA device. The values of the SIFS, DIFS, EIFS and <span class="math notranslate nohighlight">\(slotTime\)</span> depend on the underlying physical layer technology <a class="reference internal" href="../bibliography.html#ieee802-11" id="id31"><span>[IEEE802.11]</span></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># CSMA/CA simplified pseudo-code</span>
<span class="n">N</span><span class="o">=</span><span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait_until</span><span class="p">(</span><span class="n">free</span><span class="p">(</span><span class="n">channel</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">correct</span><span class="p">(</span><span class="n">last_frame</span><span class="p">):</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">channel_free_during_t</span> <span class="o">&gt;=</span> <span class="n">DIFS</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">channel_free_during_t</span> <span class="o">&gt;=</span> <span class="n">EIFS</span><span class="p">)</span>

    <span class="n">backoff_time</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">7</span> <span class="o">*</span> <span class="p">(</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))))</span> <span class="o">*</span> <span class="n">slotTime</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel</span> <span class="n">free</span> <span class="n">during</span> <span class="n">backoff_time</span><span class="p">)</span>
    <span class="c1"># backoff timer is frozen while channel is sensed to be busy</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">received</span><span class="p">(</span><span class="n">ack</span><span class="p">)</span>
        <span class="c1"># frame received correctly</span>
        <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># retransmission required</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p id="index-28">Another problem faced by wireless networks is often called the <cite>hidden station problem</cite>. In a wireless network, radio signals are not always propagated same way in all directions. For example, two devices separated by a wall may not be able to receive each other’s signal while they could both be receiving the signal produced by a third host. This is illustrated in the figure below, but it can happen in other environments. For example, two devices that are on different sides of a hill may not be able to receive each other’s signal while they are both able to receive the signal sent by a station at the top of the hill. Furthermore, the radio propagation conditions may change with time. For example, a truck may temporarily block the communication between two nearby devices.</p>
<figure class="align-center" id="id91">
<a class="reference internal image-reference" href="../_images/csmaca-hidden.png"><img alt="../_images/csmaca-hidden.png" src="../Images/1dd2db4f7a4f3df51403915d42254672.png" style="width: 350.0px; height: 162.39999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-hidden.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 204 </span><span class="caption-text">The hidden station problem</span><a class="headerlink" href="#id91" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-29">To avoid collisions in these situations, CSMA/CA allows devices to reserve the transmission channel for some time. This is done by using two control frames : <cite>Request To Send</cite> (RTS) and <cite>Clear To Send</cite> (CTS). Both are very short frames to minimize the risk of collisions. To reserve the transmission channel, a device sends a RTS frame to the intended recipient of the data frame. The RTS frame contains the duration of the requested reservation. The recipient replies, after a SIFS delay, with a CTS frame which also contains the duration of the reservation. As the duration of the reservation has been sent in both RTS and CTS, all hosts that could collide with either the sender or the reception of the data frame are informed of the reservation. They can compute the total duration of the transmission and defer their access to the transmission channel until then. This is illustrated in the figure below where host <cite>A</cite> reserves the transmission channel to send a data frame to host <cite>B</cite>. Host <cite>C</cite> notices the reservation and defers its transmission.</p>
<figure class="align-center" id="id92">
<a class="reference internal image-reference" href="../_images/csmaca-reserv.png"><img alt="../_images/csmaca-reserv.png" src="../Images/d32c0a1c69694ffdbad7f26e535e2135.png" style="width: 350.0px; height: 207.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-reserv.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 205 </span><span class="caption-text">Reservations with CSMA/CA</span><a class="headerlink" href="#id92" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The utilization of the reservations with CSMA/CA is an optimization that is useful when collisions are frequent. If there are few collisions, the time required to transmit the RTS and CTS frames can become significant and in particular when short frames are exchanged. Some devices only turn on RTS/CTS after transmission errors.</p>
</section>
<section id="deterministic-medium-access-control-algorithms">
<h3>Deterministic Medium Access Control algorithms<a class="headerlink" href="#deterministic-medium-access-control-algorithms" title="Link to this heading">#</a></h3>
<p>During the 1970s and 1980s, there were huge debates in the networking community about the best suited Medium Access Control algorithms for Local Area Networks. The optimistic algorithms that we have described until now were relatively easy to implement when they were designed. From a performance perspective, mathematical models and simulations showed the ability of these optimistic techniques to sustain load. However, none of the optimistic techniques are able to guarantee that a frame will be delivered within a given delay bound and some applications require predictable transmission delays. The deterministic MAC algorithms were considered by a fraction of the networking community as the best solution to fulfill the needs of Local Area Networks.</p>
<p>Both the proponents of the deterministic and the opportunistic techniques lobbied to develop standards for Local Area networks that would incorporate their solution. Instead of trying to find an impossible compromise between these diverging views, the IEEE 802 committee that was chartered to develop Local Area Network standards chose to work in parallel on three different LAN technologies and created three working groups. The <a class="reference external" href="http://www.ieee802.org/3/">IEEE 802.3 working group</a> became responsible for CSMA/CD. The proponents of deterministic MAC algorithms agreed on the basic principle of exchanging special frames called tokens between devices to regulate the access to the transmission medium. However, they did not agree on the most suitable physical layout for the network. IBM argued in favor of Ring-shaped networks while the manufacturing industry, led by General Motors, argued in favor of a bus-shaped network. This led to the creation of the <a class="reference external" href="http://www.ieee802.org/4/">IEEE 802.4 working group</a> to standardize Token Bus networks and the <a class="reference external" href="http://www.ieee802.org/5/">IEEE 802.5 working group</a> to standardize Token Ring networks. Although these techniques are not widely used anymore today, the principles behind a token-based protocol are still important.</p>
<p>The IEEE 802.5 Token Ring technology is defined in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id32"><span>[IEEE802.5]</span></a>. We use Token Ring as an example to explain the principles of the token-based MAC algorithms in ring-shaped networks. Other ring-shaped networks include the defunct FDDI <a class="reference internal" href="../bibliography.html#ross1989" id="id33"><span>[Ross1989]</span></a> or Resilient Pack Ring <a class="reference internal" href="../bibliography.html#dygu2004" id="id34"><span>[DYGU2004]</span></a> . A good survey of the early token ring networks may be found in <a class="reference internal" href="../bibliography.html#bux1989" id="id35"><span>[Bux1989]</span></a> .</p>
<p>A Token Ring network is composed of a set of stations that are attached to a unidirectional ring. The basic principle of the Token Ring MAC algorithm is that two types of frames travel on the ring : tokens and data frames. When the Token Ring starts, one of the stations sends the token. The token is a small frame that represents the authorization to transmit data frames on the ring. To transmit a data frame on the ring, a station must first capture the token by removing it from the ring. As only one station can capture the token at a time, the station that owns the token can safely transmit a data frame on the ring without risking collisions. After having transmitted its frame, the station must remove it from the ring and resend the token so that other stations can transmit their own frames.</p>
<figure class="align-center" id="id93">
<span id="fig-tokenring"/><a class="reference internal image-reference" href="../_images/token-ring.png"><img alt="../_images/token-ring.png" src="../Images/204991c6fea7862f915a3396c1578736.png" style="width: 350.0px; height: 154.7px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/token-ring.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 206 </span><span class="caption-text">A Token Ring network</span><a class="headerlink" href="#id93" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>While the basic principles of the Token Ring are simple, there are several subtle implementation details that add complexity to Token Ring networks. To understand these details let us analyze the operation of a Token Ring interface on a station. A Token Ring interface serves three different purposes. Like other LAN interfaces, it must be able to send and receive frames. In addition, a Token Ring interface is part of the ring, and as such, it must be able to forward the electrical signal that passes on the ring even when its station is powered off.</p>
<p>When powered-on, Token Ring interfaces operate in two different modes : <cite>listen</cite> and <cite>transmit</cite>. When operating in <cite>listen</cite> mode, a Token Ring interface receives an electrical signal from its upstream neighbor on the ring, introduces a delay equal to the transmission time of one bit on the ring and regenerates the signal before sending it to its downstream neighbor on the ring.</p>
<p>The first problem faced by a Token Ring network is that as the token represents the authorization to transmit, it must continuously travel on the ring when no data frame is being transmitted. Let us assume that a token has been produced and sent on the ring by one station. In Token Ring networks, the token is a 24 bits frame whose structure is shown below.</p>
<figure class="align-center" id="id94">
<span id="index-30"/><a class="reference internal image-reference" href="../_images/token-ring.svg"><img alt="../_images/token-ring.svg" src="../Images/d3fd5c0099436c887eea9ad8ae351d82.png" style="width: 453.0px; height: 128.0px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/token-ring.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 207 </span><span class="caption-text">802.5 token format</span><a class="headerlink" href="#id94" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-31">The token is composed of three fields. First, the <cite>Starting Delimiter</cite> is the marker that indicates the beginning of a frame. The first Token Ring networks used Manchester coding and the <cite>Starting Delimiter</cite> contained both symbols representing <cite>0</cite> and symbols that do not represent bits. The last field is the <cite>Ending Delimiter</cite> which marks the end of the token. The <cite>Access Control</cite> field is present in all frames, and contains several flags. The most important is the <cite>Token</cite> bit that is set in token frames and reset in other frames.</p>
<p id="index-32">Let us consider the five station network depicted in figure <a class="reference internal" href="#fig-tokenring"><span class="std std-ref">A Token Ring network</span></a> above and assume that station <cite>S1</cite> sends a token. If we neglect the propagation delay on the inter-station links, as each station introduces a one bit delay, the first bit of the frame would return to <cite>S1</cite> while it sends the fifth bit of the token. If station <cite>S1</cite> is powered off at that time, only the first five bits of the token will travel on the ring. To avoid this problem, there is a special station called the <cite>Monitor</cite> on each Token Ring. To ensure that the token can travel forever on the ring, this <cite>Monitor</cite> inserts a delay that is equal to at least 24 bit transmission times. If station <cite>S3</cite> was the <cite>Monitor</cite> in figure <a class="reference internal" href="#fig-tokenring"><span class="std std-ref">A Token Ring network</span></a>, <cite>S1</cite> would have been able to transmit the entire token before receiving the first bit of the token from its upstream neighbor.</p>
<p>Now that we have explained how the token can be forwarded on the ring, let us analyze how a station can capture a token to transmit a data frame. For this, we need some information about the format of the data frames. An 802.5 data frame begins with the <cite>Starting Delimiter</cite> followed by the <cite>Access Control</cite> field whose <cite>Token</cite> bit is reset, a <cite>Frame Control</cite> field that enables the definition of several types of frames, destination and source address, a payload, a CRC, the <cite>Ending Delimiter</cite> and a <cite>Frame Status</cite> field. The format of the Token Ring data frames is illustrated below.</p>
<figure class="align-center" id="id95">
<span id="index-33"/><a class="reference internal image-reference" href="../_images/8025.svg"><img alt="../_images/8025.svg" src="../Images/9f175bb0486e691ee2c9018db5a42d22.png" style="width: 604.0px; height: 320.0px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/8025.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 208 </span><span class="caption-text">802.5 data frame format</span><a class="headerlink" href="#id95" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To capture a token, a station must operate in <cite>Listen</cite> mode. In this mode, the station receives bits from its upstream neighbor. If the bits correspond to a data frame, they must be forwarded to the downstream neighbor. If they correspond to a token, the station can capture it and transmit its data frame. Both the data frame and the token are encoded as a bit string beginning with the <cite>Starting Delimiter</cite> followed by the <cite>Access Control</cite> field. When the station receives the first bit of a <cite>Starting Delimiter</cite>, it cannot know whether this is a data frame or a token and must forward the entire delimiter to its downstream neighbor. It is only when it receives the fourth bit of the <cite>Access Control</cite> field (i.e. the <cite>Token</cite> bit) that the station knows whether the frame is a data frame or a token. If the <cite>Token</cite> bit is reset, it indicates a data frame and the remaining bits of the data frame must be forwarded to the downstream station. Otherwise (<cite>Token</cite> bit is set), this is a token and the station can capture it by resetting the bit that is currently in its buffer. Thanks to this modification, the beginning of the token is now the beginning of a data frame and the station can switch to <cite>Transmit</cite> mode and send its data frame starting at the fifth bit of the <cite>Access Control</cite> field. Thus, the one-bit delay introduced by each Token Ring station plays a key role in enabling the stations to efficiently capture the token.</p>
<p>After having transmitted its data frame, the station must remain in <cite>Transmit</cite> mode until it has received the last bit of its own data frame. This ensures that the bits sent by a station do not remain in the network forever. A data frame sent by a station in a Token Ring network passes in front of all stations attached to the network. Each station can detect the data frame and analyze the destination address to possibly capture the frame.</p>
<p id="index-34">The text above describes the basic operation of a Token Ring network when all stations work correctly. Unfortunately, a real Token Ring network must be able to handle various types of anomalies and this increases the complexity of Token Ring stations. We briefly list the problems and outline their solutions below. A detailed description of the operation of Token Ring stations may be found in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id36"><span>[IEEE802.5]</span></a>. The first problem is when all the stations attached to the network start. One of them must bootstrap the network by sending the first token. For this, all stations implement a distributed election mechanism that is used to select the <cite>Monitor</cite>. Any station can become a <cite>Monitor</cite>. The <cite>Monitor</cite> manages the Token Ring network and ensures that it operates correctly. Its first role is to introduce a delay of 24 bit transmission times to ensure that the token can travel smoothly on the ring. Second, the <cite>Monitor</cite> sends the first token on the ring. It must also verify that the token passes regularly. According to the Token Ring standard <a class="reference internal" href="../bibliography.html#ieee802-5" id="id37"><span>[IEEE802.5]</span></a>, a station cannot retain the token to transmit data frames for a duration longer than the <cite>Token Holding Time</cite> (THT) (slightly less than 10 milliseconds). On a network containing <cite>N</cite> stations, the <cite>Monitor</cite> must receive the token at least every <span class="math notranslate nohighlight">\(N \times THT\)</span> seconds. If the <cite>Monitor</cite> does not receive a token during such a period, it cuts the ring for some time and then re-initializes the ring and sends a token.</p>
<p>Several other anomalies may occur in a Token Ring network. For example, a station could capture a token and be powered off before having resent the token. Another station could have captured the token, sent its data frame and be powered off before receiving all of its data frame. In this case, the bit string corresponding to the end of a frame would remain in the ring without being removed by its sender. Several techniques are defined in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id38"><span>[IEEE802.5]</span></a> to allow the <cite>Monitor</cite> to handle all these problems. If unfortunately, the <cite>Monitor</cite> fails, another station will be elected to become the new <cite>Monitor</cite>.</p>
</section>
</section>
<section id="congestion-control">
<h2>Congestion control<a class="headerlink" href="#congestion-control" title="Link to this heading">#</a></h2>
<p>Most networks contain links having different bandwidth. Some hosts can use low bandwidth wireless networks. Some servers are attached via 10 Gbps interfaces and inter-router links may vary from a few tens of kilobits per second up to hundred Gbps. Despite these huge differences in performance, any host should be able to efficiently exchange segments with a high-end server.</p>
<p id="index-35">To understand this problem better, let us consider the scenario shown in the figure below, where a server (<cite>A</cite>) attached to a <cite>10 Mbps</cite> link needs to reliably transfer segments to another computer (<cite>C</cite>) through a path that contains a <cite>2 Mbps</cite> link.</p>
<blockquote>
<div><div class="figure" id="id96" style="text-align: center"><p><img src="../Images/437492f9ed4cdc7e563973c918bb6989.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-d10fd2b247bfdcf82bfe39a116b52a6e430735e2.png"/></p>
<p><span class="caption-number">Fig. 209 </span><span class="caption-text">Reliable transport with heterogeneous links</span></p>
</div></div></blockquote>
<p>In this network, the segments sent by the server reach router <cite>R1</cite>. <cite>R1</cite> forwards the segments towards router <cite>R2</cite>. Router <cite>R1</cite> can potentially receive segments at <cite>10 Mbps</cite>, but it can only forward them at <cite>2 Mbps</cite> to router <cite>R2</cite> and then to host <cite>C</cite>.  Router <cite>R1</cite> includes buffers that allow it to store the packets that cannot immediately be forwarded to their destination. To understand the operation of a reliable transport protocol in this environment, let us consider a simplified model of this network where host <cite>A</cite> is attached to a <cite>10 Mbps</cite> link to a queue that represents the buffers of router <cite>R1</cite>. This queue is emptied at a rate of <cite>2 Mbps</cite>.</p>
<figure class="align-center" id="id97">
<a class="reference internal image-reference" href="../_images/tcp-self-clocking.png"><img alt="../_images/tcp-self-clocking.png" src="../Images/84e88a885a2921ce9a2b332adc09db01.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-self-clocking.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 210 </span><span class="caption-text">Self clocking</span><a class="headerlink" href="#id97" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let us consider that host <cite>A</cite> uses a window of three segments. It thus sends three back-to-back segments at <cite>10 Mbps</cite> and then waits for an acknowledgment. Host <cite>A</cite> stops sending segments when its window is full. These segments reach the buffers of router <cite>R1</cite>. The first segment stored in this buffer is sent by router <cite>R1</cite> at a rate of <cite>2 Mbps</cite> to the destination host. Upon reception of this segment, the destination sends an acknowledgment. This acknowledgment allows host <cite>A</cite> to transmit a new segment. This segment is stored in the buffers of router <cite>R1</cite> while it is transmitting the second segment that was sent by host <cite>A</cite>… Thus, after the transmission of the first window of segments, the reliable transport protocol sends one data segment after the reception of each acknowledgment returned by the destination. In practice, the acknowledgments sent by the destination serve as a kind of <cite>clock</cite> that allows the sending host to adapt its transmission rate to the rate at which segments are received by the destination. This <cite>self-clocking</cite> is the first mechanism that allows a window-based reliable transport protocol to adapt to heterogeneous networks <a class="reference internal" href="../bibliography.html#jacobson1988" id="id39"><span>[Jacobson1988]</span></a>. It depends on the availability of buffers to store the segments that have been sent by the sender but have not yet been transmitted to the destination.</p>
<p>However, transport protocols are not only used in this environment. In the global Internet, a large number of hosts send segments to a large number of receivers. For example, let us consider the network depicted below which is similar to the one discussed in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id40"><span>[Jacobson1988]</span></a> and <span class="target" id="index-36"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc896.html"><strong>RFC 896</strong></a>. In this network, we assume that the buffers of the router are infinite to ensure that no packet is lost.</p>
<blockquote>
<div><div class="figure" id="id98" style="text-align: center"><p><img src="../Images/0f464428bb4f3f9bf9d9d1dbaafd389d.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-af98f2fddb12f129231142a8dfa25b4f15f14611.png"/></p>
<p><span class="caption-number">Fig. 211 </span><span class="caption-text">The congestion collapse problem</span></p>
</div></div></blockquote>
<p id="index-37">If many senders are attached to the left part of the network above, they all send a window full of segments. These segments are stored in the buffers of the router before being transmitted towards their destination. If there are many senders on the left part of the network, the occupancy of the buffers quickly grows. A consequence of the buffer occupancy is that the round-trip-time, measured by the transport protocol, between the sender and the receiver increases. Consider a network where 10,000 bits segments are sent. When the buffer is empty, such a segment requires 1 millisecond to be transmitted on the <cite>10 Mbps</cite> link and 5 milliseconds to be the transmitted on the <cite>2 Mbps</cite> link. Thus, the measured round-trip-time measured is roughly 6 milliseconds if we ignore the propagation delay on the links. If the buffer contains 100 segments, the round-trip-time becomes <span class="math notranslate nohighlight">\(1+100 \times 5+ 5\)</span> milliseconds as new segments are only transmitted on the <cite>2 Mbps</cite> link once all previous segments have been transmitted. Unfortunately, if the reliable transport protocol uses a retransmission timer and performs <cite>go-back-n</cite> to recover from transmission errors it will retransmit a full window of segments. This increases the occupancy of the buffer and the delay through the buffer… Furthermore, the buffer may store and send on the low bandwidth links several retransmissions of the same segment. This problem is called <cite>congestion collapse</cite>. It occurred several times during the late 1980s on the Internet <a class="reference internal" href="../bibliography.html#jacobson1988" id="id41"><span>[Jacobson1988]</span></a>.</p>
<p>The <cite>congestion collapse</cite> is a problem that all heterogeneous networks face. Different mechanisms have been proposed in the scientific literature to avoid or control network congestion. Some of them have been implemented and deployed in real networks. To understand this problem in more detail, let us first consider a simple network with two hosts attached to a high bandwidth link that are sending segments to destination <cite>C</cite> attached to a low bandwidth link as depicted below.</p>
<blockquote>
<div><div class="figure" id="id99" style="text-align: center"><p><img src="../Images/81b044c15b073243b51b376b5d57bf3d.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-0eb3527ae8292a399f94657427a9073632cc65bf.png"/></p>
<p><span class="caption-number">Fig. 212 </span><span class="caption-text">The congestion problem</span></p>
</div></div></blockquote>
<p>To avoid <cite>congestion collapse</cite>, the hosts must regulate their transmission rate <a class="footnote-reference brackets" href="#fcredit" id="id42" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> by using a <cite>congestion control</cite> mechanism. Such a mechanism can be implemented in the transport layer or in the network layer. In TCP/IP networks, it is implemented in the transport layer, but other technologies such as <cite>Asynchronous Transfer Mode (ATM)</cite> or <cite>Frame Relay</cite> include congestion control mechanisms in lower layers.</p>
<p id="index-38">Let us first consider the simple problem of a set of <span class="math notranslate nohighlight">\(i\)</span> hosts that share a single bottleneck link as shown in the example above. In this network, the congestion control scheme must achieve the following objectives <a class="reference internal" href="../bibliography.html#cj1989" id="id43"><span>[CJ1989]</span></a> :</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>The congestion control scheme must <cite>avoid congestion</cite>. In practice, this means that the bottleneck link cannot be overloaded. If <span class="math notranslate nohighlight">\(r_i(t)\)</span> is the transmission rate allocated to host <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(R\)</span> the bandwidth of the bottleneck link, then the congestion control scheme should ensure that, on average, <span class="math notranslate nohighlight">\(\forall{t} \sum{r_i(t)} \le R\)</span>.</p></li>
<li><p>The congestion control scheme must be <cite>efficient</cite>. The bottleneck link is usually both a shared and an expensive resource. Usually, bottleneck links are wide area links that are much more expensive to upgrade than the local area networks. The congestion control scheme should ensure that such links are efficiently used. Mathematically, the control scheme should ensure that <span class="math notranslate nohighlight">\(\forall{t} \sum{r_i(t)} \approx R\)</span>.</p></li>
<li><p>The congestion control scheme should be <cite>fair</cite>. Most congestion schemes aim at achieving <cite>max-min fairness</cite>. An allocation of transmission rates to sources is said to be <cite>max-min fair</cite> if :</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>no link in the network is congested</p></li>
<li><p>the rate allocated to source <span class="math notranslate nohighlight">\(j\)</span> cannot be increased without decreasing the rate allocated to a source <span class="math notranslate nohighlight">\(i\)</span> whose allocation is smaller than the rate allocated to source <span class="math notranslate nohighlight">\(j\)</span> <a class="reference internal" href="../bibliography.html#leboudec2008" id="id44"><span>[Leboudec2008]</span></a> .</p></li>
</ul>
</div></blockquote>
</div></blockquote>
<p>Depending on the network, a <cite>max-min fair allocation</cite> may not always exist. In practice, <cite>max-min fairness</cite> is an ideal objective that cannot necessarily be achieved. When there is a single bottleneck link as in the example above, <cite>max-min fairness</cite> implies that each source should be allocated the same transmission rate.</p>
<p>To visualize the different rate allocations, it is useful to consider the graph shown below. In this graph, we plot on the <cite>x-axis</cite> (resp. <cite>y-axis</cite>) the rate allocated to host <cite>B</cite> (resp. <cite>A</cite>). A point in the graph <span class="math notranslate nohighlight">\((r_B,r_A)\)</span> corresponds to a possible allocation of the transmission rates. Since there is a <cite>2 Mbps</cite> bottleneck link in this network, the graph can be divided into two regions. The lower left part of the graph contains all allocations <span class="math notranslate nohighlight">\((r_B,r_A)\)</span> such that the bottleneck link is not congested (<span class="math notranslate nohighlight">\(r_A+r_B&lt;2\)</span>). The right border of this region is the <cite>efficiency line</cite>, i.e. the set of allocations that completely utilize the bottleneck link (<span class="math notranslate nohighlight">\(r_A+r_B=2\)</span>). Finally, the <cite>fairness line</cite> is the set of fair allocations.</p>
<figure class="align-center" id="id100">
<a class="reference internal image-reference" href="../_images/congestion-rates.png"><img alt="../_images/congestion-rates.png" src="../Images/9ccea874ced3496fad9f847266e5e96a.png" style="width: 251.29999999999998px; height: 168.0px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/congestion-rates.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 213 </span><span class="caption-text">Possible allocated transmission rates</span><a class="headerlink" href="#id100" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As shown in the graph above, a rate allocation may be fair but not efficient (e.g. <span class="math notranslate nohighlight">\(r_A=0.7,r_B=0.7\)</span>), fair and efficient ( e.g. <span class="math notranslate nohighlight">\(r_A=1,r_B=1\)</span>) or efficient but not fair (e.g. <span class="math notranslate nohighlight">\(r_A=1.5,r_B=0.5\)</span>). Ideally, the allocation should be both fair and efficient. Unfortunately, maintaining such an allocation with fluctuations in the number of flows that use the network is a challenging problem. Furthermore, there might be several thousands flows that pass through the same link <a class="footnote-reference brackets" href="#fflowslink" id="id45" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>.</p>
<p>To deal with these fluctuations in demand, which result in fluctuations in the available bandwidth, computer networks use a congestion control scheme. This congestion control scheme should achieve the three objectives listed above. Some congestion control schemes rely on a close cooperation between the end hosts and the routers, while others are mainly implemented on the end hosts with limited support from the routers.</p>
<p>A congestion control scheme can be modeled as an algorithm that adapts the transmission rate (<span class="math notranslate nohighlight">\(r_i(t)\)</span>) of host <span class="math notranslate nohighlight">\(i\)</span> based on the feedback received from the network. Different types of feedback are possible. The simplest scheme is a binary feedback <a class="reference internal" href="../bibliography.html#cj1989" id="id46"><span>[CJ1989]</span></a>  <a class="reference internal" href="../bibliography.html#jacobson1988" id="id47"><span>[Jacobson1988]</span></a> where the hosts simply learn whether the network is congested or not. Some congestion control schemes allow the network to regularly send an allocated transmission rate in Mbps to each host <a class="reference internal" href="../bibliography.html#bf1995" id="id48"><span>[BF1995]</span></a>.</p>
<p id="index-39">Let us focus on the binary feedback scheme which is the most widely used today. Intuitively, the congestion control scheme should decrease the transmission rate of a host when congestion has been detected in the network, in order to avoid congestion collapse. Furthermore, the hosts should increase their transmission rate when the network is not congested. Otherwise, the hosts would not be able to efficiently utilize the network. The rate allocated to each host fluctuates with time, depending on the feedback received from the network. Figure <a class="reference internal" href="#fig-congestion-rates"><span class="std std-numref">Fig. 214</span></a> illustrates the evolution of the transmission rates allocated to two hosts in our simple network. Initially, two hosts have a low allocation, but this is not efficient. The allocations increase until the network becomes congested. At this point, the hosts decrease their transmission rate to avoid congestion collapse. If the congestion control scheme works well, after some time the allocations should become both fair and efficient.</p>
<figure class="align-center" id="fig-congestion-rates">
<a class="reference internal image-reference" href="../_images/congestion-rates-evolution.png"><img alt="../_images/congestion-rates-evolution.png" src="../Images/26c6fbe274fb53fd9009bee48fcdb53e.png" style="width: 221.89999999999998px; height: 143.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/congestion-rates-evolution.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 214 </span><span class="caption-text">Evolution of the transmission rates</span><a class="headerlink" href="#fig-congestion-rates" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Various types of rate adaption algorithms are possible. <a class="reference external" href="https://home.ie.cuhk.edu.hk/~dmchiu/">Dah Ming Chiu</a> and <a class="reference external" href="https://www.cse.wustl.edu/~jain/">Raj Jain</a> have analyzed, in <a class="reference internal" href="../bibliography.html#cj1989" id="id49"><span>[CJ1989]</span></a>, different types of algorithms that can be used by a source to adapt its transmission rate to the feedback received from the network. Intuitively, such a rate adaptation algorithm increases the transmission rate when the network is not congested (ensure that the network is efficiently used) and decrease the transmission rate when the network is congested (to avoid congestion collapse).</p>
<p>The simplest form of feedback that the network can send to a source is a binary feedback (the network is congested or not congested). In this case, a <cite>linear</cite> rate adaptation algorithm can be expressed as :</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(rate(t+1)=\alpha_C + \beta_C rate(t)\)</span> when the network is congested</p></li>
<li><p><span class="math notranslate nohighlight">\(rate(t+1)=\alpha_N + \beta_N rate(t)\)</span> when the network is <em>not</em> congested</p></li>
</ul>
</div></blockquote>
<p>With a linear adaption algorithm, <span class="math notranslate nohighlight">\(\alpha_C,\alpha_N, \beta_C\)</span> and <span class="math notranslate nohighlight">\(\beta_N\)</span> are constants.
The analysis of <a class="reference internal" href="../bibliography.html#cj1989" id="id50"><span>[CJ1989]</span></a> shows that to be fair and efficient, such a binary rate adaption mechanism must rely on <cite>Additive Increase and Multiplicative Decrease</cite>. When the network is not congested, the hosts should slowly increase their transmission rate (<span class="math notranslate nohighlight">\(\beta_N=1~and~\alpha_N&gt;0\)</span>). When the network is congested, the hosts must multiplicatively decrease their transmission rate (<span class="math notranslate nohighlight">\(\beta_C &lt; 1~and~\alpha_C = 0\)</span>). Such an AIMD rate adaptation algorithm can be implemented by the pseudo-code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># Additive Increase Multiplicative Decrease</span>
<span class="k">if</span> <span class="n">congestion</span><span class="p">:</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">rate</span> <span class="o">*</span> <span class="n">betaC</span>    <span class="c1"># multiplicative decrease, betaC&lt;1</span>
<span class="k">else</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">rate</span> <span class="o">+</span> <span class="n">alphaN</span>    <span class="c1"># additive increase, alphaN &gt; 0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Which binary feedback ?</p>
<p>Two types of binary feedback are possible in computer networks. A first solution is to rely on implicit feedback. This is the solution chosen for TCP. TCP’s congestion control scheme <a class="reference internal" href="../bibliography.html#jacobson1988" id="id51"><span>[Jacobson1988]</span></a> does not require any cooperation from the router. It only assumes that they use buffers and that they discard packets when there is congestion. TCP uses the segment losses as an indication of congestion. When there are no losses, the network is assumed to be not congested. This implies that congestion is the main cause of packet losses. This is true in wired networks, but unfortunately not always true in wireless networks.
Another solution is to rely on explicit feedback. This is the solution proposed in the DECBit congestion control scheme <a class="reference internal" href="../bibliography.html#rj1995" id="id52"><span>[RJ1995]</span></a> and used in Frame Relay and ATM networks. This explicit feedback can be implemented in two ways. A first solution would be to define a special message that could be sent by routers to hosts when they are congested. Unfortunately, generating such messages may increase the amount of congestion in the network. Such a congestion indication packet is thus discouraged <span class="target" id="index-40"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc1812.html"><strong>RFC 1812</strong></a>. A better approach is to allow the intermediate routers to indicate, in the packets that they forward, their current congestion status. Binary feedback can be encoded by using one bit in the packet header. With such a scheme, congested routers set a special bit in the packets that they forward while non-congested routers leave this bit unmodified. The destination host returns the congestion status of the network in the acknowledgments that it sends. Details about such a solution in IP networks may be found in <span class="target" id="index-41"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a>. Unfortunately, as of this writing, this solution is still not deployed despite its potential benefits.</p>
</div>
<section id="congestion-control-with-a-window-based-transport-protocol">
<h3>Congestion control with a window-based transport protocol<a class="headerlink" href="#congestion-control-with-a-window-based-transport-protocol" title="Link to this heading">#</a></h3>
<p>AIMD controls congestion by adjusting the transmission rate of the sources in reaction to the current congestion level. If the network is not congested, the transmission rate increases. If congestion is detected, the transmission rate is multiplicatively decreased. In practice, directly adjusting the transmission rate can be difficult since it requires the utilization of fine grained timers. In reliable transport protocols, an alternative is to dynamically adjust the sending window. This is the solution chosen for protocols like TCP and SCTP that will be described in more details later. To understand how window-based protocols can adjust their transmission rate, let us consider the very simple scenario of a reliable transport protocol that uses <cite>go-back-n</cite>. Consider the very simple scenario shown in figure <a class="reference internal" href="#fig-bottleneck"><span class="std std-numref">Fig. 215</span></a>.</p>
<blockquote>
<div><div class="figure" id="id101" style="text-align: center">
<span id="fig-bottleneck"/><p><img src="../Images/fba8d6b7380280ee70f3550fb599fb9f.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-c571e39a73d7fa60a9759c2a5d1d690b0ec7f12a.png"/></p>
<p><span class="caption-number">Fig. 215 </span><span class="caption-text">A simple network with hosts sharing a bottleneck link</span></p>
</div></div></blockquote>
<p>The links between the hosts and the routers have a bandwidth of 1 Mbps while the link between the two routers has a bandwidth of 500 Kbps. There is no significant propagation delay in this network. For simplicity, assume that hosts <cite>A</cite> and <cite>B</cite> send 1000 bits packets. The transmission of such a packet on a <cite>host-router</cite> (resp. <cite>router-router</cite> ) link requires 1 msec (resp. 2 msec). If there is no traffic in the network, the round-trip-time measured by host <cite>A</cite> to reach <cite>D</cite> is slightly larger than 4 msec. Let us observe the flow of packets with different window sizes to understand the relationship between sending window and transmission rate.</p>
<p>Consider first a window of one segment. This segment takes 4 msec to reach host <cite>D</cite>. The destination replies with an acknowledgment and the next segment can be transmitted. With such a sending window, the transmission rate is roughly 250 segments per second or 250 Kbps. This is illustrated in figure <a class="reference internal" href="#fig-gbn-win-1"><span class="std std-numref">Fig. 216</span></a> where each square of the grid corresponds to one millisecond.</p>
<blockquote>
<div><div class="figure" id="id102" style="text-align: center">
<span id="fig-gbn-win-1"/><p><img src="../Images/fb17e125cf960286b8258b6251471572.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-5d002e7b389ca36e47cf6c9f674938a0d08593cb.png"/></p>
<p><span class="caption-number">Fig. 216 </span><span class="caption-text">Go-back-n transfer from A to D, window of one segment</span></p>
</div></div></blockquote>
<p>Consider now a window of two segments. Host <cite>A</cite> can send two segments within 2 msec on its 1 Mbps link. If the first segment is sent at time <span class="math notranslate nohighlight">\(t_{0}\)</span>, it reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. Host <cite>D</cite> replies with an acknowledgment that opens the sending window on host <cite>A</cite> and enables it to transmit a new segment. In the meantime, the second segment was buffered by router <cite>R1</cite>. It reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+6\)</span> and an acknowledgment is returned. With a window of two segments, host <cite>A</cite> transmits at roughly 500 Kbps, i.e. the transmission rate of the bottleneck link.</p>
<blockquote>
<div><div class="figure" id="id103" style="text-align: center"><p><img src="../Images/972780cbffde5435003f04c4edfabc56.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-d13039c2482f96c3496fdd84e458374e31963a74.png"/></p>
<p><span class="caption-number">Fig. 217 </span><span class="caption-text">Go-back-n transfer from A to D, window of two segments</span></p>
</div></div></blockquote>
<p>Our last example is a window of four segments. These segments are sent at <span class="math notranslate nohighlight">\(t_{0}\)</span>, <span class="math notranslate nohighlight">\(t_{0}+1\)</span>, <span class="math notranslate nohighlight">\(t_{0}+2\)</span> and <span class="math notranslate nohighlight">\(t_{0}+3\)</span>. The first segment reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. Host <cite>D</cite> replies to this segment by sending an acknowledgment that enables host <cite>A</cite> to transmit its fifth segment. This segment reaches router <cite>R1</cite> at <span class="math notranslate nohighlight">\(t_{0}+5\)</span>. At that time, router <cite>R1</cite> is transmitting the third segment to router <cite>R2</cite> and the fourth segment is still in its buffers. At time <span class="math notranslate nohighlight">\(t_{0}+6\)</span>, host <cite>D</cite> receives the second segment and returns the corresponding acknowledgment. This acknowledgment enables host <cite>A</cite> to send its sixth segment. This segment reaches router <cite>R1</cite> at roughly <span class="math notranslate nohighlight">\(t_{0}+7\)</span>. At that time, the router starts to transmit the fourth segment to router <cite>R2</cite>. Since link <cite>R1-R2</cite> can only sustain 500 Kbps, packets will accumulate in the buffers of <cite>R1</cite>. On average, there will be two packets waiting in the buffers of <cite>R1</cite>. The presence of these two packets will induce an increase of the round-trip-time as measured by the transport protocol. While the first segment was acknowledged within 4 msec, the fifth segment (<cite>data(4)</cite>) that was transmitted at time <span class="math notranslate nohighlight">\(t_{0}+4\)</span> is only acknowledged at time <span class="math notranslate nohighlight">\(t_{0}+11\)</span>. On average, the sender transmits at 500 Kbps, but the utilization of a large window induces a longer delay through the network.</p>
<blockquote>
<div><div class="figure" id="id104" style="text-align: center"><p><img src="../Images/6fac88cb38e8b98800e49c18ffa34a0c.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-b9493099e8639557dc24065f10c48391e80c82fd.png"/></p>
<p><span class="caption-number">Fig. 218 </span><span class="caption-text">Go-back-n transfer from A to D, window of four segments</span></p>
</div></div></blockquote>
<p id="index-42">From the above example, we can adjust the transmission rate by adjusting the sending window of a reliable transport protocol. A reliable transport protocol cannot send data faster than <span class="math notranslate nohighlight">\(\frac{window}{rtt}\)</span> segments per second where <span class="math notranslate nohighlight">\(window\)</span> is the current sending window. To control the transmission rate, we introduce a <cite>congestion window</cite>. This congestion window limits the sending window. At any time, the sending window is restricted to <span class="math notranslate nohighlight">\(\min(swin,cwin)\)</span>, where <cite>swin</cite> is the sending window and <cite>cwin</cite> the current <cite>congestion window</cite>. Of course, the window is further constrained by the receive window advertised by the remote peer. With the utilization of a congestion window, a simple reliable transport protocol that uses fixed size segments could implement <cite>AIMD</cite> as follows.</p>
<p>For the <cite>Additive Increase</cite> part our simple protocol would simply increase its <cite>congestion window</cite> by one segment every round-trip-time. The
<cite>Multiplicative Decrease</cite> part of <cite>AIMD</cite> could be implemented by halving the congestion window when congestion is detected. For simplicity, we assume that congestion is detected thanks to a binary feedback and that no segments are lost. We will discuss in more details how losses affect a real transport protocol like TCP in later sections.</p>
<p>A congestion control scheme for our simple transport protocol could be implemented as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># Initialisation</span>
<span class="n">cwin</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># congestion window measured in segments</span>

<span class="c1"># Ack arrival</span>
<span class="k">if</span> <span class="n">ack_received</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">newack</span><span class="p">:</span>  <span class="c1"># new ack, no congestion</span>
        <span class="c1"># increase cwin by one every rtt</span>
        <span class="n">cwin</span> <span class="o">=</span> <span class="n">cwin</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">cwin</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># no increase</span>

<span class="k">if</span> <span class="n">congestion_detected</span><span class="p">:</span>
    <span class="n">cwin</span> <span class="o">=</span> <span class="n">cwin</span> <span class="o">/</span> <span class="mi">2</span> <span class="c1"># only once per rtt</span>
</pre></div>
</div>
<p>In the above pseudocode, <cite>cwin</cite> contains the congestion window stored as a real number of segments. This congestion window is updated upon the arrival of each acknowledgment and when congestion is detected. For simplicity, we assume that <cite>cwin</cite> is stored as a floating point number but only full segments can be transmitted.</p>
<p>As an illustration, let us consider the network scenario above and assume that the router implements the DECBit binary feedback scheme <a class="reference internal" href="../bibliography.html#rj1995" id="id53"><span>[RJ1995]</span></a>. This scheme uses a form of Forward Explicit Congestion Notification and a router marks the congestion bit in arriving packets when its buffer contains one or more packets. In figure <a class="reference internal" href="#fig-gbn-decbit"><span class="std std-numref">Fig. 219</span></a>, we use a <cite>*</cite> to indicate a marked packet.</p>
<blockquote>
<div><div class="figure" id="id105" style="text-align: center">
<span id="fig-gbn-decbit"/><p><img src="../Images/2b9761249f9ab8572a0fc4b3416b6752.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-c2a62409e962fdfb2fe7754a330fce96bb7f3bd0.png"/></p>
<p><span class="caption-number">Fig. 219 </span><span class="caption-text">Go-back-n transfer from A to D, with AIMD congestion control and DecBit binary feedback scheme</span></p>
</div></div></blockquote>
<p>When the connection starts, its congestion window is set to one segment. Segment <cite>S0</cite> is sent an acknowledgment at roughly <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. The congestion window is increased by one segment and <cite>S1</cite> and <cite>S2</cite> are transmitted at time <span class="math notranslate nohighlight">\(t_{0}+4\)</span> and <span class="math notranslate nohighlight">\(t_{0}+5\)</span>. The corresponding acknowledgments are received at times <span class="math notranslate nohighlight">\(t_{0}+8\)</span> and <span class="math notranslate nohighlight">\(t_{0}+10\)</span>. Upon reception of this last acknowledgment, the congestion window reaches <cite>3</cite> and segments can be sent (<cite>S4</cite> and <cite>S5</cite>). When segment <cite>S6</cite> reaches router <cite>R1</cite>, its buffers already contain <cite>S5</cite>. The packet containing <cite>S6</cite> is thus marked to inform the sender of the congestion. Note that the sender will only notice the congestion once it receives the corresponding acknowledgment at <span class="math notranslate nohighlight">\(t_{0}+18\)</span>. In the meantime, the congestion window continues to increase. At <span class="math notranslate nohighlight">\(t_{0}+16\)</span>, upon reception of the acknowledgment for <cite>S5</cite>, it reaches <cite>4</cite>. When congestion is detected, the congestion window is decreased down to <cite>2</cite>. This explains the idle time between the reception of the acknowledgment for <cite>S*6</cite> and the transmission of <cite>S10</cite>.</p>
<p>In practice, a router is connected to multiple input links. Figure <a class="reference internal" href="#fig-2hosts-bottleneck"><span class="std std-numref">Fig. 220</span></a> shows an example with two hosts.</p>
<blockquote>
<div><div class="figure" id="id106" style="text-align: center">
<span id="fig-2hosts-bottleneck"/><p><img src="../Images/54044b2beb542c9ff304fe654e7cbe2e.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-699cdd177204f453f241862f3b959ea743b24e53.png"/></p>
<p><span class="caption-number">Fig. 220 </span><span class="caption-text">A simple network with hosts sharing a bottleneck</span></p>
</div><div class="figure" id="id107" style="text-align: center"><p><img src="../Images/14805061572e2ea7a08e10338a022ff8.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-966439ccf02ecbf0381106d45a0dcbcfb8a1412d.png"/></p>
<p><span class="caption-number">Fig. 221 </span><span class="caption-text">Sharing the bottleneck link between different inputs</span></p>
</div></div></blockquote>
<p>In general, the links have a non-zero delay. This is illustrated in the figure below where a delay has been added on the link between <cite>R</cite> and <cite>C</cite>.</p>
<blockquote>
<div><div class="figure" id="id108" style="text-align: center"><p><img src="../Images/515eaae674689b5424d8fbcf3dfe9c0e.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-db6942ecfbc5147fe81ef57fed06977ad34ff651.png"/></p>
<p><span class="caption-number">Fig. 222 </span><span class="caption-text">Sharing the bottleneck link between different inputs</span></p>
</div></div></blockquote>
</section>
<section id="tcpcongestion">
<span id="id54"/><h3>Congestion control<a class="headerlink" href="#tcpcongestion" title="Link to this heading">#</a></h3>
<p>In an internetwork, i.e. a networking composed of different types of networks (such as the Internet), congestion control could be implemented either in the network layer or the transport layer. The congestion problem was clearly identified in the later 1980s and the researchers who developed techniques to solve the problem opted for a solution in the transport layer. Adding congestion control to the transport layer makes sense since this layer provides a reliable data transfer and avoiding congestion is a factor in this reliable delivery. The transport layer already deals with heterogeneous networks thanks to its <cite>self-clocking</cite> property that we have already described. In this section, we explain how congestion control has been added to TCP and how this mechanism could be improved in the future.</p>
<p>The TCP congestion control scheme was initially proposed by <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id55"><span>[Jacobson1988]</span></a>. The current specification may be found in <span class="target" id="index-43"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc5681.html"><strong>RFC 5681</strong></a>. TCP relies on <cite>Additive Increase and Multiplicative Decrease (AIMD)</cite>. To implement <a class="reference internal" href="../glossary.html#term-AIMD"><span class="xref std std-term">AIMD</span></a>, a TCP host must be able to control its transmission rate. A first approach would be to use timers and adjust their expiration times in function of the rate imposed by <a class="reference internal" href="../glossary.html#term-AIMD"><span class="xref std std-term">AIMD</span></a>. Unfortunately, maintaining such timers for a large number of TCP connections can be difficult. Instead, <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> noted that the rate of TCP congestion can be artificially controlled by constraining its sending window. A TCP connection cannot send data faster than <span class="math notranslate nohighlight">\(\frac{window}{rtt}\)</span> where <span class="math notranslate nohighlight">\(window\)</span> is the minimum between the host’s sending window and the window advertised by the receiver.</p>
<p>TCP’s congestion control scheme is based on a <cite>congestion window</cite>. The current value of the congestion window (<cite>cwnd</cite>) is stored in the TCB of each TCP connection and the window that can be used by the sender is constrained by <span class="math notranslate nohighlight">\(\min(cwnd,rwin,swin)\)</span> where <span class="math notranslate nohighlight">\(swin\)</span> is the current sending window and <span class="math notranslate nohighlight">\(rwin\)</span> the last received receive window. The <cite>Additive Increase</cite> part of the TCP congestion control increments the congestion window by <a class="reference internal" href="../glossary.html#term-MSS"><span class="xref std std-term">MSS</span></a> bytes every round-trip-time. In the TCP literature, this phase is often called the <cite>congestion avoidance</cite> phase. The <cite>Multiplicative Decrease</cite> part of the TCP congestion control divides the current value of the congestion window once congestion has been detected.</p>
<p>When a TCP connection begins, the sending host does not know whether the part of the network that it uses to reach the destination is congested or not. To avoid causing too much congestion, it must start with a small congestion window. <a class="reference internal" href="../bibliography.html#jacobson1988" id="id56"><span>[Jacobson1988]</span></a> recommends an initial window of MSS bytes. As the additive increase part of the TCP congestion control scheme increments the congestion window by MSS bytes every round-trip-time, the TCP connection may have to wait many round-trip-times before being able to efficiently use the available bandwidth. This is especially important in environments where the <span class="math notranslate nohighlight">\(bandwidth \times rtt\)</span> product is high. To avoid waiting too many round-trip-times before reaching a congestion window that is large enough to efficiently utilize the network, the TCP congestion control scheme includes the <cite>slow-start</cite> algorithm. The objective of the TCP <cite>slow-start</cite> phase is to quickly reach an acceptable value for the <cite>cwnd</cite>. During <cite>slow-start</cite>, the congestion window is doubled every round-trip-time. The <cite>slow-start</cite> algorithm uses an additional variable in the TCB : <cite>ssthresh</cite> (<cite>slow-start threshold</cite>). The <cite>ssthresh</cite> is an estimation of the last value of the <cite>cwnd</cite> that did not cause congestion. It is initialized at the sending window and is updated after each congestion event.</p>
<p>A key question that must be answered by any congestion control scheme is how congestion is detected. The first implementations of the TCP congestion control scheme opted for a simple and pragmatic approach : packet losses indicate congestion. If the network is congested, router buffers are full and packets are discarded. In wired networks, packet losses are mainly caused by congestion. In wireless networks, packets can be lost due to transmission errors and for other reasons that are independent of congestion. TCP already detects segment losses to ensure a reliable delivery. The TCP congestion control scheme distinguishes between two types of congestion :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>mild congestion</cite>. TCP considers that the network is lightly congested if it receives three duplicate acknowledgments and performs a fast retransmit. If the fast retransmit is successful, this implies that only one segment has been lost. In this case, TCP performs multiplicative decrease and the congestion window is divided by <cite>2</cite>. The slow-start threshold is set to the new value of the congestion window.</p></li>
<li><p><cite>severe congestion</cite>. TCP considers that the network is severely congested when its retransmission timer expires. In this case, TCP retransmits the first segment, sets the slow-start threshold to 50% of the congestion window. The congestion window is reset to its initial value and TCP performs a slow-start.</p></li>
</ul>
</div></blockquote>
<p>The figure below illustrates the evolution of the congestion window when there is severe congestion. At the beginning of the connection, the sender performs <cite>slow-start</cite> until the first segments are lost and the retransmission timer expires. At this time, the <cite>ssthresh</cite> is set to half of the current congestion window and the congestion window is reset at one segment. The lost segments are retransmitted as the sender again performs slow-start until the congestion window reaches the <cite>sshtresh</cite>. It then switches to congestion avoidance and the congestion window increases linearly until segments are lost and the retransmission timer expires.</p>
<figure class="align-center" id="id109">
<a class="reference internal image-reference" href="../_images/tcp-congestion-severe.png"><img alt="../_images/tcp-congestion-severe.png" src="../Images/8bb9ad90fd07bc82d9cba2b40a1481a6.png" style="width: 461.99999999999994px; height: 186.89999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-severe.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 223 </span><span class="caption-text">Evaluation of the TCP congestion window with severe congestion</span><a class="headerlink" href="#id109" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The figure below illustrates the evolution of the congestion window when the network is lightly congested and all lost segments can be retransmitted using fast retransmit. The sender begins with a slow-start. A segment is lost but successfully retransmitted by a fast retransmit. The congestion window is divided by 2 and the sender immediately enters congestion avoidance as this was a mild congestion.</p>
<figure class="align-center" id="id110">
<a class="reference internal image-reference" href="../_images/tcp-congestion-mild.png"><img alt="../_images/tcp-congestion-mild.png" src="../Images/118024671401b1ade2c8daa21527e64f.png" style="width: 454.99999999999994px; height: 178.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-mild.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 224 </span><span class="caption-text">Evaluation of the TCP congestion window when the network is lightly congested</span><a class="headerlink" href="#id110" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Most TCP implementations update the congestion window when they receive an acknowledgment. If we assume that the receiver acknowledges each received segment and the sender only sends MSS sized segments, the TCP congestion control scheme can be implemented using the simplified pseudo-code <a class="footnote-reference brackets" href="#fwrap" id="id57" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> below. This pseudocode includes the optimization proposed in <span class="target" id="index-44"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3042.html"><strong>RFC 3042</strong></a> that allows a sender to send new unsent data upon reception of the first or second duplicate acknowledgment. The reception of each of these acknowledgments indicates that one segment has left the network and thus additional data can be sent without causing more congestion. Note that the congestion window is <em>not</em> increased upon reception of these first duplicate acknowledgments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># Initialization</span>
<span class="n">cwnd</span> <span class="o">=</span> <span class="n">MSS</span>  <span class="c1"># congestion window in bytes</span>
<span class="n">ssthresh</span><span class="o">=</span> <span class="n">swin</span> <span class="c1"># in bytes</span>

<span class="c1"># Ack arrival</span>
<span class="k">if</span> <span class="n">tcp</span><span class="o">.</span><span class="n">ack</span> <span class="o">&gt;</span> <span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">:</span>  <span class="c1"># new ack, no congestion</span>
    <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not currently recovering from loss</span>
        <span class="k">if</span> <span class="n">cwnd</span> <span class="o">&lt;</span> <span class="n">ssthresh</span><span class="p">:</span>
            <span class="c1"># slow-start : quickly increase cwnd</span>
            <span class="c1"># double cwnd every rtt</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># congestion avoidance : slowly increase cwnd</span>
            <span class="c1"># increase cwnd by one mss every rtt</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span> <span class="o">*</span> <span class="p">(</span><span class="n">MSS</span> <span class="o">/</span> <span class="n">cwnd</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># recovering from loss</span>
        <span class="n">cwnd</span> <span class="o">=</span> <span class="n">ssthresh</span>  <span class="c1"># deflate cwnd RFC5681</span>
        <span class="n">dupacks</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># duplicate or old ack</span>
    <span class="k">if</span> <span class="n">tcp</span><span class="o">.</span><span class="n">ack</span> <span class="o">==</span> <span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">:</span>  <span class="c1"># duplicate acknowledgment</span>
        <span class="n">dupacks</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">send_next_unacked_segment</span>  <span class="c1"># RFC3042</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">retransmitsegment</span><span class="p">(</span><span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">)</span>
            <span class="n">ssthresh</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">cwnd</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">MSS</span><span class="p">)</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">ssthresh</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># RFC5681</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span>  <span class="c1"># inflate cwnd</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># ack for old segment, ignored</span>
        <span class="k">pass</span>

<span class="n">Expiration</span> <span class="n">of</span> <span class="n">the</span> <span class="n">retransmission</span> <span class="n">timer</span><span class="p">:</span>
    <span class="n">send</span><span class="p">(</span><span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">)</span>  <span class="c1"># retransmit first lost segment</span>
    <span class="n">sshtresh</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">cwnd</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">MSS</span><span class="p">)</span>
    <span class="n">cwnd</span> <span class="o">=</span> <span class="n">MSS</span>
</pre></div>
</div>
<p>Furthermore when a TCP connection has been idle for more than its current retransmission timer, it should reset its congestion window to the congestion window size that it uses when the connection begins, as it no longer knows the current congestion state of the network.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Initial congestion window</p>
<p>The original TCP congestion control mechanism proposed in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id58"><span>[Jacobson1988]</span></a> recommended that each TCP connection should begin by setting <span class="math notranslate nohighlight">\(cwnd=MSS\)</span>. However, in today’s higher bandwidth networks, using such a small initial congestion window severely affects the performance for short TCP connections, such as those used by web servers. In 2002, <span class="target" id="index-45"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3390.html"><strong>RFC 3390</strong></a> allowed an initial congestion window of about 4 KBytes, which corresponds to 3 segments in many environments. Recently, researchers from Google proposed to further increase the initial window up to 15 KBytes <a class="reference internal" href="../bibliography.html#drc-2010" id="id59"><span>[DRC+2010]</span></a>. The measurements that they collected show that this increase would not significantly increase congestion but would significantly reduce the latency of short HTTP responses. Unsurprisingly, the chosen initial window corresponds to the average size of an HTTP response from a search engine. This proposed modification has been adopted in <span class="target" id="index-46"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc6928.html"><strong>RFC 6928</strong></a> and TCP implementations support it.</p>
</div>
<section id="controlling-congestion-without-losing-data">
<h4>Controlling congestion without losing data<a class="headerlink" href="#controlling-congestion-without-losing-data" title="Link to this heading">#</a></h4>
<p>In today’s Internet, congestion is controlled by regularly sending packets at a higher rate than the network capacity. These packets fill the buffers of the routers and are eventually discarded. But shortly after, TCP senders retransmit packets containing exactly the same data. This is potentially a waste of resources since these successive retransmissions consume resources upstream of the router that discards the packets. Packet losses are not the only signal to detect congestion inside the network. An alternative is to allow routers to explicitly indicate their current level of congestion when forwarding packets. This approach was proposed in the late 1980s <a class="reference internal" href="../bibliography.html#rj1995" id="id60"><span>[RJ1995]</span></a> and used in some networks. Unfortunately, it took almost a decade before the Internet community agreed to consider this approach. In the mean time, a large number of TCP implementations and routers were deployed on the Internet.</p>
<p>As explained earlier, Explicit Congestion Notification <span class="target" id="index-47"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> improves the detection of congestion by allowing routers to explicitly mark packets when they are lightly congested. In theory, a single bit in the packet header <a class="reference internal" href="../bibliography.html#rj1995" id="id61"><span>[RJ1995]</span></a> is sufficient to support this congestion control scheme. When a host receives a marked packet, it returns the congestion information to the source that adapts its transmission rate accordingly. Although the idea is relatively simple, deploying it on the entire Internet has proven to be challenging <a class="reference internal" href="../bibliography.html#knt2013" id="id62"><span>[KNT2013]</span></a>. It is interesting to analyze the different factors that have hindered the deployment of this technique.</p>
<p>The first difficulty in adding Explicit Congestion Notification (ECN) in TCP/IP network was to modify the format of the network packet and transport segment headers to carry the required information. In the network layer, one bit was required to allow the routers to mark the packets they forward during congestion periods. In the IP network layer, this bit is called the <cite>Congestion Experienced</cite> (<cite>CE</cite>) bit and is part of the packet header. However, using a single bit to mark packets is not sufficient. Consider a simple scenario with two sources, one congested router and one destination. Assume that the first sender and the destination support ECN, but not the second sender. If the router is congested it will mark packets from both senders. The first sender will react to the packet markings by reducing its transmission rate. However since the second sender does not support ECN, it will not react to the markings. Furthermore, this sender could continue to increase its transmission rate, which would lead to more packets being marked and the first source would decrease again its transmission rate, … In the end, the sources that implement ECN are penalized compared to the sources that do not implement it. This unfairness issue is a major hurdle to widely deploy ECN on the public Internet <a class="footnote-reference brackets" href="#fprivate" id="id63" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>. The solution proposed in <span class="target" id="index-48"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> to deal with this problem is to use a second bit in the network packet header. This bit, called the <cite>ECN-capable transport</cite> (ECT) bit, indicates whether the packet contains a segment produced by a transport protocol that supports ECN or not. Transport protocols that support ECN set the ECT bit in all packets. When a router is congested, it first verifies whether the ECT bit is set. In this case, the CE bit of the packet is set to indicate congestion. Otherwise, the packet is discarded. This eases the deployment of ECN <a class="footnote-reference brackets" href="#fecnnonce" id="id64" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>.</p>
<p>The second difficulty is how to allow the receiver to inform the sender of the reception of network packets marked with the <cite>CE</cite> bit. In reliable transport protocols like TCP and SCTP, the acknowledgments can be used to provide this feedback. For TCP, two options were possible : change some bits in the TCP segment header or define a new TCP option to carry this information. The designers of ECN opted for reusing spare bits in the TCP header. More precisely, two TCP flags have been added in the TCP header to support ECN. The <cite>ECN-Echo</cite> (ECE) is set in the acknowledgments when the <cite>CE</cite> was set in packets received on the forward path.</p>
<figure class="align-default" id="id111">
<a class="reference internal image-reference" href="../_images/tcp-enc.svg"><img alt="../_images/tcp-enc.svg" src="../Images/0b92b95f1c0dadde63905a28574e87cb.png" style="width: 664.8px; height: 115.19999999999999px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-enc.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 225 </span><span class="caption-text">The TCP flags</span><a class="headerlink" href="#id111" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The third difficulty is to allow an ECN-capable sender to detect whether the remote host also supports ECN. This is a classical negotiation of extensions to a transport protocol. In TCP, this could have been solved by defining a new TCP option used during the three-way handshake. To avoid wasting space in the TCP options, the designers of ECN opted in <span class="target" id="index-49"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> for using the <cite>ECN-Echo</cite> and <cite>CWR</cite> bits in the TCP header to perform this negotiation. In the end, the result is the same with fewer bits exchanged.</p>
<p>Thanks to the <cite>ECT</cite>, <cite>CE</cite> and <cite>ECE</cite>, routers can mark packets during congestion and receivers can return the congestion information back to the TCP senders. However, these three bits are not sufficient to allow a server to reliably send the <cite>ECE</cite> bit to a TCP sender. TCP acknowledgments are not sent reliably. A TCP acknowledgment always contains the next expected sequence number. Since TCP acknowledgments are cumulative, the loss of one acknowledgment is recovered by the correct reception of a subsequent acknowledgment.</p>
<p>If TCP acknowledgments are overloaded to carry the <cite>ECE</cite> bit, the situation is different. Consider the example shown in the figure below. A client sends packets to a server through a router. In the example below, the first packet is marked. The server returns an acknowledgment with the <cite>ECE</cite> bit set. Unfortunately, this acknowledgment is lost and never reaches the client. Shortly after, the server sends a data segment that also carries a cumulative acknowledgment. This acknowledgment confirms the reception of the data to the client, but it did not receive the congestion information through the <cite>ECE</cite> bit.</p>
<blockquote>
<div><p class="mscgen">
<img src="../Images/d2ed1589241adeb9e699082587d1916d.png" alt="msc {&#10;client [label=&quot;client&quot;, linecolour=black],&#10;router [label=&quot;router&quot;, linecolour=black],&#10;server [label=&quot;server&quot;, linecolour=black];&#10;&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;server=&gt;router [ label = &quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;router -x client [label=&quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;|||;&#10;server=&gt;router [ label = &quot;data[seq=x,ack=2,ECE=0,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;client [ label = &quot;data[seq=x,ack=2,ECE=0,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client-&gt;server [linecolour=white];&#10;}" usemap="#700c52eed1dd99ea5abd5216ffd2e044e6fee931" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/mscgen-700c52eed1dd99ea5abd5216ffd2e044e6fee931.png"/>
<map id="700c52eed1dd99ea5abd5216ffd2e044e6fee931" name="700c52eed1dd99ea5abd5216ffd2e044e6fee931"/></p>
</div></blockquote>
<p>To solve this problem, <span class="target" id="index-50"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> uses an additional bit in the TCP header : the <cite>Congestion Window Reduced</cite> (CWR) bit.</p>
<blockquote>
<div><p class="mscgen">
<img src="../Images/36533b7c4d47ba50cac29ebf6ebcc1f1.png" alt="msc {&#10;client [label=&quot;client&quot;, linecolour=black],&#10;router [label=&quot;router&quot;, linecolour=black],&#10;server [label=&quot;server&quot;, linecolour=black];&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;server=&gt;router [ label = &quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;router -x client [label=&quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;|||;&#10;server=&gt;router [ label = &quot;data[seq=x,ack=2,ECE=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;client [ label = &quot;data[seq=x,ack=2,ECE=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0,CWR=1]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1,CWR=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client-&gt;server [linecolour=white];&#10;}" usemap="#d60c580a7379dbdd26f90fd2eead54f832ee6ad6" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/mscgen-d60c580a7379dbdd26f90fd2eead54f832ee6ad6.png"/>
<map id="d60c580a7379dbdd26f90fd2eead54f832ee6ad6" name="d60c580a7379dbdd26f90fd2eead54f832ee6ad6"/></p>
</div></blockquote>
<p>The <cite>CWR</cite> bit of the TCP header provides some form of acknowledgment for the <cite>ECE</cite> bit. When a TCP receiver detects a packet marked with the <cite>CE</cite> bit, it sets the <cite>ECE</cite> bit in all segments that it returns to the sender. Upon reception of an acknowledgment with the <cite>ECE</cite> bit set, the sender reduces its congestion window to reflect a mild congestion and sets the <cite>CWR</cite> bit. This bit remains set as long as the segments received contained the <cite>ECE</cite> bit set. A sender should only react once per round-trip-time to marked packets.</p>
<p>The last point that needs to be discussed about Explicit Congestion Notification is the algorithm that is used by routers to detect congestion. On a router, congestion manifests itself by the number of packets that are stored inside the router buffers. As explained earlier, we need to distinguish between two types of routers :</p>
<blockquote>
<div><ul class="simple">
<li><p>routers that have a single FIFO queue</p></li>
<li><p>routers that have several queues served by a round-robin scheduler</p></li>
</ul>
</div></blockquote>
<p>Routers that use a single queue measure their buffer occupancy as the number of bytes of packets stored in the queue <a class="footnote-reference brackets" href="#fslot" id="id65" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. A first method to detect congestion is to measure the instantaneous buffer occupancy and consider the router to be congested as soon as this occupancy is above a threshold. Typical values of the threshold could be 40% of the total buffer. Measuring the instantaneous buffer occupancy is simple since it only requires one counter. However, this value is fragile from a control viewpoint since it changes frequently. A better solution is to measure the <em>average</em> buffer occupancy and consider the router to be congested when this average occupancy is too high. Random Early Detection (RED) <a class="reference internal" href="../bibliography.html#fj1993" id="id66"><span>[FJ1993]</span></a> is an algorithm that was designed to support Explicit Congestion Notification. In addition to measuring the average buffer occupancy, it also uses probabilistic marking. When the router is congested, the arriving packets are marked with a probability that increases with the average buffer occupancy. The main advantage of using probabilistic marking instead of marking all arriving packets is that flows will be marked in proportion of the number of packets that they transmit. If the router marks 10% of the arriving packets when congested, then a large flow that sends hundred packets per second will be marked 10 times while a flow that only sends one packet per second will not be marked. This probabilistic marking allows marking packets in proportion of their usage of the network resources.</p>
<p>If the router uses several queues served by a scheduler, the situation is different. If a large and a small flow are competing for bandwidth, the scheduler will already favor the small flow that is not using its fair share of the bandwidth. The queue for the small flow will be almost empty while the queue for the large flow will build up. On routers using such schedulers, a good way of marking the packets is to set a threshold on the occupancy of each queue and mark the packets that arrive in a particular queue as soon as its occupancy is above the configured threshold.</p>
</section>
<section id="modeling-tcp-congestion-control">
<h4>Modeling TCP congestion control<a class="headerlink" href="#modeling-tcp-congestion-control" title="Link to this heading">#</a></h4>
<p>Thanks to its congestion control scheme, TCP adapts its transmission rate to the losses that occur in the network. Intuitively, the TCP transmission rate decreases when the percentage of losses increases. Researchers have proposed detailed models that allow the prediction of the throughput of a TCP connection when losses occur <a class="reference internal" href="../bibliography.html#msmo1997" id="id67"><span>[MSMO1997]</span></a> . To have some intuition about the factors that affect the performance of TCP, let us consider a very simple model. Its assumptions are not completely realistic, but it gives us good intuition without requiring complex mathematics.</p>
<p>This model considers a hypothetical TCP connection that suffers from equally spaced segment losses. If <span class="math notranslate nohighlight">\(p\)</span> is the segment loss ratio, then the TCP connection successfully transfers <span class="math notranslate nohighlight">\(\frac{1}{p}-1\)</span> segments and the next segment is lost. If we ignore the slow-start at the beginning of the connection, TCP in this environment is always in congestion avoidance as there are only isolated losses that can be recovered by using fast retransmit. The evolution of the congestion window is thus as shown in the figure below. Note that the <cite>x-axis</cite> of this figure represents time measured in units of one round-trip-time, which is supposed to be constant in the model, and the <cite>y-axis</cite> represents the size of the congestion window measured in MSS-sized segments.</p>
<figure class="align-center" id="id112">
<a class="reference internal image-reference" href="../_images/tcp-congestion-regular.png"><img alt="../_images/tcp-congestion-regular.png" src="../Images/aba5ecafe63f482ae07b7a280f8f3275.png" style="width: 419.29999999999995px; height: 128.79999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-regular.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 226 </span><span class="caption-text">Evolution of the congestion window with regular losses</span><a class="headerlink" href="#id112" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As the losses are equally spaced, the congestion window always starts at some value (<span class="math notranslate nohighlight">\(\frac{W}{2}\)</span>), and is incremented by one MSS every round-trip-time until it reaches twice this value (<cite>W</cite>). At this point, a segment is retransmitted and the cycle starts again. If the congestion window is measured in MSS-sized segments, a cycle lasts <span class="math notranslate nohighlight">\(\frac{W}{2}\)</span> round-trip-times. The bandwidth of the TCP connection is the number of bytes that have been transmitted during a given period of time. During a cycle, the number of segments that are sent on the TCP connection is equal to the area of the yellow trapeze in the figure. Its area is thus :</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(area=(\frac{W}{2})^2 + \frac{1}{2} \times (\frac{W}{2})^2 = \frac{3 \times W^2}{8}\)</span></p>
</div></blockquote>
<p>However, given the regular losses that we consider, the number of segments that are sent between two losses (i.e. during a cycle) is by definition equal to <span class="math notranslate nohighlight">\(\frac{1}{p}\)</span>. Thus, <span class="math notranslate nohighlight">\(W=\sqrt{\frac{8}{3 \times p}}=\frac{k}{\sqrt{p}}\)</span>. The throughput (in bytes per second) of the TCP connection is equal to the number of segments transmitted divided by the duration of the cycle :</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Throughput=\frac{area \times MSS}{time} = \frac{ \frac{3 \times W^2}{8}}{\frac{W}{2} \times rtt}\)</span>
or, after having eliminated <cite>W</cite>, <span class="math notranslate nohighlight">\(Throughput=\sqrt{\frac{3}{2}} \times \frac{MSS}{rtt \times \sqrt{p}}\)</span></p>
</div></blockquote>
<p>More detailed models and the analysis of simulations have shown that a first order model of the TCP throughput when losses occur was <span class="math notranslate nohighlight">\(Throughput \approx \frac{k \times MSS}{rtt \times \sqrt{p}}\)</span>. This is an important result which shows that :</p>
<blockquote>
<div><ul class="simple">
<li><p>TCP connections with a small round-trip-time can achieve a higher throughput than TCP connections having a longer round-trip-time when losses occur. This implies that the TCP congestion control scheme is not completely fair since it favors the connections that have the shorter round-trip-times.</p></li>
<li><p>TCP connections that use a large MSS can achieve a higher throughput that the TCP connections that use a shorter MSS. This creates another source of unfairness between TCP connections. However, it should be noted that today most hosts are using almost the same MSS, roughly 1460 bytes.</p></li>
</ul>
</div></blockquote>
<p>In general, the maximum throughput that can be achieved by a TCP connection depends on its maximum window size and the round-trip-time if there are no losses. If there are losses, it depends on the MSS, the round-trip-time and the loss ratio.</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Throughput&lt;\min(\frac{window}{rtt},\frac{k \times MSS}{rtt \times \sqrt{p}})\)</span></p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The TCP congestion control zoo</p>
<p>The first TCP congestion control scheme was proposed by <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id68"><span>[Jacobson1988]</span></a>. In addition to writing the scientific paper, <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> also implemented the slow-start and congestion avoidance schemes in release 4.3 <cite>Tahoe</cite> of the BSD Unix distributed by the University of Berkeley. Later, he improved the congestion control by adding the fast retransmit and the fast recovery mechanisms in the <cite>Reno</cite> release of 4.3 BSD Unix. Since then, many researchers have proposed, simulated and implemented modifications to the TCP congestion control scheme. Some of these modifications are still used today, e.g. :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>NewReno</cite> (<span class="target" id="index-51"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3782.html"><strong>RFC 3782</strong></a>), which was proposed as an improvement of the fast recovery mechanism in the <cite>Reno</cite> implementation.</p></li>
<li><p><cite>TCP Vegas</cite>, which uses changes in the round-trip-time to estimate congestion in order to avoid it <a class="reference internal" href="../bibliography.html#bop1994" id="id69"><span>[BOP1994]</span></a>. This is one of the examples of the delay-based congestion control algorithms. A Vegas sender continuously measures the evolution of the round-trip-time and slows down when the round-trip-time increases significantly. This enables Vegas to prevent congestion when used alone. Unfortunately, if Vegas senders compete with more aggressive TCP congestion control schemes that only react to losses, Vegas senders may have difficulties to use their fair share of the available bandwidth.</p></li>
<li><p><cite>CUBIC</cite>, which was designed for high bandwidth links and is the default congestion control scheme in Linux since the Linux 2.6.19 kernel <a class="reference internal" href="../bibliography.html#hrx2008" id="id70"><span>[HRX2008]</span></a>. It is now used by several operating systems and is becoming the default congestion control scheme <span class="target" id="index-52"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc8312.html"><strong>RFC 8312</strong></a>. A key difference between CUBIC and the TCP congestion control scheme described in this chapter is that CUBIC is much more aggressive when probing the network. Instead of relying on additive increase after a fast recovery, a CUBIC sender adjusts its congestion by using a cubic function. Thanks to this function, the congestion windows grows faster. This is particularly important in high-bandwidth delay networks.</p></li>
<li><p><cite>BBR</cite>, which is being developed by Google researchers and is included in recent Linux kernels <a class="reference internal" href="../bibliography.html#ccg-2016" id="id71"><span>[CCG+2016]</span></a>. BBR periodically estimates the available bandwidth and the round-trip-times. To adapt to changes in network conditions, BBR regularly tries to send at 1.25 times the current bandwidth. This enables BBR senders to probe the network, but can also cause large amount of losses. Recent scientific articles indicate that BBR is unfair to other congestion control schemes in specific conditions <a class="reference internal" href="../bibliography.html#wmss2019" id="id72"><span>[WMSS2019]</span></a>.</p></li>
</ul>
</div></blockquote>
<p>A wide range of congestion control schemes have been proposed in the scientific literature and several of them have been widely deployed. A detailed comparison of these congestion control schemes is outside the scope of this chapter. A recent survey paper describing many of the implemented TCP congestion control schemes may be found in <a class="reference internal" href="../bibliography.html#tku2019" id="id73"><span>[TKU2019]</span></a>.</p>
</div>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fbufferbloat" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>There are still some vendors that try to put as many buffers as possible on their routers. A recent example is the buffer bloat problem that plagues some low-end Internet routers <a class="reference internal" href="../bibliography.html#gn2011" id="id74"><span>[GN2011]</span></a>.</p>
</aside>
<aside class="footnote brackets" id="fpps" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>Some examples of the performance of various types of commercial networks nodes (routers and switches) may be found in <a class="reference external" href="http://www.cisco.com/web/partners/downloads/765/tools/quickreference/routerperformance.pdf">http://www.cisco.com/web/partners/downloads/765/tools/quickreference/routerperformance.pdf</a> and <a class="reference external" href="http://www.cisco.com/web/partners/downloads/765/tools/quickreference/switchperformance.pdf">http://www.cisco.com/web/partners/downloads/765/tools/quickreference/switchperformance.pdf</a></p>
</aside>
<aside class="footnote brackets" id="fadjust" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>Some networking technologies allow to adjust dynamically the bandwidth of links. For example, some devices can reduce their bandwidth to preserve energy. We ignore these technologies in this basic course and assume that all links used inside the network have a fixed bandwidth.</p>
</aside>
<aside class="footnote brackets" id="fslottime" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">4</a><span class="fn-bracket">]</span></span>
<p>This name should not be confused with the duration of a transmission slot in slotted ALOHA. In CSMA/CD networks, the slot time is the time during which a collision can occur at the beginning of the transmission of a frame. In slotted ALOHA, the duration of a slot is the transmission time of an entire fixed-size frame.</p>
</aside>
<aside class="footnote brackets" id="fcredit" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id42">5</a><span class="fn-bracket">]</span></span>
<p>In this section, we focus on congestion control mechanisms that regulate the transmission rate of the hosts. Other types of mechanisms have been proposed in the literature. For example, <cite>credit-based</cite> flow-control has been proposed to avoid congestion in ATM networks <a class="reference internal" href="../bibliography.html#kr1995" id="id75"><span>[KR1995]</span></a>. With a credit-based mechanism, hosts can only send packets once they have received credits from the routers and the credits depend on the occupancy of the router’s buffers.</p>
</aside>
<aside class="footnote brackets" id="fflowslink" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id45">6</a><span class="fn-bracket">]</span></span>
<p>For example, the measurements performed in the Sprint network in 2004 reported more than 10k active TCP connections on a link, see <a class="reference external" href="https://research.sprintlabs.com/packstat/packetoverview.php">https://research.sprintlabs.com/packstat/packetoverview.php</a>. More recent information about backbone links may be obtained from <a class="reference external" href="https://www.caida.org">caida</a> ‘s real-time measurements, see e.g.  <a class="reference external" href="http://www.caida.org/data/realtime/passive/">http://www.caida.org/data/realtime/passive/</a></p>
</aside>
</aside>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fwrap" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id57">7</a><span class="fn-bracket">]</span></span>
<p>In this pseudo-code, we assume that TCP uses unlimited sequence and acknowledgment numbers. Furthermore, we do not detail how the <cite>cwnd</cite> is adjusted after the retransmission of the lost segment by fast retransmit. Additional details may be found in <span class="target" id="index-53"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc5681.html"><strong>RFC 5681</strong></a>.</p>
</aside>
<aside class="footnote brackets" id="fprivate" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id63">8</a><span class="fn-bracket">]</span></span>
<p>In enterprise networks or datacenters, the situation is different since a single company typically controls all the sources and all the routers. In such networks it is possible to ensure that all hosts and routers have been upgraded before turning on ECN on the routers.</p>
</aside>
<aside class="footnote brackets" id="fecnnonce" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id64">9</a><span class="fn-bracket">]</span></span>
<p>With the ECT bit, the deployment issue with ECN is solved provided that all sources cooperate. If some sources do not support ECN but still set the ECT bit in the packets that they sent, they will have an unfair advantage over the sources that correctly react to packet markings. Several solutions have been proposed to deal with this problem <span class="target" id="index-54"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3540.html"><strong>RFC 3540</strong></a>, but they are outside the scope of this book.</p>
</aside>
<aside class="footnote brackets" id="fslot" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id65">10</a><span class="fn-bracket">]</span></span>
<p>The buffers of a router can be implemented as variable or fixed-length slots. If the router uses variable length slots to store the queued packets, then the occupancy is usually measured in bytes. Some routers have use fixed-length slots with each slot large enough to store a maximum-length packet. In this case, the buffer occupancy is measured in packets.</p>
</aside>
</aside>
</section>
</section>
</section>
&#13;

<h2>Sharing bandwidth<a class="headerlink" href="#sharing-bandwidth" title="Link to this heading">#</a></h2>
<p>In all these networks, except the full-mesh, the link bandwidth is shared among all connected hosts. Various algorithms have been proposed and are used to efficiently share the access to this resource. We explain several of them in the Medium Access Control section below.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Fairness in computer networks</p>
<p>Sharing resources is important to ensure that the network efficiently serves its user. In practice, there are many ways to share resources. Some resource sharing schemes consider that some users are more important than others and should obtain more resources. For example, on the roads, police cars and ambulances have priority. In some cities, traffic lanes are reserved for buses to promote public services, … In computer networks, the same problem arise. Given that resources are limited, the network needs to enable users to efficiently share them. Before designing an efficient resource sharing scheme, one needs to first formalize its objectives. In computer networks, the most popular objective for resource sharing schemes is that they must be <cite>fair</cite>. In a simple situation, for example two hosts using a shared 2 Mbps link, the sharing scheme should allocate the same bandwidth to each user, in this case 1 Mbps. However, in a large networks, simply dividing the available resources by the number of users is not sufficient. Consider the network shown in the figure below where <cite>A1</cite> sends data to <cite>A2</cite>, <cite>B1</cite> to <cite>B2</cite>, … In this network, how should we divide the bandwidth among the different flows ? A first approach would be to allocate the same bandwidth to each flow. In this case, each flow would obtain 5 Mbps and the link between <cite>R2</cite> and <cite>R3</cite> would not be fully loaded. Another approach would be to allocate 10 Mbps to <cite>A1-A2</cite>, 20 Mbps to <cite>C1-C2</cite> and nothing to <cite>B1-B2</cite>. This is clearly unfair.</p>
<blockquote>
<div><div class="figure" id="id81" style="text-align: center"><p><img src="../Images/a92629ffc43cb0912e0548fff34a5038.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-5a1986707352b87a193f895c2ce30216e6ca221c.png"/></p>
<p><span class="caption-number">Fig. 192 </span><span class="caption-text">A small network</span></p>
</div></div></blockquote>
<p id="index-1">In large networks, fairness is always a compromise. The most widely used definition of fairness is the <cite>max-min fairness</cite>. A bandwidth allocation in a network is said to be <cite>max-min fair</cite> if it is such that it is impossible to allocate more bandwidth to one of the flows without reducing the bandwidth of a flow that already has a smaller allocation than the flow that we want to increase. If the network is completely known, it is possible to derive a <cite>max-min fair</cite> allocation as follows. Initially, all flows have a null bandwidth and they are placed in the candidate set. The bandwidth allocation of all flows in the candidate set is increased until one link becomes congested. At this point, the flows that use the congested link have reached their maximum allocation. They are removed from the candidate set and the process continues until the candidate set becomes empty.</p>
<p>In the above network, the allocation of all flows would grow until <cite>A1-A2</cite> and <cite>B1-B2</cite> reach 5 Mbps. At this point, link <cite>R1-R2</cite> becomes congested and these two flows have reached their maximum. The allocation for flow <cite>C1-C2</cite> can increase until reaching 15 Mbps. At this point, link <cite>R2-R3</cite> is congested. To increase the bandwidth allocated to <cite>C1-C2</cite>, one would need to reduce the allocation to flow <cite>B1-B2</cite>. Similarly, the only way to increase the allocation to flow <cite>B1-B2</cite> would require a decrease of the allocation to <cite>A1-A2</cite>.</p>
</div>
&#13;

<h2>Network congestion<a class="headerlink" href="#network-congestion" title="Link to this heading">#</a></h2>
<p>Sharing bandwidth among the hosts directly attached to a link is not the only sharing problem that occurs in computer networks. To understand the general problem, let us consider a very simple network which contains only point-to-point links. This network contains three hosts and two routers. All the links inside the network have the same capacity. For example, let us assume that all links have a bandwidth of 1000 bits per second and that the hosts send packets containing exactly one thousand bits.</p>
<blockquote>
<div><div class="figure" id="id82" style="text-align: center"><p><img src="../Images/f79da50c17973252bf5368a3df95f186.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-85b7450b921769000c252aef39a6e4c58c692cd7.png"/></p>
<p><span class="caption-number">Fig. 193 </span><span class="caption-text">A small network</span></p>
</div></div></blockquote>
<p>In the network above, consider the case where host <cite>A</cite> is transmitting packets to destination <cite>C</cite>. <cite>A</cite> can send one packet per second and its packets will be delivered to <cite>C</cite>. Now, let us explore what happens when host <cite>B</cite> also starts to transmit a packet. Node <cite>R1</cite> will receive two packets that must be forwarded to <cite>R2</cite>. Unfortunately, due to the limited bandwidth on the <cite>R1-R2</cite> link, only one of these two packets can be transmitted. The outcome of the second packet will depend on the available buffers on <cite>R1</cite>. If <cite>R1</cite> has one available buffer, it could store the packet that has not been transmitted on the <cite>R1-R2</cite> link until the link becomes available. If <cite>R1</cite> does not have available buffers, then the packet needs to be discarded.</p>
<p id="index-2">Besides the link bandwidth, the buffers on the network nodes are the second type of resource that needs to be shared inside the network. The node buffers play an important role in the operation of the network because that can be used to absorb transient traffic peaks. Consider again the example above. Assume that on average host <cite>A</cite> and host <cite>B</cite> send a group of three packets every ten seconds. Their combined transmission rate (0.6 packets per second) is, on average, lower than the network capacity (1 packet per second). However, if they both start to transmit at the same time, node <cite>R1</cite> will have to absorb a burst of packets. This burst of packets is a small <cite>network congestion</cite>. We will say that a network is congested, when the sum of the traffic demand from the hosts is larger than the network capacity <span class="math notranslate nohighlight">\(\sum{demand}&gt;capacity\)</span>. This <cite>network congestion</cite> problem is one of the most difficult resource sharing problem in computer networks. <cite>Congestion</cite> occurs in almost all networks. Minimizing the amount of congestion is a key objective for many network operators. In most cases, they will have to accept transient congestion, i.e. congestion lasting a few seconds or perhaps minutes, but will want to prevent congestion that lasts days or months. For this, they can rely on a wide range of solutions. We briefly present some of these in the paragraphs below.</p>
<p id="index-3">If <cite>R1</cite> has enough buffers, it will be able to absorb the load without having to discard packets. The packets sent by hosts <cite>A</cite> and <cite>B</cite> will reach their final destination <cite>C</cite>, but will experience a longer delay than when they are transmitting alone. The amount of buffering on the network node is the first parameter that a network operator can tune to control congestion inside his network. Given the decreasing cost of memory, one could be tempted to put as many buffers <a class="footnote-reference brackets" href="#fbufferbloat" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> as possible on the network nodes. Let us consider this case in the network above and assume that <cite>R1</cite> has infinite buffers. Assume now that hosts <cite>A</cite> and <cite>B</cite> try to transmit a file that corresponds to one thousand packets each. Both are using a reliable protocol that relies on go-back-n to recover from transmission errors. The transmission starts and packets start to accumulate in <cite>R1</cite>’s buffers. The presence of these packets in the buffers increases the delay between the transmission of a packet by <cite>A</cite> and the return of the corresponding acknowledgment. Given the increasing delay, host <cite>A</cite> (and <cite>B</cite> as well) will consider that some of the packets that it sent have been lost. These packets will be retransmitted and will enter the buffers of <cite>R1</cite>. The occupancy of the buffers of <cite>R1</cite> will continue to increase and the delays as well. This will cause new retransmissions, … In the end, only one file will be delivered (very slowly) to the destination, but the link <cite>R1-R2</cite> will transfer much more bytes than the size of the file due to the multiple copies of the same packets. This is known as the <cite>congestion collapse</cite> problem <span class="target" id="index-4"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc896.html"><strong>RFC 896</strong></a>. Congestion collapse is the nightmare for network operators. When it happens, the network carries packets without delivering useful data to the end users.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Congestion collapse on the Internet</p>
<p>Congestion collapse is unfortunately not only an academic experience. Van Jacobson reports in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id2"><span>[Jacobson1988]</span></a> one of these events that affected him while he was working at the Lawrence Berkeley Laboratory (LBL). LBL was two network nodes away from the University of California in Berkeley. At that time, the link between the two sites had a bandwidth of 32 Kbps, but some hosts were already attached to 10 Mbps LANs. “In October 1986,  the data throughput from LBL to UC Berkeley … dropped from 32 Kbps to 40 bps. We were fascinated by this sudden factor-of-thousand drop in bandwidth and embarked on an investigation of why things had gotten so bad.” This work lead to the development of various congestion control techniques that have allowed the Internet to continue to grow without experiencing widespread congestion collapse events.</p>
</div>
<p>Besides bandwidth and memory, a third resource that needs to be shared inside a network is the (packet) processing capacity. To forward a packet, a router needs bandwidth on the outgoing link, but it also needs to analyze the packet header to perform a lookup inside its forwarding table. Performing these lookup operations require resources such as CPU cycles or memory accesses. Routers are usually designed to be able to sustain a given packet processing rate, measured in packets per second <a class="footnote-reference brackets" href="#fpps" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Packets per second versus bits per second</p>
<p>The performance of network nodes (either routers or switches) can be characterized by two key metrics :</p>
<blockquote>
<div><ul class="simple">
<li><p>the node’s capacity measured in bits per second</p></li>
<li><p>the node’s lookup performance measured in packets per second</p></li>
</ul>
</div></blockquote>
<p>The node’s capacity in bits per second mainly depends on the physical interfaces that it uses and also on the capacity of the internal interconnection (bus, crossbar switch, …) between the different interfaces inside the node. Many vendors, in particular for low-end devices will use the sum of the bandwidth of the nodes’ interfaces as the node capacity in bits per second. Measurements do not always match this maximum theoretical capacity. A well designed network node will usually have a capacity in bits per second larger than the sum of its link capacities. Such nodes will usually reach this maximum capacity when forwarding large packets.</p>
<p>When a network node forwards small packets, its performance is usually limited by the number of lookup operations that it can perform every second. This lookup performance is measured in packets per second. The performance may depend on the length of the forwarded packets. The key performance factor is the number of minimal size packets that are forwarded by the node every second. This rate can lead to a capacity in bits per second which is much lower than the sum of the bandwidth of the node’s links.</p>
</div>
<p id="index-5">Let us now try to present a broad overview of the congestion problem in networks. We will assume that the network is composed of dedicated links having a fixed bandwidth <a class="footnote-reference brackets" href="#fadjust" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>. A network contains hosts that generate and receive packets and nodes (routers and switches) that forward packets. Assuming that each host is connected via a single link to the network, the largest demand is <span class="math notranslate nohighlight">\(\sum{Access Links}\)</span>. In practice, this largest demand is never reached and the network will be engineered to sustain a much lower traffic demand. The difference between the worst-case traffic demand and the sustainable traffic demand can be large, up to several orders of magnitude. Fortunately, the hosts are not completely dumb and they can adapt their traffic demand to the current state of the network and the available bandwidth. For this, the hosts need to <cite>sense</cite> the current level of congestion and adjust their own traffic demand based on the estimated congestion. Network nodes can react in different ways to network congestion and hosts can sense the level of congestion in different ways.</p>
<p>Let us first explore which mechanisms can be used inside a network to control congestion and how these mechanisms can influence the behavior of the end hosts.</p>
<p>As explained earlier, one of the first manifestation of congestion on network nodes is the saturation of the network links that leads to a growth in the occupancy of the buffers of the node. This growth of the buffer occupancy implies that some packets will spend more time in the buffer and thus in the network. If hosts measure the network delays (e.g. by measuring the round-trip-time between the transmission of a packet and the return of the corresponding acknowledgment) they could start to sense congestion. On low bandwidth links, a growth in the buffer occupancy can lead to an increase of the delays which can be easily measured by the end hosts. On high bandwidth links, a few packets inside the buffer will cause a small variation in the delay which may not necessarily be larger that the natural fluctuations of the delay measurements.</p>
<p>If the buffer’s occupancy continues to grow, it will overflow and packets will need to be discarded. Discarding packets during congestion is the second possible reaction of a network node to congestion. Before looking at how a node can discard packets, it is interesting to discuss qualitatively the impact of the buffer occupancy on the reliable delivery of data through a network. This is illustrated by figure <a class="reference internal" href="#fig-congestion-jain"><span class="std std-numref">Fig. 194</span></a>, adapted from <a class="reference internal" href="../bibliography.html#jain1990" id="id5"><span>[Jain1990]</span></a>.</p>
<figure class="align-center" id="fig-congestion-jain">
<img alt="../_images/jain.png" src="../Images/b9e0fdb8cefccc00202ebacd9b171691.png" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/jain.png"/>
<figcaption>
<p><span class="caption-number">Fig. 194 </span><span class="caption-text">Network congestion</span><a class="headerlink" href="#fig-congestion-jain" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>When the network load is low, buffer occupancy and link utilization are low. The buffers on the network nodes are mainly used to absorb very short bursts of packets, but on average the traffic demand is lower than the network capacity. If the demand increases, the average buffer occupancy will increase as well. Measurements have shown that the total throughput increases as well. If the buffer occupancy is zero or very low, transmission opportunities on network links can be missed. This is not the case when the buffer occupancy is small but non zero. However, if the buffer occupancy continues to increase, the buffer becomes overloaded and the throughput does not increase anymore. When the buffer occupancy is close to the maximum, the throughput may decrease. This drop in throughput can be caused by excessive retransmissions of reliable protocols that incorrectly assume that previously sent packets have been lost while they are still waiting in the buffer. The network delay on the other hand increases with the buffer occupancy. In practice, a good operating point for a network buffer is a low occupancy to achieve high link utilization and also low delay for interactive applications.</p>
<p id="index-6">Discarding packets is one of the signals that the network nodes can use to inform the hosts of the current level of congestion. Buffers on network nodes are usually used as FIFO queues to preserve packet ordering. Several <cite>packet discard mechanisms</cite> have been proposed for network nodes. These techniques basically answer two different questions :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>What triggers a packet to be discarded ?</cite> What are the conditions that lead a network node to decide to discard a packet? The simplest answer to this question is : <cite>When the buffer is full</cite>. Although this is a good congestion indication, it is probably not the best one from a performance viewpoint. An alternative is to discard packets when the buffer occupancy grows too much. In this case, it is likely that the buffer will become full shortly. Since packet discarding is an information that allows hosts to adapt their transmission rate, discarding packets early could allow hosts to react earlier and thus prevent congestion from happening.</p></li>
<li><p><cite>Which packet(s) should be discarded ?</cite> Once the network node has decided to discard packets, it needs to actually discard real packets.</p></li>
</ul>
</div></blockquote>
<p>By combining different answers to these questions, network researchers have developed different packet discard mechanisms.</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>Tail drop</cite> is the simplest packet discard technique. When a buffer is full, the arriving packet is discarded. <cite>Tail drop</cite> can be easily implemented. This is, by far, the most widely used packet discard mechanism. However, it suffers from two important drawbacks. First, since <cite>tail drop</cite> discards packets only when the buffer is full, buffers tend to be congested and real-time applications may suffer from increased delays. Second, <cite>tail drop</cite> is blind when it discards a packet. It may discard a packet from a low bandwidth interactive flow while most of the buffer is used by large file transfers.</p></li>
<li><p><cite>Drop from front</cite> is an alternative packet discard technique. Instead of removing the arriving packet, it removes the packet that was at the head of the queue. Discarding this packet instead of the arriving one can have two advantages. First, it already stayed a long time in the buffer. Second, hosts should be able to detect the loss (and thus the congestion) earlier.</p></li>
<li><p><cite>Probabilistic drop</cite>. Various random drop techniques have been proposed. A frequently cited technique is <cite>Random Early Discard</cite> (RED) <a class="reference internal" href="../bibliography.html#fj1993" id="id6"><span>[FJ1993]</span></a>. RED measures the average buffer occupancy and discards packets with a given probability when this average occupancy is too high. Compared to <cite>tail drop</cite> and <cite>drop from front</cite>, an advantage of <cite>RED</cite> is that thanks to the probabilistic drops, packets should be discarded from different flows in proportion of their bandwidth.</p></li>
</ul>
</div></blockquote>
<p>Discarding packets is a frequent reaction to network congestion. Unfortunately, discarding packets is not optimal since a packet which is discarded on a network node has already consumed resources on the upstream nodes. There are other ways for the network to inform the end hosts of the current congestion level. A first solution is to mark the packets when a node is congested. Several networking technologies have relied on this kind of packet marking.</p>
<p id="index-7">In datagram networks, <cite>Forward Explicit Congestion Notification</cite> (FECN) can be used. One field of the packet header, typically one bit, is used to indicate congestion. When a host sends a packet, the congestion bit is unset. If the packet passes through a congested node, the congestion bit is set. The destination can then determine the current congestion level by measuring the fraction of the packets that it received with the congestion bit set. It may then return this information to the sending host to allow it to adapt its retransmission rate. Compared to packet discarding, the main advantage of FECN is that hosts can detect congestion explicitly without having to rely on packet losses.</p>
<p>In virtual circuit networks, packet marking can be improved if the return packets follow the reverse path of the forward packets. It this case, a network node can detect congestion on the forward path (e.g. due to the size of its buffer), but mark the packets on the return path. Marking the return packets (e.g. the acknowledgments used by reliable protocols) provides a faster feedback to the sending hosts compared to FECN. This technique is usually called <cite>Backward Explicit Congestion Notification (BECN)</cite>.</p>
<p>If the packet header does not contain any bit in the header to represent the current congestion level, an alternative is to allow the network nodes to send a control packet to the source to indicate the current congestion level. Some networking technologies use such control packets to explicitly regulate the transmission rate of sources. However, their usage is mainly restricted to small networks. In large networks, network nodes usually avoid using such control packets. These control packets are even considered to be dangerous in some networks. First, using them increases the network load when the network is congested. Second, while network nodes are optimized to forward packets, they are usually pretty slow at creating new packets.</p>
<p id="index-8">Dropping and marking packets is not the only possible reaction of a router that becomes congested. A router could also selectively delay packets belonging to some flows. There are different algorithms that can be used by a router to delay packets. If the objective of the router is to fairly distribute to bandwidth of an output link among competing flows, one possibility is to organize the buffers of the router as a set of queues. For simplicity, let us assume that the router is capable of supporting a fixed number of concurrent flows, say <cite>N</cite>. One of the queues of the router is associated to each flow and when a packet arrives, it is placed at the tail of the corresponding queue. All the queues are controlled by a <cite>scheduler</cite>. A <cite>scheduler</cite> is an algorithm that is run each time there is an opportunity to transmit a packet on the outgoing link. Various schedulers have been proposed in the scientific literature and some are used in real routers.</p>
<blockquote>
<div><div class="figure" id="id83" style="text-align: center"><p><img src="../Images/e6916b3dd773fa746d317a93e0a077db.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-1ab27e7b575b28dcf1d5296681e54c7f6463c119.png"/></p>
<p><span class="caption-number">Fig. 195 </span><span class="caption-text">A round-robin scheduler, where N = 5</span></p>
</div></div></blockquote>
<p>A very simple scheduler is the <cite>round-robin scheduler</cite>. This scheduler serves all the queues in a round-robin fashion. If all flows send packets of the same size, then the round-robin scheduler fairly allocates the bandwidth among the different flows. Otherwise, it favors flows that are using larger packets. Extensions to the <cite>round-robin scheduler</cite> have been proposed to provide a fair distribution of the bandwidth with variable-length packets <a class="reference internal" href="../bibliography.html#sv1995" id="id7"><span>[SV1995]</span></a> but these are outside the scope of this chapter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># N queues</span>
<span class="c1"># state variable : next_queue</span>
<span class="n">next_queue</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">isEmpty</span><span class="p">(</span><span class="n">buffer</span><span class="p">):</span>
        <span class="c1"># Wait for next packet in buffer</span>
        <span class="n">wait</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isEmpty</span><span class="p">(</span><span class="n">queue</span><span class="p">[</span><span class="n">next_queue</span><span class="p">])):</span>
        <span class="c1"># Send packet at head of next_queue</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">remove_packet</span><span class="p">(</span><span class="n">queue</span><span class="p">[</span><span class="n">next_queue</span><span class="p">])</span>
        <span class="n">send</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">next_queue</span><span class="o">=</span><span class="p">(</span><span class="n">next_queue</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">N</span>
<span class="c1"># end while</span>
</pre></div>
</div>
&#13;

<h2>Distributing the load across the network<a class="headerlink" href="#distributing-the-load-across-the-network" title="Link to this heading">#</a></h2>
<p>Delays, packet discards, packet markings and control packets are the main types of information that the network can exchange with the end hosts. Discarding packets is the main action that a network node can perform if the congestion is too severe. Besides tackling congestion at each node, it is also possible to divert some traffic flows from heavily loaded links to reduce congestion. Early routing algorithms <a class="reference internal" href="../bibliography.html#mrr1980" id="id8"><span>[MRR1980]</span></a> have used delay measurements to detect congestion between network nodes and update the link weights dynamically. By reflecting the delay perceived by applications in the link weights used for the shortest paths computation, these routing algorithms managed to dynamically change the forwarding paths in reaction to congestion. However, deployment experience showed that these dynamic routing algorithms could cause oscillations and did not necessarily lower congestion. Deployed datagram networks rarely use dynamic routing algorithms, except in some wireless networks. In datagram networks, the state of the art reaction to long term congestion, i.e. congestion lasting hours, days or more, is to measure the traffic demand and then select the link weights <a class="reference internal" href="../bibliography.html#frt2002" id="id9"><span>[FRT2002]</span></a> that allow minimizing the maximum link loads. If the congestion lasts longer, changing the weights is not sufficient anymore and the network needs to be upgraded with additional or faster links. However, in Wide Area Networks, adding new links can take months.</p>
<p>In virtual circuit networks, another way to manage or prevent congestion is to limit the number of circuits that use the network at any time. This technique is usually called <cite>connection admission control</cite>. When a host requests the creation of a new circuit in the network, it specifies the destination and in some networking technologies the required bandwidth. With this information, the network can check whether there are enough resources available to reach this particular destination. If yes, the circuit is established. If not, the request is denied and the host will have to defer the creation of its virtual circuit. <cite>Connection admission control</cite> schemes are widely used in the telephone networks. In these networks, a busy tone corresponds to an unavailable destination or a congested network.</p>
<p>In datagram networks, this technique cannot be easily used since the basic assumption of such a network is that a host can send any packet towards any destination at any time. A host does not need to request the authorization of the network to send packets towards a particular destination.</p>
<p>Based on the feedback received from the network, the hosts can adjust their transmission rate. We discuss in section <cite>Congestion control</cite> some techniques that allow hosts to react to congestion.</p>
<p>Another way to share the network resources is to distribute the load across multiple links. Many techniques have been designed to spread the load over the network. As an illustration, let us briefly consider how the load can be shared when accessing some content. Consider a large and popular file such as the image of a Linux distribution or the upgrade of a commercial operating system that will be downloaded by many users. There are many ways to distribute this large file. A naive solution is to place one copy of the file on a server and allow all users to download this file from the server. If the file is popular and millions of users want to download it, the server will quickly become overloaded. There are two classes of solutions that can be used to serve a large number of users. A first approach is to store the file on servers whose name is known by the clients. Before retrieving the file, each client will query the name service to obtain the address of the server. If the file is available from many servers, the name service can provide different addresses to different clients. This will automatically spread the load since different clients will download the file from different servers. Most large content providers use such a solution to distribute large files or videos.</p>
<p>There is another solution that allows spreading the load among many sources without relying on the name service. The popular <a class="reference external" href="https://www.bittorrent.com">bittorent</a> service
is an example of this approach. With this solution, each file is divided in blocks of fixed size. To retrieve a file, a client needs to retrieve all the blocks that compose the file. However, nothing forces the client to retrieve all the blocks in sequence and from the same server. Each file is associated with metadata that indicates for each block a list of addresses of hosts that store this block. To retrieve a complete file, a client first downloads the metadata. Then, it tries to retrieve each block from one of the hosts that store the block. In practice, implementations often try to download several blocks in parallel. Once one block has been successfully downloaded, the next block can be requested. If a host is slow to provide one block or becomes unavailable, the client can contact another host listed in the metadata. Most deployments of bittorrent allow the clients to participate to the distribution of blocks. Once a client has downloaded one block, it contacts the server which stores the metadata to indicate that it can also provide this block. With this scheme, when a file is popular, its blocks are downloaded by many hosts that automatically participate in the distribution of the blocks. Thus, the number of <cite>servers</cite> that are capable of providing blocks from a popular file automatically increases with the file’s popularity.</p>
<p>Now that we have provided a broad overview of the techniques that can be used to spread the load and allocate resources in the network, let us analyze two techniques in more details : Medium Access Control and Congestion control.</p>
&#13;

<h2>Medium Access Control algorithms<a class="headerlink" href="#medium-access-control-algorithms" title="Link to this heading">#</a></h2>
<p id="index-9">The common problem among Local Area Networks is how to efficiently share the available bandwidth. If two devices send a frame at the same time, the two electrical, optical or radio signals that correspond to these frames will appear at the same time on the transmission medium and a receiver will not be able to decode either frame. Such simultaneous transmissions are called <cite>collisions</cite>. A <cite>collision</cite> may involve frames transmitted by two or more devices attached to the Local Area Network. Collisions are the main cause of errors in wired Local Area Networks.</p>
<p>All Local Area Network technologies rely on a <cite>Medium Access Control</cite> algorithm to regulate the transmissions to either minimize or avoid collisions. There are two broad families of <cite>Medium Access Control</cite> algorithms :</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><cite>Deterministic</cite> or <cite>pessimistic</cite> MAC algorithms. These algorithms assume that collisions are a very severe problem and that they must be completely avoided. These algorithms ensure that at any time, at most one device is allowed to send a frame on the LAN. This is usually achieved by using a distributed protocol which elects one device that is allowed to transmit at each time. A deterministic MAC algorithm ensures that no collision will happen, but there is some overhead in regulating the transmission of all the devices attached to the LAN.</p></li>
<li><p><cite>Stochastic</cite> or <cite>optimistic</cite> MAC algorithms. These algorithms assume that collisions are part of the normal operation of a Local Area Network. They aim to minimize the number of collisions, but they do not try to avoid all collisions. Stochastic algorithms are usually easier to implement than deterministic ones.</p></li>
</ol>
</div></blockquote>
<p>We first discuss a simple deterministic MAC algorithm and then we describe several important optimistic algorithms, before coming back to a distributed and deterministic MAC algorithm.</p>
<section id="static-allocation-methods">
<h3>Static allocation methods<a class="headerlink" href="#static-allocation-methods" title="Link to this heading">#</a></h3>
<p>A first solution to share the available resources among all the devices attached to one Local Area Network is to define, <cite>a priori</cite>, the distribution of the transmission resources among the different devices. If <cite>N</cite> devices need to share the transmission capacities of a LAN operating at <cite>b</cite> Mbps, each device could be allocated a bandwidth of <span class="math notranslate nohighlight">\(\frac{b}{N}\)</span> Mbps.</p>
<p id="index-10">Limited resources need to be shared in other environments than Local Area Networks. Since the first radio transmissions by <a class="reference external" href="http://en.wikipedia.org/wiki/Guglielmo_Marconi">Marconi</a> more than one century ago, many applications that exchange information through radio signals have been developed. Each radio signal is an electromagnetic wave whose power is centered around a given frequency. The radio spectrum corresponds to frequencies ranging between roughly 3 KHz and 300 GHz. Frequency allocation plans negotiated among governments reserve most frequency ranges for specific applications such as broadcast radio, broadcast television, mobile communications, aeronautical radio navigation, amateur radio, satellite, etc. Each frequency range is then subdivided into channels and each channel can be reserved for a given application, e.g. a radio broadcaster in a given region.</p>
<p id="index-11"><cite>Frequency Division Multiplexing</cite> (FDM) is a static allocation scheme in which a frequency is allocated to each device attached to the shared medium. As each device uses a different transmission frequency, collisions cannot occur. In optical networks, a variant of FDM called <cite>Wavelength Division Multiplexing</cite> (WDM) can be used. An optical fiber can transport light at different wavelengths without interference. With WDM, a different wavelength is allocated to each of the devices that share the same optical fiber.</p>
<p id="index-12"><cite>Time Division Multiplexing</cite> (TDM) is a static bandwidth allocation method that was initially defined for the telephone network. In the fixed telephone network, a voice conversation is usually transmitted as a 64 Kbps signal. Thus, a telephone conservation generates 8 KBytes per second or one byte every 125 microseconds. Telephone conversations often need to be multiplexed together on a single line. For example, in Europe, thirty 64 Kbps voice signals are multiplexed over a single 2 Mbps (E1) line. This is done by using  <cite>Time Division Multiplexing</cite> (TDM). TDM divides the transmission opportunities into slots. In the telephone network, a slot corresponds to 125 microseconds. A position inside each slot is reserved for each voice signal. The figure below illustrates TDM on a link that is used to carry four voice conversations. The vertical lines represent the slot boundaries and the letters the different voice conversations. One byte from each voice conversation is sent during each 125 microseconds slot. The byte corresponding to a given conversation is always sent at the same position in each slot.</p>
<blockquote>
<div><div class="figure" id="id84" style="text-align: center"><p><img src="../Images/a5ce7017001779fb85df0a821c9dfa33.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-d77b364fc4d8103f3e76051b25f626c2864e0059.png"/></p>
<p><span class="caption-number">Fig. 196 </span><span class="caption-text">Time-division multiplexing</span></p>
</div></div></blockquote>
<p>TDM as shown above can be completely static, i.e. the same conversations always share the link, or dynamic. In the latter case, the two endpoints of the link must exchange messages specifying which conversation uses which byte inside each slot. Thanks to these control messages, it is possible to dynamically add and remove voice conversations from a given link.</p>
<p>TDM and FDM are widely used in telephone networks to support fixed bandwidth conversations. Using them in Local Area Networks that support computers would probably be inefficient. Computers usually do not send information at a fixed rate. Instead, they often have an on-off behavior. During the on period, the computer tries to send at the highest possible rate, e.g. to transfer a file. During the off period, which is often much longer than the on period, the computer does not transmit any packet. Using a static allocation scheme for computers attached to a LAN would lead to huge inefficiencies, as they would only be able to transmit at <span class="math notranslate nohighlight">\(\frac{1}{N}\)</span> of the total bandwidth during their on period, despite the fact that the other computers are in their off period and thus do not need to transmit any information. The dynamic MAC algorithms discussed in the remainder of this chapter aim to solve this problem.</p>
</section>
<section id="aloha">
<h3>ALOHA<a class="headerlink" href="#aloha" title="Link to this heading">#</a></h3>
<p id="index-13">In the 1960s, computers were mainly mainframes with a few dozen terminals attached to them. These terminals were usually in the same building as the mainframe and were directly connected to it. In some cases, the terminals were installed in remote locations and connected through a <a class="reference internal" href="../glossary.html#term-modem"><span class="xref std std-term">modem</span></a> attached to a <a class="reference internal" href="../glossary.html#term-dial-up-line"><span class="xref std std-term">dial-up  line</span></a>. The university of Hawaii chose a different organization. Instead of using telephone lines to connect the distant terminals, they developed the first <cite>packet radio</cite> technology <a class="reference internal" href="../bibliography.html#abramson1970" id="id10"><span>[Abramson1970]</span></a>. Until then, computer networks were built on top of either the telephone network or physical cables. ALOHANet showed that it is possible to use radio signals to interconnect computers.</p>
<p id="index-14">The first version of ALOHANet, described in <a class="reference internal" href="../bibliography.html#abramson1970" id="id11"><span>[Abramson1970]</span></a>, operated as follows. First, the terminals and the mainframe exchanged fixed-length frames composed of 704 bits. Each frame contained 80 8-bit characters, some control bits and parity information to detect transmission errors. Two channels in the 400 MHz range were reserved for the operation of ALOHANet. The first channel was used by the mainframe to send frames to all terminals. The second channel was shared among all terminals to send frames to the mainframe. As all terminals share the same transmission channel, there is a risk of collision. To deal with this problem as well as transmission errors, the mainframe verified the parity bits of the received frame and sent an acknowledgment on its channel for each correctly received frame. The terminals on the other hand had to retransmit the unacknowledged frames. As for TCP, retransmitting these frames immediately upon expiration of a fixed timeout is not a good approach as several terminals may retransmit their frames at the same time leading to a network collapse. A better approach, but still far from perfect, is for each terminal to wait a random amount of time after the expiration of its retransmission timeout. This avoids synchronization among multiple retransmitting terminals.</p>
<p>The pseudo-code below shows the operation of an ALOHANet terminal. We use this python syntax for all Medium Access Control algorithms described in this chapter. The algorithm is applied to each new frame that needs to be transmitted. It attempts to transmit a frame at most <cite>max</cite> times (<cite>while loop</cite>). Each transmission attempt is performed as follows. First, the frame is sent. Each frame is protected by a timeout. Then, the terminal waits for either a valid acknowledgment frame or the expiration of its timeout. If the terminal receives an acknowledgment, the frame has been delivered correctly and the algorithm terminates. Otherwise, the terminal waits for a random time and attempts to retransmit the frame.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># ALOHA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack_on_return_channel</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ack_on_return_channel</span><span class="p">):</span>
       <span class="k">break</span>  <span class="c1"># transmission was successful</span>
    <span class="k">else</span><span class="p">:</span>
             <span class="c1"># timeout</span>
             <span class="n">wait</span><span class="p">(</span><span class="n">random_time</span><span class="p">)</span>
             <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p><a class="reference internal" href="../bibliography.html#abramson1970" id="id12"><span>[Abramson1970]</span></a> analyzed the performance of ALOHANet under particular assumptions and found that ALOHANet worked well when the channel was lightly loaded. In this case, the frames are rarely retransmitted and the <cite>channel traffic</cite>, i.e. the total number of (correct and retransmitted) frames transmitted per unit of time is close to the <cite>channel utilization</cite>, i.e. the number of correctly transmitted frames per unit of time. Unfortunately, the analysis also reveals that the <cite>channel utilization</cite> reaches its maximum at <span class="math notranslate nohighlight">\(\frac{1}{2 \times e}=0.186\)</span> times the channel bandwidth. At higher utilization, ALOHANet becomes unstable and the network collapses due to collided retransmissions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Amateur packet radio</p>
<p>Packet radio technologies have evolved in various directions since the first experiments performed at the University of Hawaii. The Amateur packet radio service developed by amateur radio operators is one of the descendants ALOHANet. Many amateur radio operators are very interested in new technologies and they often spend countless hours developing new antennas or transceivers. When the first personal computers appeared, several amateur radio operators designed radio modems and their own datalink layer protocols <a class="reference internal" href="../bibliography.html#kpd1985" id="id13"><span>[KPD1985]</span></a> <a class="reference internal" href="../bibliography.html#bnt1997" id="id14"><span>[BNT1997]</span></a>. This network grew and it was possible to connect to servers in several European countries by only using packet radio relays. Some amateur radio operators also developed TCP/IP protocol stacks that were used over the packet radio service. Some parts of the <a class="reference external" href="http://www.ampr.org/">amateur packet radio network</a> are connected to the global Internet and use the <cite>44.0.0.0/8</cite> IPv4 prefix.</p>
</div>
<p id="index-15">Many improvements to ALOHANet have been proposed since the publication of <a class="reference internal" href="../bibliography.html#abramson1970" id="id15"><span>[Abramson1970]</span></a>, and this technique, or some of its variants, are still found in wireless networks today. The slotted technique proposed in <a class="reference internal" href="../bibliography.html#roberts1975" id="id16"><span>[Roberts1975]</span></a> is important because it shows that a simple modification can significantly improve channel utilization. Instead of allowing all terminals to transmit at any time, <a class="reference internal" href="../bibliography.html#roberts1975" id="id17"><span>[Roberts1975]</span></a> proposed to divide time into slots and allow terminals to transmit only at the beginning of each slot. Each slot corresponds to the time required to transmit one fixed size frame. In practice, these slots can be imposed by a single clock that is received by all terminals. In ALOHANet, it could have been located on the central mainframe. The analysis in <a class="reference internal" href="../bibliography.html#roberts1975" id="id18"><span>[Roberts1975]</span></a> reveals that this simple modification improves the channel utilization by a factor of two.</p>
</section>
<section id="carrier-sense-multiple-access">
<span id="index-16"/><h3>Carrier Sense Multiple Access<a class="headerlink" href="#carrier-sense-multiple-access" title="Link to this heading">#</a></h3>
<p>ALOHA and slotted ALOHA can easily be implemented, but unfortunately, they can only be used in networks that are very lightly loaded. Designing a network for a very low utilization is possible, but it clearly increases the cost of the network. To overcome the problems of ALOHA, many Medium Access Control mechanisms have been proposed which improve channel utilization. Carrier Sense Multiple Access (CSMA) is a significant improvement compared to ALOHA. CSMA requires all nodes to listen to the transmission channel to verify that it is free before transmitting a frame <a class="reference internal" href="../bibliography.html#kt1975" id="id19"><span>[KT1975]</span></a>. When a node senses the channel to be busy, it defers its transmission until the channel becomes free again. The pseudo-code below provides a more detailed description of the operation of CSMA.</p>
<div class="highlight-python notranslate" id="index-17"><div class="highlight"><pre><span/><span class="c1"># persistent CSMA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel_becomes_free</span><span class="p">)</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ack</span><span class="p">:</span>
       <span class="k">break</span>  <span class="c1"># transmission was successful</span>
    <span class="k">else</span><span class="p">:</span>
             <span class="c1"># timeout</span>
             <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p>The above pseudo-code is often called <cite>persistent CSMA</cite> <a class="reference internal" href="../bibliography.html#kt1975" id="id20"><span>[KT1975]</span></a> as the terminal will continuously listen to the channel and transmit its frame as soon as the channel becomes free. Another important variant of CSMA is the <cite>non-persistent CSMA</cite> <a class="reference internal" href="../bibliography.html#kt1975" id="id21"><span>[KT1975]</span></a>. The main difference between persistent and non-persistent CSMA described in the pseudo-code below is that a non-persistent CSMA node does not continuously listen to the channel to determine when it becomes free. When a non-persistent CSMA terminal senses the transmission channel to be busy, it waits for a random time before sensing the channel again. This improves channel utilization compared to persistent CSMA. With persistent CSMA, when two terminals sense the channel to be busy, they will both transmit (and thus cause a collision) as soon as the channel becomes free. With non-persistent CSMA, this synchronization does not occur, as the terminals wait a random time after having sensed the transmission channel. However, the higher channel utilization achieved by non-persistent CSMA comes at the expense of a slightly higher waiting time in the terminals when the network is lightly loaded.</p>
<div class="highlight-python notranslate" id="index-18"><div class="highlight"><pre><span/><span class="c1"># Non persistent CSMA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">listen</span><span class="p">(</span><span class="n">channel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">free</span><span class="p">(</span><span class="n">channel</span><span class="p">):</span>
        <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">received</span><span class="p">(</span><span class="n">ack</span><span class="p">):</span>
           <span class="k">break</span>  <span class="c1"># transmission was successful</span>
        <span class="k">else</span><span class="p">:</span>
                 <span class="c1"># timeout</span>
                 <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">random_time</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p><a class="reference internal" href="../bibliography.html#kt1975" id="id22"><span>[KT1975]</span></a> analyzes in detail the performance of several CSMA variants. Under some assumptions about the transmission channel and the traffic, the analysis compares ALOHA, slotted ALOHA, persistent and non-persistent CSMA. Under these assumptions, ALOHA achieves a channel utilization of only 18.4% of the channel capacity. Slotted ALOHA is able to use 36.6% of this capacity. Persistent CSMA improves the utilization by reaching 52.9% of the capacity while non-persistent CSMA achieves 81.5% of the channel capacity.</p>
</section>
<section id="carrier-sense-multiple-access-with-collision-detection">
<span id="index-19"/><h3>Carrier Sense Multiple Access with Collision Detection<a class="headerlink" href="#carrier-sense-multiple-access-with-collision-detection" title="Link to this heading">#</a></h3>
<p id="index-20">CSMA improves channel utilization compared to ALOHA. However, the performance can still be improved, especially in wired networks. Consider the situation of two terminals that are connected to the same cable. This cable could, for example, be a coaxial cable as in the early days of Ethernet <a class="reference internal" href="../bibliography.html#metcalfe1976" id="id23"><span>[Metcalfe1976]</span></a>. It could also be built with twisted pairs. Before extending CSMA, it is useful to understand, more intuitively, how frames are transmitted in such a network and how collisions can occur. The figure below illustrates the physical transmission of a frame on such a cable. To transmit its frame, host A must send an electrical signal on the shared medium. The first step is thus to begin the transmission of the electrical signal. This is point <cite>(1)</cite> in the figure below. This electrical signal will travel along the cable. Although electrical signals travel fast, we know that information cannot travel faster than the speed of light (i.e. 300.000 kilometers/second). On a coaxial cable, an electrical signal is slightly slower than the speed of light and 200.000 kilometers per second is a reasonable estimation. This implies that if the cable has a length of one kilometer, the electrical signal will need 5 microseconds to travel from one end of the cable to the other. The ends of coaxial cables are equipped with termination points that ensure that the electrical signal is not reflected back to its source. This is illustrated at point <cite>(3)</cite> in the figure, where the electrical signal has reached the left endpoint and host B. At this point, B starts to receive the frame being transmitted by A. Notice that there is a delay between the transmission of a bit on host A and its reception by host B. If there were other hosts attached to the cable, they would receive the first bit of the frame at slightly different times. As we will see later, this timing difference is a key problem for MAC algorithms. At point <cite>(4)</cite>, the electrical signal has reached both ends of the cable and occupies it completely. Host A continues to transmit the electrical signal until the end of the frame. As shown at point <cite>(5)</cite>, when the sending host stops its transmission, the electrical signal corresponding to the end of the frame leaves the coaxial cable. The channel becomes empty again once the entire electrical signal has been removed from the cable.</p>
<figure class="align-center" id="id85">
<a class="reference internal image-reference" href="../_images/frame-bus.png"><img alt="../_images/frame-bus.png" src="../Images/ab89b8b4019e54d32dc8e8a85f104ba1.png" style="width: 452.2px; height: 354.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-bus.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 197 </span><span class="caption-text">Frame transmission on a shared bus</span><a class="headerlink" href="#id85" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Now that we have looked at how a frame is actually transmitted as an electrical signal on a shared bus, it is interesting to look in more detail at what happens when two hosts transmit a frame at almost the same time. This is illustrated in the figure below, where hosts A and B start their transmission at the same time (point <cite>(1)</cite>). At this time, if host C senses the channel, it will consider it to be free. This will not last a long time and at point <cite>(2)</cite> the electrical signals from both host A and host B reach host C. The combined electrical signal (shown graphically as the superposition of the two curves in the figure) cannot be decoded by host C. Host C detects a collision, as it receives a signal that it cannot decode. Since host C cannot decode the frames, it cannot determine which hosts are sending the colliding frames. Note that host A (and host B) will detect the collision after host C (point <cite>(3)</cite> in figure <a class="reference internal" href="#fig-collision-bus"><span class="std std-numref">Fig. 198</span></a>).</p>
<figure class="align-center" id="fig-collision-bus">
<a class="reference internal image-reference" href="../_images/frame-collision.png"><img alt="../_images/frame-collision.png" src="../Images/7b88fb2c9cc28c5857f616b145a56cb6.png" style="width: 446.59999999999997px; height: 298.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 198 </span><span class="caption-text">Frame collision on a shared bus</span><a class="headerlink" href="#fig-collision-bus" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-21">As shown above, hosts detect collisions when they receive an electrical signal that they cannot decode. In a wired network, a host is able to detect such a collision both while it is listening (e.g. like host C in the figure above) and also while it is sending its own frame. When a host transmits a frame, it can compare the electrical signal that it transmits with the electrical signal that it senses on the wire. At points <cite>(1)</cite> and <cite>(2)</cite> in the figure above, host A senses only its own signal. At point <cite>(3)</cite>, it senses an electrical signal that differs from its own signal and can thus detects the collision. At this point, its frame is corrupted and it can stop its transmission. The ability to detect collisions while transmitting is the starting point for the <cite>Carrier Sense Multiple Access with Collision Detection (CSMA/CD)</cite> Medium Access Control algorithm, which is used in Ethernet networks <a class="reference internal" href="../bibliography.html#metcalfe1976" id="id24"><span>[Metcalfe1976]</span></a> <a class="reference internal" href="../bibliography.html#ieee802-3" id="id25"><span>[IEEE802.3]</span></a> . When an Ethernet host detects a collision while it is transmitting, it immediately stops its transmission. Compared with pure CSMA, CSMA/CD is an important improvement since when collisions occur, they only last until colliding hosts have detected it and stopped their transmission. In practice, when a host detects a collision, it sends a special jamming signal on the cable to ensure that all hosts have detected the collision.</p>
<p>To better understand these collisions, it is useful to analyze what would be the worst collision on a shared bus network. Let us consider a wire with two hosts attached at both ends, as shown in the figure below. Host A starts to transmit its frame and its electrical signal is propagated on the cable. Its propagation time depends on the physical length of the cable and the speed of the electrical signal. Let us use <span class="math notranslate nohighlight">\(\tau\)</span> to represent this propagation delay in seconds. Slightly less than <span class="math notranslate nohighlight">\(\tau\)</span> seconds after the beginning of the transmission of A’s frame, B decides to start transmitting its own frame. After <span class="math notranslate nohighlight">\(\epsilon\)</span> seconds, B senses A’s frame, detects the collision and stops transmitting. The beginning of B’s frame travels on the cable until it reaches host A. Host A can thus detect the collision at time <span class="math notranslate nohighlight">\(\tau-\epsilon+\tau \approx 2\times\tau\)</span>. An important point to note is that a collision can only occur during the first <span class="math notranslate nohighlight">\(2\times\tau\)</span> seconds of its transmission. If a collision did not occur during this period, it cannot occur afterwards since the transmission channel is busy after <span class="math notranslate nohighlight">\(\tau\)</span> seconds and CSMA/CD hosts sense the transmission channel before transmitting their frame.</p>
<figure class="align-center" id="id86">
<a class="reference internal image-reference" href="../_images/frame-collision-worst.png"><img alt="../_images/frame-collision-worst.png" src="../Images/cb963f4bef31245667608aad2e590553.png" style="width: 448.7px; height: 208.6px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision-worst.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 199 </span><span class="caption-text">The worst collision on a shared bus</span><a class="headerlink" href="#id86" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Furthermore, on the wired networks where CSMA/CD is used, collisions are almost the only cause of transmission errors that affect frames. Transmission errors that only affect a few bits inside a frame seldom occur in these wired networks. For this reason, the designers of CSMA/CD chose to completely remove the acknowledgment frames in the datalink layer. When a host transmits a frame, it verifies whether its transmission has been affected by a collision. If not, given the negligible Bit Error Ratio of the underlying network, it assumes that the frame was received correctly by its destination. Otherwise the frame is retransmitted after some delay.</p>
<p>Removing acknowledgments is an interesting optimization as it reduces the number of frames that are exchanged on the network and the number of frames that need to be processed by the hosts. However, to use this optimization, we must ensure that all hosts will be able to detect all the collisions that affect their frames. The problem is important for short frames. Let us consider two hosts, A and B, that are sending a small frame to host C as illustrated in the figure below. If the frames sent by A and B are very short, the situation illustrated below may occur. Hosts A and B send their frame and stop transmitting (point <cite>(1)</cite>). When the two short frames arrive at the location of host C, they collide and host C cannot decode them (point <cite>(2)</cite>). The two frames are absorbed by the ends of the wire. Neither host A nor host B have detected the collision. They both consider their frame to have been received correctly by its destination.</p>
<figure class="align-center" id="id87">
<a class="reference internal image-reference" href="../_images/frame-collision-short.png"><img alt="../_images/frame-collision-short.png" src="../Images/9a5d6d727ec45fd15b48e2c3fdc820d7.png" style="width: 438.9px; height: 298.9px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision-short.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 200 </span><span class="caption-text">The short-frame collision problem</span><a class="headerlink" href="#id87" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-22">To solve this problem, networks using CSMA/CD require hosts to transmit for at least <span class="math notranslate nohighlight">\(2\times\tau\)</span> seconds. Since the network transmission speed is fixed for a given network technology, this implies that a technology that uses CSMA/CD enforces a minimum frame size. In the most popular CSMA/CD technology, Ethernet, <span class="math notranslate nohighlight">\(2\times\tau\)</span> is called the <cite>slot time</cite> <a class="footnote-reference brackets" href="#fslottime" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.</p>
<p id="index-23">The last innovation introduced by CSMA/CD is the computation of the retransmission timeout. As for ALOHA, this timeout cannot be fixed, otherwise hosts could become synchronized and always retransmit at the same time. Setting such a timeout is always a compromise between the network access delay and the amount of collisions. A short timeout would lead to a low network access delay but with a higher risk of collisions. On the other hand, a long timeout would cause a long network access delay but a lower risk of collisions. The <cite>binary exponential back-off</cite> algorithm was introduced in CSMA/CD networks to solve this problem.</p>
<p>To understand <cite>binary exponential back-off</cite>, let us consider a collision caused by exactly two hosts. Once it has detected the collision, a host can either retransmit its frame immediately or defer its transmission for some time. If each colliding host flips a coin to decide whether to retransmit immediately or to defer its retransmission, four cases are possible :</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Both hosts retransmit immediately and a new collision occurs</p></li>
<li><p>The first host retransmits immediately and the second defers its retransmission</p></li>
<li><p>The second host retransmits immediately and the first defers its retransmission</p></li>
<li><p>Both hosts defer their retransmission and a new collision occurs</p></li>
</ol>
</div></blockquote>
<p>In the second and third cases, both hosts have flipped different coins. The delay chosen by the host that defers its retransmission should be long enough to ensure that its retransmission will not collide with the immediate retransmission of the other host. However the delay should not be longer than the time necessary to avoid the collision, because if both hosts decide to defer their transmission, the network will be idle during this delay. The <cite>slot time</cite> is the optimal delay since it is the shortest delay that ensures that the first host will be able to retransmit its frame completely without any collision.</p>
<p>If two hosts are competing, the algorithm above will avoid a second collision 50% of the time. However, if the network is heavily loaded, several hosts may be competing at the same time. In this case, the hosts should be able to automatically adapt their retransmission delay. The <cite>binary exponential back-off</cite> performs this adaptation based on the number of collisions that have affected a frame. After the first collision, the host flips a coin and waits 0 or 1 <cite>slot time</cite>. After the second collision, it generates a random number and waits 0, 1, 2 or 3 <cite>slot times</cite>, etc. The duration of the waiting time is doubled after each collision. The complete pseudo-code for the CSMA/CD algorithm is shown in the figure below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># CSMA/CD pseudo-code</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel_becomes_free</span><span class="p">)</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait_until</span> <span class="p">(</span><span class="n">end_of_frame</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">collision</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">collision</span> <span class="n">detected</span><span class="p">:</span>
        <span class="n">stop_transmitting</span><span class="p">()</span>
        <span class="n">send</span><span class="p">(</span><span class="n">jamming</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">random</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">r</span> <span class="o">*</span> <span class="n">slotTime</span><span class="p">)</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">inter</span><span class="o">-</span><span class="n">frame_delay</span><span class="p">)</span>
        <span class="k">break</span>  <span class="c1"># transmission was successful</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p>The inter-frame delay used in this pseudo-code is a short delay corresponding to the time required by a network adapter to switch from transmit to receive mode. It is also used to prevent a host from sending a continuous stream of frames without leaving any transmission opportunities for other hosts on the network. This contributes to the fairness of CSMA/CD. Despite this delay, there are still conditions where CSMA/CD is not completely fair <a class="reference internal" href="../bibliography.html#ry1994" id="id27"><span>[RY1994]</span></a>. Consider for example a network with two hosts : a server sending long frames and a client sending acknowledgments. Measurements reported in <a class="reference internal" href="../bibliography.html#ry1994" id="id28"><span>[RY1994]</span></a> have shown that there are situations where the client could suffer from repeated collisions that lead it to wait for long periods of time due to the exponential back-off algorithm.</p>
</section>
<section id="carrier-sense-multiple-access-with-collision-avoidance">
<span id="index-24"/><h3>Carrier Sense Multiple Access with Collision Avoidance<a class="headerlink" href="#carrier-sense-multiple-access-with-collision-avoidance" title="Link to this heading">#</a></h3>
<p>The <cite>Carrier Sense Multiple Access with Collision Avoidance</cite> (CSMA/CA) Medium Access Control algorithm was designed for the popular WiFi wireless network technology <a class="reference internal" href="../bibliography.html#ieee802-11" id="id29"><span>[IEEE802.11]</span></a>. CSMA/CA also senses the transmission channel before transmitting a frame. Furthermore, CSMA/CA tries to avoid collisions by carefully tuning the timers used by CSMA/CA devices.</p>
<p id="index-25">CSMA/CA uses acknowledgments like CSMA. Each frame contains a sequence number and a CRC. The CRC is used to detect transmission errors while the sequence number is used to avoid frame duplication. When a device receives a correct frame, it returns a special acknowledgment frame to the sender. CSMA/CA introduces a small delay, named <cite>Short Inter Frame Spacing</cite>  (SIFS), between the reception of a frame and the transmission of the acknowledgment frame. This delay corresponds to the time that is required to switch the radio of a device between the reception and transmission modes.</p>
<p id="index-26">Compared to CSMA, CSMA/CA defines more precisely when a device is allowed to send a frame. First, CSMA/CA defines two delays : <cite>DIFS</cite> and <cite>EIFS</cite>. To send a frame, a device must first wait until the channel has been idle for at least the <cite>Distributed Coordination Function Inter Frame Space</cite> (DIFS) if the previous frame was received correctly. However, if the previously received frame was corrupted, this indicates that there are collisions and the device must sense the channel idle for at least the <cite>Extended Inter Frame Space</cite> (EIFS), with <span class="math notranslate nohighlight">\(SIFS&lt;DIFS&lt;EIFS\)</span>. The exact values for SIFS, DIFS and EIFS depend on the underlying physical layer <a class="reference internal" href="../bibliography.html#ieee802-11" id="id30"><span>[IEEE802.11]</span></a>.</p>
<p>The figure below shows the basic operation of CSMA/CA devices. Before transmitting, host <cite>A</cite> verifies that the channel is empty for a long enough period. Then, its sends its data frame. After checking the validity of the received frame, the recipient sends an acknowledgment frame after a short SIFS delay. Host <cite>C</cite>, which does not participate in the frame exchange, senses the channel to be busy at the beginning of the data frame. Host <cite>C</cite> can use this information to determine how long the channel will be busy for. Note that as <span class="math notranslate nohighlight">\(SIFS&lt;DIFS&lt;EIFS\)</span>, even a device that would start to sense the channel immediately after the last bit of the data frame could not decide to transmit its own frame during the transmission of the acknowledgment frame.</p>
<figure class="align-center" id="id88">
<a class="reference internal image-reference" href="../_images/csmaca-1.png"><img alt="../_images/csmaca-1.png" src="../Images/a384b3211b0772e04aaf1c46baf44b77.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-1.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 201 </span><span class="caption-text">Operation of a CSMA/CA device</span><a class="headerlink" href="#id88" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-27">The main difficulty with CSMA/CA is when two or more devices transmit at the same time and cause collisions. This is illustrated in the figure below, assuming a fixed timeout after the transmission of a data frame. With CSMA/CA, the timeout after the transmission of a data frame is very small, since it corresponds to the SIFS plus the time required to transmit the acknowledgment frame.</p>
<figure class="align-center" id="id89">
<a class="reference internal image-reference" href="../_images/csmaca-2.png"><img alt="../_images/csmaca-2.png" src="../Images/0dbf88b43018a324dcb149e4a1fe2cae.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-2.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 202 </span><span class="caption-text">Collisions with CSMA/CA</span><a class="headerlink" href="#id89" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To deal with this problem, CSMA/CA relies on a backoff timer. This backoff timer is a random delay that is chosen by each device in a range that depends on the number of retransmissions for the current frame. The range grows exponentially with the retransmissions as in CSMA/CD. The minimum range for the backoff timer is <span class="math notranslate nohighlight">\([0,7*slotTime]\)</span> where the <cite>slotTime</cite> is a parameter that depends on the underlying physical layer. Compared to CSMA/CD’s exponential backoff, there are two important differences to notice. First, the initial range for the backoff timer is seven times larger. This is because it is impossible in CSMA/CA to detect collisions as they happen. With CSMA/CA, a collision may affect the entire frame while with CSMA/CD it can only affect the beginning of the frame. Second, a CSMA/CA device must regularly sense the transmission channel during its back off timer. If the channel becomes busy (i.e. because another device is transmitting), then the back off timer must be frozen until the channel becomes free again. Once the channel becomes free, the back off timer is restarted. This is in contrast with CSMA/CD where the back off is recomputed after each collision. This is illustrated in the figure below. Host <cite>A</cite> chooses a smaller backoff than host <cite>C</cite>. When <cite>C</cite> senses the channel to be busy, it freezes its backoff timer and only restarts it once the channel is free again.</p>
<figure class="align-center" id="id90">
<a class="reference internal image-reference" href="../_images/csmaca-3.png"><img alt="../_images/csmaca-3.png" src="../Images/9634d388481fa8d42a5b4b7de8b205c0.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-3.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 203 </span><span class="caption-text">Detailed example with CSMA/CA</span><a class="headerlink" href="#id90" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The pseudo-code below summarizes the operation of a CSMA/CA device. The values of the SIFS, DIFS, EIFS and <span class="math notranslate nohighlight">\(slotTime\)</span> depend on the underlying physical layer technology <a class="reference internal" href="../bibliography.html#ieee802-11" id="id31"><span>[IEEE802.11]</span></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># CSMA/CA simplified pseudo-code</span>
<span class="n">N</span><span class="o">=</span><span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait_until</span><span class="p">(</span><span class="n">free</span><span class="p">(</span><span class="n">channel</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">correct</span><span class="p">(</span><span class="n">last_frame</span><span class="p">):</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">channel_free_during_t</span> <span class="o">&gt;=</span> <span class="n">DIFS</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">channel_free_during_t</span> <span class="o">&gt;=</span> <span class="n">EIFS</span><span class="p">)</span>

    <span class="n">backoff_time</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">7</span> <span class="o">*</span> <span class="p">(</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))))</span> <span class="o">*</span> <span class="n">slotTime</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel</span> <span class="n">free</span> <span class="n">during</span> <span class="n">backoff_time</span><span class="p">)</span>
    <span class="c1"># backoff timer is frozen while channel is sensed to be busy</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">received</span><span class="p">(</span><span class="n">ack</span><span class="p">)</span>
        <span class="c1"># frame received correctly</span>
        <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># retransmission required</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p id="index-28">Another problem faced by wireless networks is often called the <cite>hidden station problem</cite>. In a wireless network, radio signals are not always propagated same way in all directions. For example, two devices separated by a wall may not be able to receive each other’s signal while they could both be receiving the signal produced by a third host. This is illustrated in the figure below, but it can happen in other environments. For example, two devices that are on different sides of a hill may not be able to receive each other’s signal while they are both able to receive the signal sent by a station at the top of the hill. Furthermore, the radio propagation conditions may change with time. For example, a truck may temporarily block the communication between two nearby devices.</p>
<figure class="align-center" id="id91">
<a class="reference internal image-reference" href="../_images/csmaca-hidden.png"><img alt="../_images/csmaca-hidden.png" src="../Images/1dd2db4f7a4f3df51403915d42254672.png" style="width: 350.0px; height: 162.39999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-hidden.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 204 </span><span class="caption-text">The hidden station problem</span><a class="headerlink" href="#id91" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-29">To avoid collisions in these situations, CSMA/CA allows devices to reserve the transmission channel for some time. This is done by using two control frames : <cite>Request To Send</cite> (RTS) and <cite>Clear To Send</cite> (CTS). Both are very short frames to minimize the risk of collisions. To reserve the transmission channel, a device sends a RTS frame to the intended recipient of the data frame. The RTS frame contains the duration of the requested reservation. The recipient replies, after a SIFS delay, with a CTS frame which also contains the duration of the reservation. As the duration of the reservation has been sent in both RTS and CTS, all hosts that could collide with either the sender or the reception of the data frame are informed of the reservation. They can compute the total duration of the transmission and defer their access to the transmission channel until then. This is illustrated in the figure below where host <cite>A</cite> reserves the transmission channel to send a data frame to host <cite>B</cite>. Host <cite>C</cite> notices the reservation and defers its transmission.</p>
<figure class="align-center" id="id92">
<a class="reference internal image-reference" href="../_images/csmaca-reserv.png"><img alt="../_images/csmaca-reserv.png" src="../Images/d32c0a1c69694ffdbad7f26e535e2135.png" style="width: 350.0px; height: 207.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-reserv.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 205 </span><span class="caption-text">Reservations with CSMA/CA</span><a class="headerlink" href="#id92" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The utilization of the reservations with CSMA/CA is an optimization that is useful when collisions are frequent. If there are few collisions, the time required to transmit the RTS and CTS frames can become significant and in particular when short frames are exchanged. Some devices only turn on RTS/CTS after transmission errors.</p>
</section>
<section id="deterministic-medium-access-control-algorithms">
<h3>Deterministic Medium Access Control algorithms<a class="headerlink" href="#deterministic-medium-access-control-algorithms" title="Link to this heading">#</a></h3>
<p>During the 1970s and 1980s, there were huge debates in the networking community about the best suited Medium Access Control algorithms for Local Area Networks. The optimistic algorithms that we have described until now were relatively easy to implement when they were designed. From a performance perspective, mathematical models and simulations showed the ability of these optimistic techniques to sustain load. However, none of the optimistic techniques are able to guarantee that a frame will be delivered within a given delay bound and some applications require predictable transmission delays. The deterministic MAC algorithms were considered by a fraction of the networking community as the best solution to fulfill the needs of Local Area Networks.</p>
<p>Both the proponents of the deterministic and the opportunistic techniques lobbied to develop standards for Local Area networks that would incorporate their solution. Instead of trying to find an impossible compromise between these diverging views, the IEEE 802 committee that was chartered to develop Local Area Network standards chose to work in parallel on three different LAN technologies and created three working groups. The <a class="reference external" href="http://www.ieee802.org/3/">IEEE 802.3 working group</a> became responsible for CSMA/CD. The proponents of deterministic MAC algorithms agreed on the basic principle of exchanging special frames called tokens between devices to regulate the access to the transmission medium. However, they did not agree on the most suitable physical layout for the network. IBM argued in favor of Ring-shaped networks while the manufacturing industry, led by General Motors, argued in favor of a bus-shaped network. This led to the creation of the <a class="reference external" href="http://www.ieee802.org/4/">IEEE 802.4 working group</a> to standardize Token Bus networks and the <a class="reference external" href="http://www.ieee802.org/5/">IEEE 802.5 working group</a> to standardize Token Ring networks. Although these techniques are not widely used anymore today, the principles behind a token-based protocol are still important.</p>
<p>The IEEE 802.5 Token Ring technology is defined in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id32"><span>[IEEE802.5]</span></a>. We use Token Ring as an example to explain the principles of the token-based MAC algorithms in ring-shaped networks. Other ring-shaped networks include the defunct FDDI <a class="reference internal" href="../bibliography.html#ross1989" id="id33"><span>[Ross1989]</span></a> or Resilient Pack Ring <a class="reference internal" href="../bibliography.html#dygu2004" id="id34"><span>[DYGU2004]</span></a> . A good survey of the early token ring networks may be found in <a class="reference internal" href="../bibliography.html#bux1989" id="id35"><span>[Bux1989]</span></a> .</p>
<p>A Token Ring network is composed of a set of stations that are attached to a unidirectional ring. The basic principle of the Token Ring MAC algorithm is that two types of frames travel on the ring : tokens and data frames. When the Token Ring starts, one of the stations sends the token. The token is a small frame that represents the authorization to transmit data frames on the ring. To transmit a data frame on the ring, a station must first capture the token by removing it from the ring. As only one station can capture the token at a time, the station that owns the token can safely transmit a data frame on the ring without risking collisions. After having transmitted its frame, the station must remove it from the ring and resend the token so that other stations can transmit their own frames.</p>
<figure class="align-center" id="id93">
<span id="fig-tokenring"/><a class="reference internal image-reference" href="../_images/token-ring.png"><img alt="../_images/token-ring.png" src="../Images/204991c6fea7862f915a3396c1578736.png" style="width: 350.0px; height: 154.7px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/token-ring.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 206 </span><span class="caption-text">A Token Ring network</span><a class="headerlink" href="#id93" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>While the basic principles of the Token Ring are simple, there are several subtle implementation details that add complexity to Token Ring networks. To understand these details let us analyze the operation of a Token Ring interface on a station. A Token Ring interface serves three different purposes. Like other LAN interfaces, it must be able to send and receive frames. In addition, a Token Ring interface is part of the ring, and as such, it must be able to forward the electrical signal that passes on the ring even when its station is powered off.</p>
<p>When powered-on, Token Ring interfaces operate in two different modes : <cite>listen</cite> and <cite>transmit</cite>. When operating in <cite>listen</cite> mode, a Token Ring interface receives an electrical signal from its upstream neighbor on the ring, introduces a delay equal to the transmission time of one bit on the ring and regenerates the signal before sending it to its downstream neighbor on the ring.</p>
<p>The first problem faced by a Token Ring network is that as the token represents the authorization to transmit, it must continuously travel on the ring when no data frame is being transmitted. Let us assume that a token has been produced and sent on the ring by one station. In Token Ring networks, the token is a 24 bits frame whose structure is shown below.</p>
<figure class="align-center" id="id94">
<span id="index-30"/><a class="reference internal image-reference" href="../_images/token-ring.svg"><img alt="../_images/token-ring.svg" src="../Images/d3fd5c0099436c887eea9ad8ae351d82.png" style="width: 453.0px; height: 128.0px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/token-ring.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 207 </span><span class="caption-text">802.5 token format</span><a class="headerlink" href="#id94" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-31">The token is composed of three fields. First, the <cite>Starting Delimiter</cite> is the marker that indicates the beginning of a frame. The first Token Ring networks used Manchester coding and the <cite>Starting Delimiter</cite> contained both symbols representing <cite>0</cite> and symbols that do not represent bits. The last field is the <cite>Ending Delimiter</cite> which marks the end of the token. The <cite>Access Control</cite> field is present in all frames, and contains several flags. The most important is the <cite>Token</cite> bit that is set in token frames and reset in other frames.</p>
<p id="index-32">Let us consider the five station network depicted in figure <a class="reference internal" href="#fig-tokenring"><span class="std std-ref">A Token Ring network</span></a> above and assume that station <cite>S1</cite> sends a token. If we neglect the propagation delay on the inter-station links, as each station introduces a one bit delay, the first bit of the frame would return to <cite>S1</cite> while it sends the fifth bit of the token. If station <cite>S1</cite> is powered off at that time, only the first five bits of the token will travel on the ring. To avoid this problem, there is a special station called the <cite>Monitor</cite> on each Token Ring. To ensure that the token can travel forever on the ring, this <cite>Monitor</cite> inserts a delay that is equal to at least 24 bit transmission times. If station <cite>S3</cite> was the <cite>Monitor</cite> in figure <a class="reference internal" href="#fig-tokenring"><span class="std std-ref">A Token Ring network</span></a>, <cite>S1</cite> would have been able to transmit the entire token before receiving the first bit of the token from its upstream neighbor.</p>
<p>Now that we have explained how the token can be forwarded on the ring, let us analyze how a station can capture a token to transmit a data frame. For this, we need some information about the format of the data frames. An 802.5 data frame begins with the <cite>Starting Delimiter</cite> followed by the <cite>Access Control</cite> field whose <cite>Token</cite> bit is reset, a <cite>Frame Control</cite> field that enables the definition of several types of frames, destination and source address, a payload, a CRC, the <cite>Ending Delimiter</cite> and a <cite>Frame Status</cite> field. The format of the Token Ring data frames is illustrated below.</p>
<figure class="align-center" id="id95">
<span id="index-33"/><a class="reference internal image-reference" href="../_images/8025.svg"><img alt="../_images/8025.svg" src="../Images/9f175bb0486e691ee2c9018db5a42d22.png" style="width: 604.0px; height: 320.0px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/8025.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 208 </span><span class="caption-text">802.5 data frame format</span><a class="headerlink" href="#id95" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To capture a token, a station must operate in <cite>Listen</cite> mode. In this mode, the station receives bits from its upstream neighbor. If the bits correspond to a data frame, they must be forwarded to the downstream neighbor. If they correspond to a token, the station can capture it and transmit its data frame. Both the data frame and the token are encoded as a bit string beginning with the <cite>Starting Delimiter</cite> followed by the <cite>Access Control</cite> field. When the station receives the first bit of a <cite>Starting Delimiter</cite>, it cannot know whether this is a data frame or a token and must forward the entire delimiter to its downstream neighbor. It is only when it receives the fourth bit of the <cite>Access Control</cite> field (i.e. the <cite>Token</cite> bit) that the station knows whether the frame is a data frame or a token. If the <cite>Token</cite> bit is reset, it indicates a data frame and the remaining bits of the data frame must be forwarded to the downstream station. Otherwise (<cite>Token</cite> bit is set), this is a token and the station can capture it by resetting the bit that is currently in its buffer. Thanks to this modification, the beginning of the token is now the beginning of a data frame and the station can switch to <cite>Transmit</cite> mode and send its data frame starting at the fifth bit of the <cite>Access Control</cite> field. Thus, the one-bit delay introduced by each Token Ring station plays a key role in enabling the stations to efficiently capture the token.</p>
<p>After having transmitted its data frame, the station must remain in <cite>Transmit</cite> mode until it has received the last bit of its own data frame. This ensures that the bits sent by a station do not remain in the network forever. A data frame sent by a station in a Token Ring network passes in front of all stations attached to the network. Each station can detect the data frame and analyze the destination address to possibly capture the frame.</p>
<p id="index-34">The text above describes the basic operation of a Token Ring network when all stations work correctly. Unfortunately, a real Token Ring network must be able to handle various types of anomalies and this increases the complexity of Token Ring stations. We briefly list the problems and outline their solutions below. A detailed description of the operation of Token Ring stations may be found in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id36"><span>[IEEE802.5]</span></a>. The first problem is when all the stations attached to the network start. One of them must bootstrap the network by sending the first token. For this, all stations implement a distributed election mechanism that is used to select the <cite>Monitor</cite>. Any station can become a <cite>Monitor</cite>. The <cite>Monitor</cite> manages the Token Ring network and ensures that it operates correctly. Its first role is to introduce a delay of 24 bit transmission times to ensure that the token can travel smoothly on the ring. Second, the <cite>Monitor</cite> sends the first token on the ring. It must also verify that the token passes regularly. According to the Token Ring standard <a class="reference internal" href="../bibliography.html#ieee802-5" id="id37"><span>[IEEE802.5]</span></a>, a station cannot retain the token to transmit data frames for a duration longer than the <cite>Token Holding Time</cite> (THT) (slightly less than 10 milliseconds). On a network containing <cite>N</cite> stations, the <cite>Monitor</cite> must receive the token at least every <span class="math notranslate nohighlight">\(N \times THT\)</span> seconds. If the <cite>Monitor</cite> does not receive a token during such a period, it cuts the ring for some time and then re-initializes the ring and sends a token.</p>
<p>Several other anomalies may occur in a Token Ring network. For example, a station could capture a token and be powered off before having resent the token. Another station could have captured the token, sent its data frame and be powered off before receiving all of its data frame. In this case, the bit string corresponding to the end of a frame would remain in the ring without being removed by its sender. Several techniques are defined in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id38"><span>[IEEE802.5]</span></a> to allow the <cite>Monitor</cite> to handle all these problems. If unfortunately, the <cite>Monitor</cite> fails, another station will be elected to become the new <cite>Monitor</cite>.</p>
</section>
&#13;

<h3>Static allocation methods<a class="headerlink" href="#static-allocation-methods" title="Link to this heading">#</a></h3>
<p>A first solution to share the available resources among all the devices attached to one Local Area Network is to define, <cite>a priori</cite>, the distribution of the transmission resources among the different devices. If <cite>N</cite> devices need to share the transmission capacities of a LAN operating at <cite>b</cite> Mbps, each device could be allocated a bandwidth of <span class="math notranslate nohighlight">\(\frac{b}{N}\)</span> Mbps.</p>
<p id="index-10">Limited resources need to be shared in other environments than Local Area Networks. Since the first radio transmissions by <a class="reference external" href="http://en.wikipedia.org/wiki/Guglielmo_Marconi">Marconi</a> more than one century ago, many applications that exchange information through radio signals have been developed. Each radio signal is an electromagnetic wave whose power is centered around a given frequency. The radio spectrum corresponds to frequencies ranging between roughly 3 KHz and 300 GHz. Frequency allocation plans negotiated among governments reserve most frequency ranges for specific applications such as broadcast radio, broadcast television, mobile communications, aeronautical radio navigation, amateur radio, satellite, etc. Each frequency range is then subdivided into channels and each channel can be reserved for a given application, e.g. a radio broadcaster in a given region.</p>
<p id="index-11"><cite>Frequency Division Multiplexing</cite> (FDM) is a static allocation scheme in which a frequency is allocated to each device attached to the shared medium. As each device uses a different transmission frequency, collisions cannot occur. In optical networks, a variant of FDM called <cite>Wavelength Division Multiplexing</cite> (WDM) can be used. An optical fiber can transport light at different wavelengths without interference. With WDM, a different wavelength is allocated to each of the devices that share the same optical fiber.</p>
<p id="index-12"><cite>Time Division Multiplexing</cite> (TDM) is a static bandwidth allocation method that was initially defined for the telephone network. In the fixed telephone network, a voice conversation is usually transmitted as a 64 Kbps signal. Thus, a telephone conservation generates 8 KBytes per second or one byte every 125 microseconds. Telephone conversations often need to be multiplexed together on a single line. For example, in Europe, thirty 64 Kbps voice signals are multiplexed over a single 2 Mbps (E1) line. This is done by using  <cite>Time Division Multiplexing</cite> (TDM). TDM divides the transmission opportunities into slots. In the telephone network, a slot corresponds to 125 microseconds. A position inside each slot is reserved for each voice signal. The figure below illustrates TDM on a link that is used to carry four voice conversations. The vertical lines represent the slot boundaries and the letters the different voice conversations. One byte from each voice conversation is sent during each 125 microseconds slot. The byte corresponding to a given conversation is always sent at the same position in each slot.</p>
<blockquote>
<div><div class="figure" id="id84" style="text-align: center"><p><img src="../Images/a5ce7017001779fb85df0a821c9dfa33.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-d77b364fc4d8103f3e76051b25f626c2864e0059.png"/></p>
<p><span class="caption-number">Fig. 196 </span><span class="caption-text">Time-division multiplexing</span></p>
</div></div></blockquote>
<p>TDM as shown above can be completely static, i.e. the same conversations always share the link, or dynamic. In the latter case, the two endpoints of the link must exchange messages specifying which conversation uses which byte inside each slot. Thanks to these control messages, it is possible to dynamically add and remove voice conversations from a given link.</p>
<p>TDM and FDM are widely used in telephone networks to support fixed bandwidth conversations. Using them in Local Area Networks that support computers would probably be inefficient. Computers usually do not send information at a fixed rate. Instead, they often have an on-off behavior. During the on period, the computer tries to send at the highest possible rate, e.g. to transfer a file. During the off period, which is often much longer than the on period, the computer does not transmit any packet. Using a static allocation scheme for computers attached to a LAN would lead to huge inefficiencies, as they would only be able to transmit at <span class="math notranslate nohighlight">\(\frac{1}{N}\)</span> of the total bandwidth during their on period, despite the fact that the other computers are in their off period and thus do not need to transmit any information. The dynamic MAC algorithms discussed in the remainder of this chapter aim to solve this problem.</p>
&#13;

<h3>ALOHA<a class="headerlink" href="#aloha" title="Link to this heading">#</a></h3>
<p id="index-13">In the 1960s, computers were mainly mainframes with a few dozen terminals attached to them. These terminals were usually in the same building as the mainframe and were directly connected to it. In some cases, the terminals were installed in remote locations and connected through a <a class="reference internal" href="../glossary.html#term-modem"><span class="xref std std-term">modem</span></a> attached to a <a class="reference internal" href="../glossary.html#term-dial-up-line"><span class="xref std std-term">dial-up  line</span></a>. The university of Hawaii chose a different organization. Instead of using telephone lines to connect the distant terminals, they developed the first <cite>packet radio</cite> technology <a class="reference internal" href="../bibliography.html#abramson1970" id="id10"><span>[Abramson1970]</span></a>. Until then, computer networks were built on top of either the telephone network or physical cables. ALOHANet showed that it is possible to use radio signals to interconnect computers.</p>
<p id="index-14">The first version of ALOHANet, described in <a class="reference internal" href="../bibliography.html#abramson1970" id="id11"><span>[Abramson1970]</span></a>, operated as follows. First, the terminals and the mainframe exchanged fixed-length frames composed of 704 bits. Each frame contained 80 8-bit characters, some control bits and parity information to detect transmission errors. Two channels in the 400 MHz range were reserved for the operation of ALOHANet. The first channel was used by the mainframe to send frames to all terminals. The second channel was shared among all terminals to send frames to the mainframe. As all terminals share the same transmission channel, there is a risk of collision. To deal with this problem as well as transmission errors, the mainframe verified the parity bits of the received frame and sent an acknowledgment on its channel for each correctly received frame. The terminals on the other hand had to retransmit the unacknowledged frames. As for TCP, retransmitting these frames immediately upon expiration of a fixed timeout is not a good approach as several terminals may retransmit their frames at the same time leading to a network collapse. A better approach, but still far from perfect, is for each terminal to wait a random amount of time after the expiration of its retransmission timeout. This avoids synchronization among multiple retransmitting terminals.</p>
<p>The pseudo-code below shows the operation of an ALOHANet terminal. We use this python syntax for all Medium Access Control algorithms described in this chapter. The algorithm is applied to each new frame that needs to be transmitted. It attempts to transmit a frame at most <cite>max</cite> times (<cite>while loop</cite>). Each transmission attempt is performed as follows. First, the frame is sent. Each frame is protected by a timeout. Then, the terminal waits for either a valid acknowledgment frame or the expiration of its timeout. If the terminal receives an acknowledgment, the frame has been delivered correctly and the algorithm terminates. Otherwise, the terminal waits for a random time and attempts to retransmit the frame.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># ALOHA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack_on_return_channel</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ack_on_return_channel</span><span class="p">):</span>
       <span class="k">break</span>  <span class="c1"># transmission was successful</span>
    <span class="k">else</span><span class="p">:</span>
             <span class="c1"># timeout</span>
             <span class="n">wait</span><span class="p">(</span><span class="n">random_time</span><span class="p">)</span>
             <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p><a class="reference internal" href="../bibliography.html#abramson1970" id="id12"><span>[Abramson1970]</span></a> analyzed the performance of ALOHANet under particular assumptions and found that ALOHANet worked well when the channel was lightly loaded. In this case, the frames are rarely retransmitted and the <cite>channel traffic</cite>, i.e. the total number of (correct and retransmitted) frames transmitted per unit of time is close to the <cite>channel utilization</cite>, i.e. the number of correctly transmitted frames per unit of time. Unfortunately, the analysis also reveals that the <cite>channel utilization</cite> reaches its maximum at <span class="math notranslate nohighlight">\(\frac{1}{2 \times e}=0.186\)</span> times the channel bandwidth. At higher utilization, ALOHANet becomes unstable and the network collapses due to collided retransmissions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Amateur packet radio</p>
<p>Packet radio technologies have evolved in various directions since the first experiments performed at the University of Hawaii. The Amateur packet radio service developed by amateur radio operators is one of the descendants ALOHANet. Many amateur radio operators are very interested in new technologies and they often spend countless hours developing new antennas or transceivers. When the first personal computers appeared, several amateur radio operators designed radio modems and their own datalink layer protocols <a class="reference internal" href="../bibliography.html#kpd1985" id="id13"><span>[KPD1985]</span></a> <a class="reference internal" href="../bibliography.html#bnt1997" id="id14"><span>[BNT1997]</span></a>. This network grew and it was possible to connect to servers in several European countries by only using packet radio relays. Some amateur radio operators also developed TCP/IP protocol stacks that were used over the packet radio service. Some parts of the <a class="reference external" href="http://www.ampr.org/">amateur packet radio network</a> are connected to the global Internet and use the <cite>44.0.0.0/8</cite> IPv4 prefix.</p>
</div>
<p id="index-15">Many improvements to ALOHANet have been proposed since the publication of <a class="reference internal" href="../bibliography.html#abramson1970" id="id15"><span>[Abramson1970]</span></a>, and this technique, or some of its variants, are still found in wireless networks today. The slotted technique proposed in <a class="reference internal" href="../bibliography.html#roberts1975" id="id16"><span>[Roberts1975]</span></a> is important because it shows that a simple modification can significantly improve channel utilization. Instead of allowing all terminals to transmit at any time, <a class="reference internal" href="../bibliography.html#roberts1975" id="id17"><span>[Roberts1975]</span></a> proposed to divide time into slots and allow terminals to transmit only at the beginning of each slot. Each slot corresponds to the time required to transmit one fixed size frame. In practice, these slots can be imposed by a single clock that is received by all terminals. In ALOHANet, it could have been located on the central mainframe. The analysis in <a class="reference internal" href="../bibliography.html#roberts1975" id="id18"><span>[Roberts1975]</span></a> reveals that this simple modification improves the channel utilization by a factor of two.</p>
&#13;

<span id="index-16"/><h3>Carrier Sense Multiple Access<a class="headerlink" href="#carrier-sense-multiple-access" title="Link to this heading">#</a></h3>
<p>ALOHA and slotted ALOHA can easily be implemented, but unfortunately, they can only be used in networks that are very lightly loaded. Designing a network for a very low utilization is possible, but it clearly increases the cost of the network. To overcome the problems of ALOHA, many Medium Access Control mechanisms have been proposed which improve channel utilization. Carrier Sense Multiple Access (CSMA) is a significant improvement compared to ALOHA. CSMA requires all nodes to listen to the transmission channel to verify that it is free before transmitting a frame <a class="reference internal" href="../bibliography.html#kt1975" id="id19"><span>[KT1975]</span></a>. When a node senses the channel to be busy, it defers its transmission until the channel becomes free again. The pseudo-code below provides a more detailed description of the operation of CSMA.</p>
<div class="highlight-python notranslate" id="index-17"><div class="highlight"><pre><span/><span class="c1"># persistent CSMA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel_becomes_free</span><span class="p">)</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ack</span><span class="p">:</span>
       <span class="k">break</span>  <span class="c1"># transmission was successful</span>
    <span class="k">else</span><span class="p">:</span>
             <span class="c1"># timeout</span>
             <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p>The above pseudo-code is often called <cite>persistent CSMA</cite> <a class="reference internal" href="../bibliography.html#kt1975" id="id20"><span>[KT1975]</span></a> as the terminal will continuously listen to the channel and transmit its frame as soon as the channel becomes free. Another important variant of CSMA is the <cite>non-persistent CSMA</cite> <a class="reference internal" href="../bibliography.html#kt1975" id="id21"><span>[KT1975]</span></a>. The main difference between persistent and non-persistent CSMA described in the pseudo-code below is that a non-persistent CSMA node does not continuously listen to the channel to determine when it becomes free. When a non-persistent CSMA terminal senses the transmission channel to be busy, it waits for a random time before sensing the channel again. This improves channel utilization compared to persistent CSMA. With persistent CSMA, when two terminals sense the channel to be busy, they will both transmit (and thus cause a collision) as soon as the channel becomes free. With non-persistent CSMA, this synchronization does not occur, as the terminals wait a random time after having sensed the transmission channel. However, the higher channel utilization achieved by non-persistent CSMA comes at the expense of a slightly higher waiting time in the terminals when the network is lightly loaded.</p>
<div class="highlight-python notranslate" id="index-18"><div class="highlight"><pre><span/><span class="c1"># Non persistent CSMA</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">listen</span><span class="p">(</span><span class="n">channel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">free</span><span class="p">(</span><span class="n">channel</span><span class="p">):</span>
        <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">received</span><span class="p">(</span><span class="n">ack</span><span class="p">):</span>
           <span class="k">break</span>  <span class="c1"># transmission was successful</span>
        <span class="k">else</span><span class="p">:</span>
                 <span class="c1"># timeout</span>
                 <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">random_time</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p><a class="reference internal" href="../bibliography.html#kt1975" id="id22"><span>[KT1975]</span></a> analyzes in detail the performance of several CSMA variants. Under some assumptions about the transmission channel and the traffic, the analysis compares ALOHA, slotted ALOHA, persistent and non-persistent CSMA. Under these assumptions, ALOHA achieves a channel utilization of only 18.4% of the channel capacity. Slotted ALOHA is able to use 36.6% of this capacity. Persistent CSMA improves the utilization by reaching 52.9% of the capacity while non-persistent CSMA achieves 81.5% of the channel capacity.</p>
&#13;

<span id="index-19"/><h3>Carrier Sense Multiple Access with Collision Detection<a class="headerlink" href="#carrier-sense-multiple-access-with-collision-detection" title="Link to this heading">#</a></h3>
<p id="index-20">CSMA improves channel utilization compared to ALOHA. However, the performance can still be improved, especially in wired networks. Consider the situation of two terminals that are connected to the same cable. This cable could, for example, be a coaxial cable as in the early days of Ethernet <a class="reference internal" href="../bibliography.html#metcalfe1976" id="id23"><span>[Metcalfe1976]</span></a>. It could also be built with twisted pairs. Before extending CSMA, it is useful to understand, more intuitively, how frames are transmitted in such a network and how collisions can occur. The figure below illustrates the physical transmission of a frame on such a cable. To transmit its frame, host A must send an electrical signal on the shared medium. The first step is thus to begin the transmission of the electrical signal. This is point <cite>(1)</cite> in the figure below. This electrical signal will travel along the cable. Although electrical signals travel fast, we know that information cannot travel faster than the speed of light (i.e. 300.000 kilometers/second). On a coaxial cable, an electrical signal is slightly slower than the speed of light and 200.000 kilometers per second is a reasonable estimation. This implies that if the cable has a length of one kilometer, the electrical signal will need 5 microseconds to travel from one end of the cable to the other. The ends of coaxial cables are equipped with termination points that ensure that the electrical signal is not reflected back to its source. This is illustrated at point <cite>(3)</cite> in the figure, where the electrical signal has reached the left endpoint and host B. At this point, B starts to receive the frame being transmitted by A. Notice that there is a delay between the transmission of a bit on host A and its reception by host B. If there were other hosts attached to the cable, they would receive the first bit of the frame at slightly different times. As we will see later, this timing difference is a key problem for MAC algorithms. At point <cite>(4)</cite>, the electrical signal has reached both ends of the cable and occupies it completely. Host A continues to transmit the electrical signal until the end of the frame. As shown at point <cite>(5)</cite>, when the sending host stops its transmission, the electrical signal corresponding to the end of the frame leaves the coaxial cable. The channel becomes empty again once the entire electrical signal has been removed from the cable.</p>
<figure class="align-center" id="id85">
<a class="reference internal image-reference" href="../_images/frame-bus.png"><img alt="../_images/frame-bus.png" src="../Images/ab89b8b4019e54d32dc8e8a85f104ba1.png" style="width: 452.2px; height: 354.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-bus.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 197 </span><span class="caption-text">Frame transmission on a shared bus</span><a class="headerlink" href="#id85" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Now that we have looked at how a frame is actually transmitted as an electrical signal on a shared bus, it is interesting to look in more detail at what happens when two hosts transmit a frame at almost the same time. This is illustrated in the figure below, where hosts A and B start their transmission at the same time (point <cite>(1)</cite>). At this time, if host C senses the channel, it will consider it to be free. This will not last a long time and at point <cite>(2)</cite> the electrical signals from both host A and host B reach host C. The combined electrical signal (shown graphically as the superposition of the two curves in the figure) cannot be decoded by host C. Host C detects a collision, as it receives a signal that it cannot decode. Since host C cannot decode the frames, it cannot determine which hosts are sending the colliding frames. Note that host A (and host B) will detect the collision after host C (point <cite>(3)</cite> in figure <a class="reference internal" href="#fig-collision-bus"><span class="std std-numref">Fig. 198</span></a>).</p>
<figure class="align-center" id="fig-collision-bus">
<a class="reference internal image-reference" href="../_images/frame-collision.png"><img alt="../_images/frame-collision.png" src="../Images/7b88fb2c9cc28c5857f616b145a56cb6.png" style="width: 446.59999999999997px; height: 298.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 198 </span><span class="caption-text">Frame collision on a shared bus</span><a class="headerlink" href="#fig-collision-bus" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-21">As shown above, hosts detect collisions when they receive an electrical signal that they cannot decode. In a wired network, a host is able to detect such a collision both while it is listening (e.g. like host C in the figure above) and also while it is sending its own frame. When a host transmits a frame, it can compare the electrical signal that it transmits with the electrical signal that it senses on the wire. At points <cite>(1)</cite> and <cite>(2)</cite> in the figure above, host A senses only its own signal. At point <cite>(3)</cite>, it senses an electrical signal that differs from its own signal and can thus detects the collision. At this point, its frame is corrupted and it can stop its transmission. The ability to detect collisions while transmitting is the starting point for the <cite>Carrier Sense Multiple Access with Collision Detection (CSMA/CD)</cite> Medium Access Control algorithm, which is used in Ethernet networks <a class="reference internal" href="../bibliography.html#metcalfe1976" id="id24"><span>[Metcalfe1976]</span></a> <a class="reference internal" href="../bibliography.html#ieee802-3" id="id25"><span>[IEEE802.3]</span></a> . When an Ethernet host detects a collision while it is transmitting, it immediately stops its transmission. Compared with pure CSMA, CSMA/CD is an important improvement since when collisions occur, they only last until colliding hosts have detected it and stopped their transmission. In practice, when a host detects a collision, it sends a special jamming signal on the cable to ensure that all hosts have detected the collision.</p>
<p>To better understand these collisions, it is useful to analyze what would be the worst collision on a shared bus network. Let us consider a wire with two hosts attached at both ends, as shown in the figure below. Host A starts to transmit its frame and its electrical signal is propagated on the cable. Its propagation time depends on the physical length of the cable and the speed of the electrical signal. Let us use <span class="math notranslate nohighlight">\(\tau\)</span> to represent this propagation delay in seconds. Slightly less than <span class="math notranslate nohighlight">\(\tau\)</span> seconds after the beginning of the transmission of A’s frame, B decides to start transmitting its own frame. After <span class="math notranslate nohighlight">\(\epsilon\)</span> seconds, B senses A’s frame, detects the collision and stops transmitting. The beginning of B’s frame travels on the cable until it reaches host A. Host A can thus detect the collision at time <span class="math notranslate nohighlight">\(\tau-\epsilon+\tau \approx 2\times\tau\)</span>. An important point to note is that a collision can only occur during the first <span class="math notranslate nohighlight">\(2\times\tau\)</span> seconds of its transmission. If a collision did not occur during this period, it cannot occur afterwards since the transmission channel is busy after <span class="math notranslate nohighlight">\(\tau\)</span> seconds and CSMA/CD hosts sense the transmission channel before transmitting their frame.</p>
<figure class="align-center" id="id86">
<a class="reference internal image-reference" href="../_images/frame-collision-worst.png"><img alt="../_images/frame-collision-worst.png" src="../Images/cb963f4bef31245667608aad2e590553.png" style="width: 448.7px; height: 208.6px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision-worst.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 199 </span><span class="caption-text">The worst collision on a shared bus</span><a class="headerlink" href="#id86" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Furthermore, on the wired networks where CSMA/CD is used, collisions are almost the only cause of transmission errors that affect frames. Transmission errors that only affect a few bits inside a frame seldom occur in these wired networks. For this reason, the designers of CSMA/CD chose to completely remove the acknowledgment frames in the datalink layer. When a host transmits a frame, it verifies whether its transmission has been affected by a collision. If not, given the negligible Bit Error Ratio of the underlying network, it assumes that the frame was received correctly by its destination. Otherwise the frame is retransmitted after some delay.</p>
<p>Removing acknowledgments is an interesting optimization as it reduces the number of frames that are exchanged on the network and the number of frames that need to be processed by the hosts. However, to use this optimization, we must ensure that all hosts will be able to detect all the collisions that affect their frames. The problem is important for short frames. Let us consider two hosts, A and B, that are sending a small frame to host C as illustrated in the figure below. If the frames sent by A and B are very short, the situation illustrated below may occur. Hosts A and B send their frame and stop transmitting (point <cite>(1)</cite>). When the two short frames arrive at the location of host C, they collide and host C cannot decode them (point <cite>(2)</cite>). The two frames are absorbed by the ends of the wire. Neither host A nor host B have detected the collision. They both consider their frame to have been received correctly by its destination.</p>
<figure class="align-center" id="id87">
<a class="reference internal image-reference" href="../_images/frame-collision-short.png"><img alt="../_images/frame-collision-short.png" src="../Images/9a5d6d727ec45fd15b48e2c3fdc820d7.png" style="width: 438.9px; height: 298.9px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/frame-collision-short.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 200 </span><span class="caption-text">The short-frame collision problem</span><a class="headerlink" href="#id87" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-22">To solve this problem, networks using CSMA/CD require hosts to transmit for at least <span class="math notranslate nohighlight">\(2\times\tau\)</span> seconds. Since the network transmission speed is fixed for a given network technology, this implies that a technology that uses CSMA/CD enforces a minimum frame size. In the most popular CSMA/CD technology, Ethernet, <span class="math notranslate nohighlight">\(2\times\tau\)</span> is called the <cite>slot time</cite> <a class="footnote-reference brackets" href="#fslottime" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.</p>
<p id="index-23">The last innovation introduced by CSMA/CD is the computation of the retransmission timeout. As for ALOHA, this timeout cannot be fixed, otherwise hosts could become synchronized and always retransmit at the same time. Setting such a timeout is always a compromise between the network access delay and the amount of collisions. A short timeout would lead to a low network access delay but with a higher risk of collisions. On the other hand, a long timeout would cause a long network access delay but a lower risk of collisions. The <cite>binary exponential back-off</cite> algorithm was introduced in CSMA/CD networks to solve this problem.</p>
<p>To understand <cite>binary exponential back-off</cite>, let us consider a collision caused by exactly two hosts. Once it has detected the collision, a host can either retransmit its frame immediately or defer its transmission for some time. If each colliding host flips a coin to decide whether to retransmit immediately or to defer its retransmission, four cases are possible :</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Both hosts retransmit immediately and a new collision occurs</p></li>
<li><p>The first host retransmits immediately and the second defers its retransmission</p></li>
<li><p>The second host retransmits immediately and the first defers its retransmission</p></li>
<li><p>Both hosts defer their retransmission and a new collision occurs</p></li>
</ol>
</div></blockquote>
<p>In the second and third cases, both hosts have flipped different coins. The delay chosen by the host that defers its retransmission should be long enough to ensure that its retransmission will not collide with the immediate retransmission of the other host. However the delay should not be longer than the time necessary to avoid the collision, because if both hosts decide to defer their transmission, the network will be idle during this delay. The <cite>slot time</cite> is the optimal delay since it is the shortest delay that ensures that the first host will be able to retransmit its frame completely without any collision.</p>
<p>If two hosts are competing, the algorithm above will avoid a second collision 50% of the time. However, if the network is heavily loaded, several hosts may be competing at the same time. In this case, the hosts should be able to automatically adapt their retransmission delay. The <cite>binary exponential back-off</cite> performs this adaptation based on the number of collisions that have affected a frame. After the first collision, the host flips a coin and waits 0 or 1 <cite>slot time</cite>. After the second collision, it generates a random number and waits 0, 1, 2 or 3 <cite>slot times</cite>, etc. The duration of the waiting time is doubled after each collision. The complete pseudo-code for the CSMA/CD algorithm is shown in the figure below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># CSMA/CD pseudo-code</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel_becomes_free</span><span class="p">)</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait_until</span> <span class="p">(</span><span class="n">end_of_frame</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">collision</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">collision</span> <span class="n">detected</span><span class="p">:</span>
        <span class="n">stop_transmitting</span><span class="p">()</span>
        <span class="n">send</span><span class="p">(</span><span class="n">jamming</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">random</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">r</span> <span class="o">*</span> <span class="n">slotTime</span><span class="p">)</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">inter</span><span class="o">-</span><span class="n">frame_delay</span><span class="p">)</span>
        <span class="k">break</span>  <span class="c1"># transmission was successful</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p>The inter-frame delay used in this pseudo-code is a short delay corresponding to the time required by a network adapter to switch from transmit to receive mode. It is also used to prevent a host from sending a continuous stream of frames without leaving any transmission opportunities for other hosts on the network. This contributes to the fairness of CSMA/CD. Despite this delay, there are still conditions where CSMA/CD is not completely fair <a class="reference internal" href="../bibliography.html#ry1994" id="id27"><span>[RY1994]</span></a>. Consider for example a network with two hosts : a server sending long frames and a client sending acknowledgments. Measurements reported in <a class="reference internal" href="../bibliography.html#ry1994" id="id28"><span>[RY1994]</span></a> have shown that there are situations where the client could suffer from repeated collisions that lead it to wait for long periods of time due to the exponential back-off algorithm.</p>
&#13;

<span id="index-24"/><h3>Carrier Sense Multiple Access with Collision Avoidance<a class="headerlink" href="#carrier-sense-multiple-access-with-collision-avoidance" title="Link to this heading">#</a></h3>
<p>The <cite>Carrier Sense Multiple Access with Collision Avoidance</cite> (CSMA/CA) Medium Access Control algorithm was designed for the popular WiFi wireless network technology <a class="reference internal" href="../bibliography.html#ieee802-11" id="id29"><span>[IEEE802.11]</span></a>. CSMA/CA also senses the transmission channel before transmitting a frame. Furthermore, CSMA/CA tries to avoid collisions by carefully tuning the timers used by CSMA/CA devices.</p>
<p id="index-25">CSMA/CA uses acknowledgments like CSMA. Each frame contains a sequence number and a CRC. The CRC is used to detect transmission errors while the sequence number is used to avoid frame duplication. When a device receives a correct frame, it returns a special acknowledgment frame to the sender. CSMA/CA introduces a small delay, named <cite>Short Inter Frame Spacing</cite>  (SIFS), between the reception of a frame and the transmission of the acknowledgment frame. This delay corresponds to the time that is required to switch the radio of a device between the reception and transmission modes.</p>
<p id="index-26">Compared to CSMA, CSMA/CA defines more precisely when a device is allowed to send a frame. First, CSMA/CA defines two delays : <cite>DIFS</cite> and <cite>EIFS</cite>. To send a frame, a device must first wait until the channel has been idle for at least the <cite>Distributed Coordination Function Inter Frame Space</cite> (DIFS) if the previous frame was received correctly. However, if the previously received frame was corrupted, this indicates that there are collisions and the device must sense the channel idle for at least the <cite>Extended Inter Frame Space</cite> (EIFS), with <span class="math notranslate nohighlight">\(SIFS&lt;DIFS&lt;EIFS\)</span>. The exact values for SIFS, DIFS and EIFS depend on the underlying physical layer <a class="reference internal" href="../bibliography.html#ieee802-11" id="id30"><span>[IEEE802.11]</span></a>.</p>
<p>The figure below shows the basic operation of CSMA/CA devices. Before transmitting, host <cite>A</cite> verifies that the channel is empty for a long enough period. Then, its sends its data frame. After checking the validity of the received frame, the recipient sends an acknowledgment frame after a short SIFS delay. Host <cite>C</cite>, which does not participate in the frame exchange, senses the channel to be busy at the beginning of the data frame. Host <cite>C</cite> can use this information to determine how long the channel will be busy for. Note that as <span class="math notranslate nohighlight">\(SIFS&lt;DIFS&lt;EIFS\)</span>, even a device that would start to sense the channel immediately after the last bit of the data frame could not decide to transmit its own frame during the transmission of the acknowledgment frame.</p>
<figure class="align-center" id="id88">
<a class="reference internal image-reference" href="../_images/csmaca-1.png"><img alt="../_images/csmaca-1.png" src="../Images/a384b3211b0772e04aaf1c46baf44b77.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-1.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 201 </span><span class="caption-text">Operation of a CSMA/CA device</span><a class="headerlink" href="#id88" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-27">The main difficulty with CSMA/CA is when two or more devices transmit at the same time and cause collisions. This is illustrated in the figure below, assuming a fixed timeout after the transmission of a data frame. With CSMA/CA, the timeout after the transmission of a data frame is very small, since it corresponds to the SIFS plus the time required to transmit the acknowledgment frame.</p>
<figure class="align-center" id="id89">
<a class="reference internal image-reference" href="../_images/csmaca-2.png"><img alt="../_images/csmaca-2.png" src="../Images/0dbf88b43018a324dcb149e4a1fe2cae.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-2.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 202 </span><span class="caption-text">Collisions with CSMA/CA</span><a class="headerlink" href="#id89" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To deal with this problem, CSMA/CA relies on a backoff timer. This backoff timer is a random delay that is chosen by each device in a range that depends on the number of retransmissions for the current frame. The range grows exponentially with the retransmissions as in CSMA/CD. The minimum range for the backoff timer is <span class="math notranslate nohighlight">\([0,7*slotTime]\)</span> where the <cite>slotTime</cite> is a parameter that depends on the underlying physical layer. Compared to CSMA/CD’s exponential backoff, there are two important differences to notice. First, the initial range for the backoff timer is seven times larger. This is because it is impossible in CSMA/CA to detect collisions as they happen. With CSMA/CA, a collision may affect the entire frame while with CSMA/CD it can only affect the beginning of the frame. Second, a CSMA/CA device must regularly sense the transmission channel during its back off timer. If the channel becomes busy (i.e. because another device is transmitting), then the back off timer must be frozen until the channel becomes free again. Once the channel becomes free, the back off timer is restarted. This is in contrast with CSMA/CD where the back off is recomputed after each collision. This is illustrated in the figure below. Host <cite>A</cite> chooses a smaller backoff than host <cite>C</cite>. When <cite>C</cite> senses the channel to be busy, it freezes its backoff timer and only restarts it once the channel is free again.</p>
<figure class="align-center" id="id90">
<a class="reference internal image-reference" href="../_images/csmaca-3.png"><img alt="../_images/csmaca-3.png" src="../Images/9634d388481fa8d42a5b4b7de8b205c0.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-3.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 203 </span><span class="caption-text">Detailed example with CSMA/CA</span><a class="headerlink" href="#id90" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The pseudo-code below summarizes the operation of a CSMA/CA device. The values of the SIFS, DIFS, EIFS and <span class="math notranslate nohighlight">\(slotTime\)</span> depend on the underlying physical layer technology <a class="reference internal" href="../bibliography.html#ieee802-11" id="id31"><span>[IEEE802.11]</span></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># CSMA/CA simplified pseudo-code</span>
<span class="n">N</span><span class="o">=</span><span class="mi">1</span>
<span class="k">while</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="nb">max</span><span class="p">:</span>
    <span class="n">wait_until</span><span class="p">(</span><span class="n">free</span><span class="p">(</span><span class="n">channel</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">correct</span><span class="p">(</span><span class="n">last_frame</span><span class="p">):</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">channel_free_during_t</span> <span class="o">&gt;=</span> <span class="n">DIFS</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">wait</span><span class="p">(</span><span class="n">channel_free_during_t</span> <span class="o">&gt;=</span> <span class="n">EIFS</span><span class="p">)</span>

    <span class="n">backoff_time</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">7</span> <span class="o">*</span> <span class="p">(</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))))</span> <span class="o">*</span> <span class="n">slotTime</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">channel</span> <span class="n">free</span> <span class="n">during</span> <span class="n">backoff_time</span><span class="p">)</span>
    <span class="c1"># backoff timer is frozen while channel is sensed to be busy</span>
    <span class="n">send</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">wait</span><span class="p">(</span><span class="n">ack</span> <span class="ow">or</span> <span class="n">timeout</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">received</span><span class="p">(</span><span class="n">ack</span><span class="p">)</span>
        <span class="c1"># frame received correctly</span>
        <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># retransmission required</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Too many transmission attempts</span>
</pre></div>
</div>
<p id="index-28">Another problem faced by wireless networks is often called the <cite>hidden station problem</cite>. In a wireless network, radio signals are not always propagated same way in all directions. For example, two devices separated by a wall may not be able to receive each other’s signal while they could both be receiving the signal produced by a third host. This is illustrated in the figure below, but it can happen in other environments. For example, two devices that are on different sides of a hill may not be able to receive each other’s signal while they are both able to receive the signal sent by a station at the top of the hill. Furthermore, the radio propagation conditions may change with time. For example, a truck may temporarily block the communication between two nearby devices.</p>
<figure class="align-center" id="id91">
<a class="reference internal image-reference" href="../_images/csmaca-hidden.png"><img alt="../_images/csmaca-hidden.png" src="../Images/1dd2db4f7a4f3df51403915d42254672.png" style="width: 350.0px; height: 162.39999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-hidden.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 204 </span><span class="caption-text">The hidden station problem</span><a class="headerlink" href="#id91" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-29">To avoid collisions in these situations, CSMA/CA allows devices to reserve the transmission channel for some time. This is done by using two control frames : <cite>Request To Send</cite> (RTS) and <cite>Clear To Send</cite> (CTS). Both are very short frames to minimize the risk of collisions. To reserve the transmission channel, a device sends a RTS frame to the intended recipient of the data frame. The RTS frame contains the duration of the requested reservation. The recipient replies, after a SIFS delay, with a CTS frame which also contains the duration of the reservation. As the duration of the reservation has been sent in both RTS and CTS, all hosts that could collide with either the sender or the reception of the data frame are informed of the reservation. They can compute the total duration of the transmission and defer their access to the transmission channel until then. This is illustrated in the figure below where host <cite>A</cite> reserves the transmission channel to send a data frame to host <cite>B</cite>. Host <cite>C</cite> notices the reservation and defers its transmission.</p>
<figure class="align-center" id="id92">
<a class="reference internal image-reference" href="../_images/csmaca-reserv.png"><img alt="../_images/csmaca-reserv.png" src="../Images/d32c0a1c69694ffdbad7f26e535e2135.png" style="width: 350.0px; height: 207.2px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/csmaca-reserv.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 205 </span><span class="caption-text">Reservations with CSMA/CA</span><a class="headerlink" href="#id92" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The utilization of the reservations with CSMA/CA is an optimization that is useful when collisions are frequent. If there are few collisions, the time required to transmit the RTS and CTS frames can become significant and in particular when short frames are exchanged. Some devices only turn on RTS/CTS after transmission errors.</p>
&#13;

<h3>Deterministic Medium Access Control algorithms<a class="headerlink" href="#deterministic-medium-access-control-algorithms" title="Link to this heading">#</a></h3>
<p>During the 1970s and 1980s, there were huge debates in the networking community about the best suited Medium Access Control algorithms for Local Area Networks. The optimistic algorithms that we have described until now were relatively easy to implement when they were designed. From a performance perspective, mathematical models and simulations showed the ability of these optimistic techniques to sustain load. However, none of the optimistic techniques are able to guarantee that a frame will be delivered within a given delay bound and some applications require predictable transmission delays. The deterministic MAC algorithms were considered by a fraction of the networking community as the best solution to fulfill the needs of Local Area Networks.</p>
<p>Both the proponents of the deterministic and the opportunistic techniques lobbied to develop standards for Local Area networks that would incorporate their solution. Instead of trying to find an impossible compromise between these diverging views, the IEEE 802 committee that was chartered to develop Local Area Network standards chose to work in parallel on three different LAN technologies and created three working groups. The <a class="reference external" href="http://www.ieee802.org/3/">IEEE 802.3 working group</a> became responsible for CSMA/CD. The proponents of deterministic MAC algorithms agreed on the basic principle of exchanging special frames called tokens between devices to regulate the access to the transmission medium. However, they did not agree on the most suitable physical layout for the network. IBM argued in favor of Ring-shaped networks while the manufacturing industry, led by General Motors, argued in favor of a bus-shaped network. This led to the creation of the <a class="reference external" href="http://www.ieee802.org/4/">IEEE 802.4 working group</a> to standardize Token Bus networks and the <a class="reference external" href="http://www.ieee802.org/5/">IEEE 802.5 working group</a> to standardize Token Ring networks. Although these techniques are not widely used anymore today, the principles behind a token-based protocol are still important.</p>
<p>The IEEE 802.5 Token Ring technology is defined in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id32"><span>[IEEE802.5]</span></a>. We use Token Ring as an example to explain the principles of the token-based MAC algorithms in ring-shaped networks. Other ring-shaped networks include the defunct FDDI <a class="reference internal" href="../bibliography.html#ross1989" id="id33"><span>[Ross1989]</span></a> or Resilient Pack Ring <a class="reference internal" href="../bibliography.html#dygu2004" id="id34"><span>[DYGU2004]</span></a> . A good survey of the early token ring networks may be found in <a class="reference internal" href="../bibliography.html#bux1989" id="id35"><span>[Bux1989]</span></a> .</p>
<p>A Token Ring network is composed of a set of stations that are attached to a unidirectional ring. The basic principle of the Token Ring MAC algorithm is that two types of frames travel on the ring : tokens and data frames. When the Token Ring starts, one of the stations sends the token. The token is a small frame that represents the authorization to transmit data frames on the ring. To transmit a data frame on the ring, a station must first capture the token by removing it from the ring. As only one station can capture the token at a time, the station that owns the token can safely transmit a data frame on the ring without risking collisions. After having transmitted its frame, the station must remove it from the ring and resend the token so that other stations can transmit their own frames.</p>
<figure class="align-center" id="id93">
<span id="fig-tokenring"/><a class="reference internal image-reference" href="../_images/token-ring.png"><img alt="../_images/token-ring.png" src="../Images/204991c6fea7862f915a3396c1578736.png" style="width: 350.0px; height: 154.7px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/token-ring.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 206 </span><span class="caption-text">A Token Ring network</span><a class="headerlink" href="#id93" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>While the basic principles of the Token Ring are simple, there are several subtle implementation details that add complexity to Token Ring networks. To understand these details let us analyze the operation of a Token Ring interface on a station. A Token Ring interface serves three different purposes. Like other LAN interfaces, it must be able to send and receive frames. In addition, a Token Ring interface is part of the ring, and as such, it must be able to forward the electrical signal that passes on the ring even when its station is powered off.</p>
<p>When powered-on, Token Ring interfaces operate in two different modes : <cite>listen</cite> and <cite>transmit</cite>. When operating in <cite>listen</cite> mode, a Token Ring interface receives an electrical signal from its upstream neighbor on the ring, introduces a delay equal to the transmission time of one bit on the ring and regenerates the signal before sending it to its downstream neighbor on the ring.</p>
<p>The first problem faced by a Token Ring network is that as the token represents the authorization to transmit, it must continuously travel on the ring when no data frame is being transmitted. Let us assume that a token has been produced and sent on the ring by one station. In Token Ring networks, the token is a 24 bits frame whose structure is shown below.</p>
<figure class="align-center" id="id94">
<span id="index-30"/><a class="reference internal image-reference" href="../_images/token-ring.svg"><img alt="../_images/token-ring.svg" src="../Images/d3fd5c0099436c887eea9ad8ae351d82.png" style="width: 453.0px; height: 128.0px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/token-ring.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 207 </span><span class="caption-text">802.5 token format</span><a class="headerlink" href="#id94" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p id="index-31">The token is composed of three fields. First, the <cite>Starting Delimiter</cite> is the marker that indicates the beginning of a frame. The first Token Ring networks used Manchester coding and the <cite>Starting Delimiter</cite> contained both symbols representing <cite>0</cite> and symbols that do not represent bits. The last field is the <cite>Ending Delimiter</cite> which marks the end of the token. The <cite>Access Control</cite> field is present in all frames, and contains several flags. The most important is the <cite>Token</cite> bit that is set in token frames and reset in other frames.</p>
<p id="index-32">Let us consider the five station network depicted in figure <a class="reference internal" href="#fig-tokenring"><span class="std std-ref">A Token Ring network</span></a> above and assume that station <cite>S1</cite> sends a token. If we neglect the propagation delay on the inter-station links, as each station introduces a one bit delay, the first bit of the frame would return to <cite>S1</cite> while it sends the fifth bit of the token. If station <cite>S1</cite> is powered off at that time, only the first five bits of the token will travel on the ring. To avoid this problem, there is a special station called the <cite>Monitor</cite> on each Token Ring. To ensure that the token can travel forever on the ring, this <cite>Monitor</cite> inserts a delay that is equal to at least 24 bit transmission times. If station <cite>S3</cite> was the <cite>Monitor</cite> in figure <a class="reference internal" href="#fig-tokenring"><span class="std std-ref">A Token Ring network</span></a>, <cite>S1</cite> would have been able to transmit the entire token before receiving the first bit of the token from its upstream neighbor.</p>
<p>Now that we have explained how the token can be forwarded on the ring, let us analyze how a station can capture a token to transmit a data frame. For this, we need some information about the format of the data frames. An 802.5 data frame begins with the <cite>Starting Delimiter</cite> followed by the <cite>Access Control</cite> field whose <cite>Token</cite> bit is reset, a <cite>Frame Control</cite> field that enables the definition of several types of frames, destination and source address, a payload, a CRC, the <cite>Ending Delimiter</cite> and a <cite>Frame Status</cite> field. The format of the Token Ring data frames is illustrated below.</p>
<figure class="align-center" id="id95">
<span id="index-33"/><a class="reference internal image-reference" href="../_images/8025.svg"><img alt="../_images/8025.svg" src="../Images/9f175bb0486e691ee2c9018db5a42d22.png" style="width: 604.0px; height: 320.0px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/8025.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 208 </span><span class="caption-text">802.5 data frame format</span><a class="headerlink" href="#id95" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To capture a token, a station must operate in <cite>Listen</cite> mode. In this mode, the station receives bits from its upstream neighbor. If the bits correspond to a data frame, they must be forwarded to the downstream neighbor. If they correspond to a token, the station can capture it and transmit its data frame. Both the data frame and the token are encoded as a bit string beginning with the <cite>Starting Delimiter</cite> followed by the <cite>Access Control</cite> field. When the station receives the first bit of a <cite>Starting Delimiter</cite>, it cannot know whether this is a data frame or a token and must forward the entire delimiter to its downstream neighbor. It is only when it receives the fourth bit of the <cite>Access Control</cite> field (i.e. the <cite>Token</cite> bit) that the station knows whether the frame is a data frame or a token. If the <cite>Token</cite> bit is reset, it indicates a data frame and the remaining bits of the data frame must be forwarded to the downstream station. Otherwise (<cite>Token</cite> bit is set), this is a token and the station can capture it by resetting the bit that is currently in its buffer. Thanks to this modification, the beginning of the token is now the beginning of a data frame and the station can switch to <cite>Transmit</cite> mode and send its data frame starting at the fifth bit of the <cite>Access Control</cite> field. Thus, the one-bit delay introduced by each Token Ring station plays a key role in enabling the stations to efficiently capture the token.</p>
<p>After having transmitted its data frame, the station must remain in <cite>Transmit</cite> mode until it has received the last bit of its own data frame. This ensures that the bits sent by a station do not remain in the network forever. A data frame sent by a station in a Token Ring network passes in front of all stations attached to the network. Each station can detect the data frame and analyze the destination address to possibly capture the frame.</p>
<p id="index-34">The text above describes the basic operation of a Token Ring network when all stations work correctly. Unfortunately, a real Token Ring network must be able to handle various types of anomalies and this increases the complexity of Token Ring stations. We briefly list the problems and outline their solutions below. A detailed description of the operation of Token Ring stations may be found in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id36"><span>[IEEE802.5]</span></a>. The first problem is when all the stations attached to the network start. One of them must bootstrap the network by sending the first token. For this, all stations implement a distributed election mechanism that is used to select the <cite>Monitor</cite>. Any station can become a <cite>Monitor</cite>. The <cite>Monitor</cite> manages the Token Ring network and ensures that it operates correctly. Its first role is to introduce a delay of 24 bit transmission times to ensure that the token can travel smoothly on the ring. Second, the <cite>Monitor</cite> sends the first token on the ring. It must also verify that the token passes regularly. According to the Token Ring standard <a class="reference internal" href="../bibliography.html#ieee802-5" id="id37"><span>[IEEE802.5]</span></a>, a station cannot retain the token to transmit data frames for a duration longer than the <cite>Token Holding Time</cite> (THT) (slightly less than 10 milliseconds). On a network containing <cite>N</cite> stations, the <cite>Monitor</cite> must receive the token at least every <span class="math notranslate nohighlight">\(N \times THT\)</span> seconds. If the <cite>Monitor</cite> does not receive a token during such a period, it cuts the ring for some time and then re-initializes the ring and sends a token.</p>
<p>Several other anomalies may occur in a Token Ring network. For example, a station could capture a token and be powered off before having resent the token. Another station could have captured the token, sent its data frame and be powered off before receiving all of its data frame. In this case, the bit string corresponding to the end of a frame would remain in the ring without being removed by its sender. Several techniques are defined in <a class="reference internal" href="../bibliography.html#ieee802-5" id="id38"><span>[IEEE802.5]</span></a> to allow the <cite>Monitor</cite> to handle all these problems. If unfortunately, the <cite>Monitor</cite> fails, another station will be elected to become the new <cite>Monitor</cite>.</p>
&#13;

<h2>Congestion control<a class="headerlink" href="#congestion-control" title="Link to this heading">#</a></h2>
<p>Most networks contain links having different bandwidth. Some hosts can use low bandwidth wireless networks. Some servers are attached via 10 Gbps interfaces and inter-router links may vary from a few tens of kilobits per second up to hundred Gbps. Despite these huge differences in performance, any host should be able to efficiently exchange segments with a high-end server.</p>
<p id="index-35">To understand this problem better, let us consider the scenario shown in the figure below, where a server (<cite>A</cite>) attached to a <cite>10 Mbps</cite> link needs to reliably transfer segments to another computer (<cite>C</cite>) through a path that contains a <cite>2 Mbps</cite> link.</p>
<blockquote>
<div><div class="figure" id="id96" style="text-align: center"><p><img src="../Images/437492f9ed4cdc7e563973c918bb6989.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-d10fd2b247bfdcf82bfe39a116b52a6e430735e2.png"/></p>
<p><span class="caption-number">Fig. 209 </span><span class="caption-text">Reliable transport with heterogeneous links</span></p>
</div></div></blockquote>
<p>In this network, the segments sent by the server reach router <cite>R1</cite>. <cite>R1</cite> forwards the segments towards router <cite>R2</cite>. Router <cite>R1</cite> can potentially receive segments at <cite>10 Mbps</cite>, but it can only forward them at <cite>2 Mbps</cite> to router <cite>R2</cite> and then to host <cite>C</cite>.  Router <cite>R1</cite> includes buffers that allow it to store the packets that cannot immediately be forwarded to their destination. To understand the operation of a reliable transport protocol in this environment, let us consider a simplified model of this network where host <cite>A</cite> is attached to a <cite>10 Mbps</cite> link to a queue that represents the buffers of router <cite>R1</cite>. This queue is emptied at a rate of <cite>2 Mbps</cite>.</p>
<figure class="align-center" id="id97">
<a class="reference internal image-reference" href="../_images/tcp-self-clocking.png"><img alt="../_images/tcp-self-clocking.png" src="../Images/84e88a885a2921ce9a2b332adc09db01.png" style="width: 350.0px; height: 262.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-self-clocking.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 210 </span><span class="caption-text">Self clocking</span><a class="headerlink" href="#id97" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let us consider that host <cite>A</cite> uses a window of three segments. It thus sends three back-to-back segments at <cite>10 Mbps</cite> and then waits for an acknowledgment. Host <cite>A</cite> stops sending segments when its window is full. These segments reach the buffers of router <cite>R1</cite>. The first segment stored in this buffer is sent by router <cite>R1</cite> at a rate of <cite>2 Mbps</cite> to the destination host. Upon reception of this segment, the destination sends an acknowledgment. This acknowledgment allows host <cite>A</cite> to transmit a new segment. This segment is stored in the buffers of router <cite>R1</cite> while it is transmitting the second segment that was sent by host <cite>A</cite>… Thus, after the transmission of the first window of segments, the reliable transport protocol sends one data segment after the reception of each acknowledgment returned by the destination. In practice, the acknowledgments sent by the destination serve as a kind of <cite>clock</cite> that allows the sending host to adapt its transmission rate to the rate at which segments are received by the destination. This <cite>self-clocking</cite> is the first mechanism that allows a window-based reliable transport protocol to adapt to heterogeneous networks <a class="reference internal" href="../bibliography.html#jacobson1988" id="id39"><span>[Jacobson1988]</span></a>. It depends on the availability of buffers to store the segments that have been sent by the sender but have not yet been transmitted to the destination.</p>
<p>However, transport protocols are not only used in this environment. In the global Internet, a large number of hosts send segments to a large number of receivers. For example, let us consider the network depicted below which is similar to the one discussed in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id40"><span>[Jacobson1988]</span></a> and <span class="target" id="index-36"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc896.html"><strong>RFC 896</strong></a>. In this network, we assume that the buffers of the router are infinite to ensure that no packet is lost.</p>
<blockquote>
<div><div class="figure" id="id98" style="text-align: center"><p><img src="../Images/0f464428bb4f3f9bf9d9d1dbaafd389d.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-af98f2fddb12f129231142a8dfa25b4f15f14611.png"/></p>
<p><span class="caption-number">Fig. 211 </span><span class="caption-text">The congestion collapse problem</span></p>
</div></div></blockquote>
<p id="index-37">If many senders are attached to the left part of the network above, they all send a window full of segments. These segments are stored in the buffers of the router before being transmitted towards their destination. If there are many senders on the left part of the network, the occupancy of the buffers quickly grows. A consequence of the buffer occupancy is that the round-trip-time, measured by the transport protocol, between the sender and the receiver increases. Consider a network where 10,000 bits segments are sent. When the buffer is empty, such a segment requires 1 millisecond to be transmitted on the <cite>10 Mbps</cite> link and 5 milliseconds to be the transmitted on the <cite>2 Mbps</cite> link. Thus, the measured round-trip-time measured is roughly 6 milliseconds if we ignore the propagation delay on the links. If the buffer contains 100 segments, the round-trip-time becomes <span class="math notranslate nohighlight">\(1+100 \times 5+ 5\)</span> milliseconds as new segments are only transmitted on the <cite>2 Mbps</cite> link once all previous segments have been transmitted. Unfortunately, if the reliable transport protocol uses a retransmission timer and performs <cite>go-back-n</cite> to recover from transmission errors it will retransmit a full window of segments. This increases the occupancy of the buffer and the delay through the buffer… Furthermore, the buffer may store and send on the low bandwidth links several retransmissions of the same segment. This problem is called <cite>congestion collapse</cite>. It occurred several times during the late 1980s on the Internet <a class="reference internal" href="../bibliography.html#jacobson1988" id="id41"><span>[Jacobson1988]</span></a>.</p>
<p>The <cite>congestion collapse</cite> is a problem that all heterogeneous networks face. Different mechanisms have been proposed in the scientific literature to avoid or control network congestion. Some of them have been implemented and deployed in real networks. To understand this problem in more detail, let us first consider a simple network with two hosts attached to a high bandwidth link that are sending segments to destination <cite>C</cite> attached to a low bandwidth link as depicted below.</p>
<blockquote>
<div><div class="figure" id="id99" style="text-align: center"><p><img src="../Images/81b044c15b073243b51b376b5d57bf3d.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-0eb3527ae8292a399f94657427a9073632cc65bf.png"/></p>
<p><span class="caption-number">Fig. 212 </span><span class="caption-text">The congestion problem</span></p>
</div></div></blockquote>
<p>To avoid <cite>congestion collapse</cite>, the hosts must regulate their transmission rate <a class="footnote-reference brackets" href="#fcredit" id="id42" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> by using a <cite>congestion control</cite> mechanism. Such a mechanism can be implemented in the transport layer or in the network layer. In TCP/IP networks, it is implemented in the transport layer, but other technologies such as <cite>Asynchronous Transfer Mode (ATM)</cite> or <cite>Frame Relay</cite> include congestion control mechanisms in lower layers.</p>
<p id="index-38">Let us first consider the simple problem of a set of <span class="math notranslate nohighlight">\(i\)</span> hosts that share a single bottleneck link as shown in the example above. In this network, the congestion control scheme must achieve the following objectives <a class="reference internal" href="../bibliography.html#cj1989" id="id43"><span>[CJ1989]</span></a> :</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>The congestion control scheme must <cite>avoid congestion</cite>. In practice, this means that the bottleneck link cannot be overloaded. If <span class="math notranslate nohighlight">\(r_i(t)\)</span> is the transmission rate allocated to host <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(R\)</span> the bandwidth of the bottleneck link, then the congestion control scheme should ensure that, on average, <span class="math notranslate nohighlight">\(\forall{t} \sum{r_i(t)} \le R\)</span>.</p></li>
<li><p>The congestion control scheme must be <cite>efficient</cite>. The bottleneck link is usually both a shared and an expensive resource. Usually, bottleneck links are wide area links that are much more expensive to upgrade than the local area networks. The congestion control scheme should ensure that such links are efficiently used. Mathematically, the control scheme should ensure that <span class="math notranslate nohighlight">\(\forall{t} \sum{r_i(t)} \approx R\)</span>.</p></li>
<li><p>The congestion control scheme should be <cite>fair</cite>. Most congestion schemes aim at achieving <cite>max-min fairness</cite>. An allocation of transmission rates to sources is said to be <cite>max-min fair</cite> if :</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>no link in the network is congested</p></li>
<li><p>the rate allocated to source <span class="math notranslate nohighlight">\(j\)</span> cannot be increased without decreasing the rate allocated to a source <span class="math notranslate nohighlight">\(i\)</span> whose allocation is smaller than the rate allocated to source <span class="math notranslate nohighlight">\(j\)</span> <a class="reference internal" href="../bibliography.html#leboudec2008" id="id44"><span>[Leboudec2008]</span></a> .</p></li>
</ul>
</div></blockquote>
</div></blockquote>
<p>Depending on the network, a <cite>max-min fair allocation</cite> may not always exist. In practice, <cite>max-min fairness</cite> is an ideal objective that cannot necessarily be achieved. When there is a single bottleneck link as in the example above, <cite>max-min fairness</cite> implies that each source should be allocated the same transmission rate.</p>
<p>To visualize the different rate allocations, it is useful to consider the graph shown below. In this graph, we plot on the <cite>x-axis</cite> (resp. <cite>y-axis</cite>) the rate allocated to host <cite>B</cite> (resp. <cite>A</cite>). A point in the graph <span class="math notranslate nohighlight">\((r_B,r_A)\)</span> corresponds to a possible allocation of the transmission rates. Since there is a <cite>2 Mbps</cite> bottleneck link in this network, the graph can be divided into two regions. The lower left part of the graph contains all allocations <span class="math notranslate nohighlight">\((r_B,r_A)\)</span> such that the bottleneck link is not congested (<span class="math notranslate nohighlight">\(r_A+r_B&lt;2\)</span>). The right border of this region is the <cite>efficiency line</cite>, i.e. the set of allocations that completely utilize the bottleneck link (<span class="math notranslate nohighlight">\(r_A+r_B=2\)</span>). Finally, the <cite>fairness line</cite> is the set of fair allocations.</p>
<figure class="align-center" id="id100">
<a class="reference internal image-reference" href="../_images/congestion-rates.png"><img alt="../_images/congestion-rates.png" src="../Images/9ccea874ced3496fad9f847266e5e96a.png" style="width: 251.29999999999998px; height: 168.0px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/congestion-rates.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 213 </span><span class="caption-text">Possible allocated transmission rates</span><a class="headerlink" href="#id100" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As shown in the graph above, a rate allocation may be fair but not efficient (e.g. <span class="math notranslate nohighlight">\(r_A=0.7,r_B=0.7\)</span>), fair and efficient ( e.g. <span class="math notranslate nohighlight">\(r_A=1,r_B=1\)</span>) or efficient but not fair (e.g. <span class="math notranslate nohighlight">\(r_A=1.5,r_B=0.5\)</span>). Ideally, the allocation should be both fair and efficient. Unfortunately, maintaining such an allocation with fluctuations in the number of flows that use the network is a challenging problem. Furthermore, there might be several thousands flows that pass through the same link <a class="footnote-reference brackets" href="#fflowslink" id="id45" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>.</p>
<p>To deal with these fluctuations in demand, which result in fluctuations in the available bandwidth, computer networks use a congestion control scheme. This congestion control scheme should achieve the three objectives listed above. Some congestion control schemes rely on a close cooperation between the end hosts and the routers, while others are mainly implemented on the end hosts with limited support from the routers.</p>
<p>A congestion control scheme can be modeled as an algorithm that adapts the transmission rate (<span class="math notranslate nohighlight">\(r_i(t)\)</span>) of host <span class="math notranslate nohighlight">\(i\)</span> based on the feedback received from the network. Different types of feedback are possible. The simplest scheme is a binary feedback <a class="reference internal" href="../bibliography.html#cj1989" id="id46"><span>[CJ1989]</span></a>  <a class="reference internal" href="../bibliography.html#jacobson1988" id="id47"><span>[Jacobson1988]</span></a> where the hosts simply learn whether the network is congested or not. Some congestion control schemes allow the network to regularly send an allocated transmission rate in Mbps to each host <a class="reference internal" href="../bibliography.html#bf1995" id="id48"><span>[BF1995]</span></a>.</p>
<p id="index-39">Let us focus on the binary feedback scheme which is the most widely used today. Intuitively, the congestion control scheme should decrease the transmission rate of a host when congestion has been detected in the network, in order to avoid congestion collapse. Furthermore, the hosts should increase their transmission rate when the network is not congested. Otherwise, the hosts would not be able to efficiently utilize the network. The rate allocated to each host fluctuates with time, depending on the feedback received from the network. Figure <a class="reference internal" href="#fig-congestion-rates"><span class="std std-numref">Fig. 214</span></a> illustrates the evolution of the transmission rates allocated to two hosts in our simple network. Initially, two hosts have a low allocation, but this is not efficient. The allocations increase until the network becomes congested. At this point, the hosts decrease their transmission rate to avoid congestion collapse. If the congestion control scheme works well, after some time the allocations should become both fair and efficient.</p>
<figure class="align-center" id="fig-congestion-rates">
<a class="reference internal image-reference" href="../_images/congestion-rates-evolution.png"><img alt="../_images/congestion-rates-evolution.png" src="../Images/26c6fbe274fb53fd9009bee48fcdb53e.png" style="width: 221.89999999999998px; height: 143.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/congestion-rates-evolution.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 214 </span><span class="caption-text">Evolution of the transmission rates</span><a class="headerlink" href="#fig-congestion-rates" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Various types of rate adaption algorithms are possible. <a class="reference external" href="https://home.ie.cuhk.edu.hk/~dmchiu/">Dah Ming Chiu</a> and <a class="reference external" href="https://www.cse.wustl.edu/~jain/">Raj Jain</a> have analyzed, in <a class="reference internal" href="../bibliography.html#cj1989" id="id49"><span>[CJ1989]</span></a>, different types of algorithms that can be used by a source to adapt its transmission rate to the feedback received from the network. Intuitively, such a rate adaptation algorithm increases the transmission rate when the network is not congested (ensure that the network is efficiently used) and decrease the transmission rate when the network is congested (to avoid congestion collapse).</p>
<p>The simplest form of feedback that the network can send to a source is a binary feedback (the network is congested or not congested). In this case, a <cite>linear</cite> rate adaptation algorithm can be expressed as :</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(rate(t+1)=\alpha_C + \beta_C rate(t)\)</span> when the network is congested</p></li>
<li><p><span class="math notranslate nohighlight">\(rate(t+1)=\alpha_N + \beta_N rate(t)\)</span> when the network is <em>not</em> congested</p></li>
</ul>
</div></blockquote>
<p>With a linear adaption algorithm, <span class="math notranslate nohighlight">\(\alpha_C,\alpha_N, \beta_C\)</span> and <span class="math notranslate nohighlight">\(\beta_N\)</span> are constants.
The analysis of <a class="reference internal" href="../bibliography.html#cj1989" id="id50"><span>[CJ1989]</span></a> shows that to be fair and efficient, such a binary rate adaption mechanism must rely on <cite>Additive Increase and Multiplicative Decrease</cite>. When the network is not congested, the hosts should slowly increase their transmission rate (<span class="math notranslate nohighlight">\(\beta_N=1~and~\alpha_N&gt;0\)</span>). When the network is congested, the hosts must multiplicatively decrease their transmission rate (<span class="math notranslate nohighlight">\(\beta_C &lt; 1~and~\alpha_C = 0\)</span>). Such an AIMD rate adaptation algorithm can be implemented by the pseudo-code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># Additive Increase Multiplicative Decrease</span>
<span class="k">if</span> <span class="n">congestion</span><span class="p">:</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">rate</span> <span class="o">*</span> <span class="n">betaC</span>    <span class="c1"># multiplicative decrease, betaC&lt;1</span>
<span class="k">else</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">rate</span> <span class="o">+</span> <span class="n">alphaN</span>    <span class="c1"># additive increase, alphaN &gt; 0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Which binary feedback ?</p>
<p>Two types of binary feedback are possible in computer networks. A first solution is to rely on implicit feedback. This is the solution chosen for TCP. TCP’s congestion control scheme <a class="reference internal" href="../bibliography.html#jacobson1988" id="id51"><span>[Jacobson1988]</span></a> does not require any cooperation from the router. It only assumes that they use buffers and that they discard packets when there is congestion. TCP uses the segment losses as an indication of congestion. When there are no losses, the network is assumed to be not congested. This implies that congestion is the main cause of packet losses. This is true in wired networks, but unfortunately not always true in wireless networks.
Another solution is to rely on explicit feedback. This is the solution proposed in the DECBit congestion control scheme <a class="reference internal" href="../bibliography.html#rj1995" id="id52"><span>[RJ1995]</span></a> and used in Frame Relay and ATM networks. This explicit feedback can be implemented in two ways. A first solution would be to define a special message that could be sent by routers to hosts when they are congested. Unfortunately, generating such messages may increase the amount of congestion in the network. Such a congestion indication packet is thus discouraged <span class="target" id="index-40"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc1812.html"><strong>RFC 1812</strong></a>. A better approach is to allow the intermediate routers to indicate, in the packets that they forward, their current congestion status. Binary feedback can be encoded by using one bit in the packet header. With such a scheme, congested routers set a special bit in the packets that they forward while non-congested routers leave this bit unmodified. The destination host returns the congestion status of the network in the acknowledgments that it sends. Details about such a solution in IP networks may be found in <span class="target" id="index-41"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a>. Unfortunately, as of this writing, this solution is still not deployed despite its potential benefits.</p>
</div>
<section id="congestion-control-with-a-window-based-transport-protocol">
<h3>Congestion control with a window-based transport protocol<a class="headerlink" href="#congestion-control-with-a-window-based-transport-protocol" title="Link to this heading">#</a></h3>
<p>AIMD controls congestion by adjusting the transmission rate of the sources in reaction to the current congestion level. If the network is not congested, the transmission rate increases. If congestion is detected, the transmission rate is multiplicatively decreased. In practice, directly adjusting the transmission rate can be difficult since it requires the utilization of fine grained timers. In reliable transport protocols, an alternative is to dynamically adjust the sending window. This is the solution chosen for protocols like TCP and SCTP that will be described in more details later. To understand how window-based protocols can adjust their transmission rate, let us consider the very simple scenario of a reliable transport protocol that uses <cite>go-back-n</cite>. Consider the very simple scenario shown in figure <a class="reference internal" href="#fig-bottleneck"><span class="std std-numref">Fig. 215</span></a>.</p>
<blockquote>
<div><div class="figure" id="id101" style="text-align: center">
<span id="fig-bottleneck"/><p><img src="../Images/fba8d6b7380280ee70f3550fb599fb9f.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-c571e39a73d7fa60a9759c2a5d1d690b0ec7f12a.png"/></p>
<p><span class="caption-number">Fig. 215 </span><span class="caption-text">A simple network with hosts sharing a bottleneck link</span></p>
</div></div></blockquote>
<p>The links between the hosts and the routers have a bandwidth of 1 Mbps while the link between the two routers has a bandwidth of 500 Kbps. There is no significant propagation delay in this network. For simplicity, assume that hosts <cite>A</cite> and <cite>B</cite> send 1000 bits packets. The transmission of such a packet on a <cite>host-router</cite> (resp. <cite>router-router</cite> ) link requires 1 msec (resp. 2 msec). If there is no traffic in the network, the round-trip-time measured by host <cite>A</cite> to reach <cite>D</cite> is slightly larger than 4 msec. Let us observe the flow of packets with different window sizes to understand the relationship between sending window and transmission rate.</p>
<p>Consider first a window of one segment. This segment takes 4 msec to reach host <cite>D</cite>. The destination replies with an acknowledgment and the next segment can be transmitted. With such a sending window, the transmission rate is roughly 250 segments per second or 250 Kbps. This is illustrated in figure <a class="reference internal" href="#fig-gbn-win-1"><span class="std std-numref">Fig. 216</span></a> where each square of the grid corresponds to one millisecond.</p>
<blockquote>
<div><div class="figure" id="id102" style="text-align: center">
<span id="fig-gbn-win-1"/><p><img src="../Images/fb17e125cf960286b8258b6251471572.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-5d002e7b389ca36e47cf6c9f674938a0d08593cb.png"/></p>
<p><span class="caption-number">Fig. 216 </span><span class="caption-text">Go-back-n transfer from A to D, window of one segment</span></p>
</div></div></blockquote>
<p>Consider now a window of two segments. Host <cite>A</cite> can send two segments within 2 msec on its 1 Mbps link. If the first segment is sent at time <span class="math notranslate nohighlight">\(t_{0}\)</span>, it reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. Host <cite>D</cite> replies with an acknowledgment that opens the sending window on host <cite>A</cite> and enables it to transmit a new segment. In the meantime, the second segment was buffered by router <cite>R1</cite>. It reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+6\)</span> and an acknowledgment is returned. With a window of two segments, host <cite>A</cite> transmits at roughly 500 Kbps, i.e. the transmission rate of the bottleneck link.</p>
<blockquote>
<div><div class="figure" id="id103" style="text-align: center"><p><img src="../Images/972780cbffde5435003f04c4edfabc56.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-d13039c2482f96c3496fdd84e458374e31963a74.png"/></p>
<p><span class="caption-number">Fig. 217 </span><span class="caption-text">Go-back-n transfer from A to D, window of two segments</span></p>
</div></div></blockquote>
<p>Our last example is a window of four segments. These segments are sent at <span class="math notranslate nohighlight">\(t_{0}\)</span>, <span class="math notranslate nohighlight">\(t_{0}+1\)</span>, <span class="math notranslate nohighlight">\(t_{0}+2\)</span> and <span class="math notranslate nohighlight">\(t_{0}+3\)</span>. The first segment reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. Host <cite>D</cite> replies to this segment by sending an acknowledgment that enables host <cite>A</cite> to transmit its fifth segment. This segment reaches router <cite>R1</cite> at <span class="math notranslate nohighlight">\(t_{0}+5\)</span>. At that time, router <cite>R1</cite> is transmitting the third segment to router <cite>R2</cite> and the fourth segment is still in its buffers. At time <span class="math notranslate nohighlight">\(t_{0}+6\)</span>, host <cite>D</cite> receives the second segment and returns the corresponding acknowledgment. This acknowledgment enables host <cite>A</cite> to send its sixth segment. This segment reaches router <cite>R1</cite> at roughly <span class="math notranslate nohighlight">\(t_{0}+7\)</span>. At that time, the router starts to transmit the fourth segment to router <cite>R2</cite>. Since link <cite>R1-R2</cite> can only sustain 500 Kbps, packets will accumulate in the buffers of <cite>R1</cite>. On average, there will be two packets waiting in the buffers of <cite>R1</cite>. The presence of these two packets will induce an increase of the round-trip-time as measured by the transport protocol. While the first segment was acknowledged within 4 msec, the fifth segment (<cite>data(4)</cite>) that was transmitted at time <span class="math notranslate nohighlight">\(t_{0}+4\)</span> is only acknowledged at time <span class="math notranslate nohighlight">\(t_{0}+11\)</span>. On average, the sender transmits at 500 Kbps, but the utilization of a large window induces a longer delay through the network.</p>
<blockquote>
<div><div class="figure" id="id104" style="text-align: center"><p><img src="../Images/6fac88cb38e8b98800e49c18ffa34a0c.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-b9493099e8639557dc24065f10c48391e80c82fd.png"/></p>
<p><span class="caption-number">Fig. 218 </span><span class="caption-text">Go-back-n transfer from A to D, window of four segments</span></p>
</div></div></blockquote>
<p id="index-42">From the above example, we can adjust the transmission rate by adjusting the sending window of a reliable transport protocol. A reliable transport protocol cannot send data faster than <span class="math notranslate nohighlight">\(\frac{window}{rtt}\)</span> segments per second where <span class="math notranslate nohighlight">\(window\)</span> is the current sending window. To control the transmission rate, we introduce a <cite>congestion window</cite>. This congestion window limits the sending window. At any time, the sending window is restricted to <span class="math notranslate nohighlight">\(\min(swin,cwin)\)</span>, where <cite>swin</cite> is the sending window and <cite>cwin</cite> the current <cite>congestion window</cite>. Of course, the window is further constrained by the receive window advertised by the remote peer. With the utilization of a congestion window, a simple reliable transport protocol that uses fixed size segments could implement <cite>AIMD</cite> as follows.</p>
<p>For the <cite>Additive Increase</cite> part our simple protocol would simply increase its <cite>congestion window</cite> by one segment every round-trip-time. The
<cite>Multiplicative Decrease</cite> part of <cite>AIMD</cite> could be implemented by halving the congestion window when congestion is detected. For simplicity, we assume that congestion is detected thanks to a binary feedback and that no segments are lost. We will discuss in more details how losses affect a real transport protocol like TCP in later sections.</p>
<p>A congestion control scheme for our simple transport protocol could be implemented as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># Initialisation</span>
<span class="n">cwin</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># congestion window measured in segments</span>

<span class="c1"># Ack arrival</span>
<span class="k">if</span> <span class="n">ack_received</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">newack</span><span class="p">:</span>  <span class="c1"># new ack, no congestion</span>
        <span class="c1"># increase cwin by one every rtt</span>
        <span class="n">cwin</span> <span class="o">=</span> <span class="n">cwin</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">cwin</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># no increase</span>

<span class="k">if</span> <span class="n">congestion_detected</span><span class="p">:</span>
    <span class="n">cwin</span> <span class="o">=</span> <span class="n">cwin</span> <span class="o">/</span> <span class="mi">2</span> <span class="c1"># only once per rtt</span>
</pre></div>
</div>
<p>In the above pseudocode, <cite>cwin</cite> contains the congestion window stored as a real number of segments. This congestion window is updated upon the arrival of each acknowledgment and when congestion is detected. For simplicity, we assume that <cite>cwin</cite> is stored as a floating point number but only full segments can be transmitted.</p>
<p>As an illustration, let us consider the network scenario above and assume that the router implements the DECBit binary feedback scheme <a class="reference internal" href="../bibliography.html#rj1995" id="id53"><span>[RJ1995]</span></a>. This scheme uses a form of Forward Explicit Congestion Notification and a router marks the congestion bit in arriving packets when its buffer contains one or more packets. In figure <a class="reference internal" href="#fig-gbn-decbit"><span class="std std-numref">Fig. 219</span></a>, we use a <cite>*</cite> to indicate a marked packet.</p>
<blockquote>
<div><div class="figure" id="id105" style="text-align: center">
<span id="fig-gbn-decbit"/><p><img src="../Images/2b9761249f9ab8572a0fc4b3416b6752.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-c2a62409e962fdfb2fe7754a330fce96bb7f3bd0.png"/></p>
<p><span class="caption-number">Fig. 219 </span><span class="caption-text">Go-back-n transfer from A to D, with AIMD congestion control and DecBit binary feedback scheme</span></p>
</div></div></blockquote>
<p>When the connection starts, its congestion window is set to one segment. Segment <cite>S0</cite> is sent an acknowledgment at roughly <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. The congestion window is increased by one segment and <cite>S1</cite> and <cite>S2</cite> are transmitted at time <span class="math notranslate nohighlight">\(t_{0}+4\)</span> and <span class="math notranslate nohighlight">\(t_{0}+5\)</span>. The corresponding acknowledgments are received at times <span class="math notranslate nohighlight">\(t_{0}+8\)</span> and <span class="math notranslate nohighlight">\(t_{0}+10\)</span>. Upon reception of this last acknowledgment, the congestion window reaches <cite>3</cite> and segments can be sent (<cite>S4</cite> and <cite>S5</cite>). When segment <cite>S6</cite> reaches router <cite>R1</cite>, its buffers already contain <cite>S5</cite>. The packet containing <cite>S6</cite> is thus marked to inform the sender of the congestion. Note that the sender will only notice the congestion once it receives the corresponding acknowledgment at <span class="math notranslate nohighlight">\(t_{0}+18\)</span>. In the meantime, the congestion window continues to increase. At <span class="math notranslate nohighlight">\(t_{0}+16\)</span>, upon reception of the acknowledgment for <cite>S5</cite>, it reaches <cite>4</cite>. When congestion is detected, the congestion window is decreased down to <cite>2</cite>. This explains the idle time between the reception of the acknowledgment for <cite>S*6</cite> and the transmission of <cite>S10</cite>.</p>
<p>In practice, a router is connected to multiple input links. Figure <a class="reference internal" href="#fig-2hosts-bottleneck"><span class="std std-numref">Fig. 220</span></a> shows an example with two hosts.</p>
<blockquote>
<div><div class="figure" id="id106" style="text-align: center">
<span id="fig-2hosts-bottleneck"/><p><img src="../Images/54044b2beb542c9ff304fe654e7cbe2e.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-699cdd177204f453f241862f3b959ea743b24e53.png"/></p>
<p><span class="caption-number">Fig. 220 </span><span class="caption-text">A simple network with hosts sharing a bottleneck</span></p>
</div><div class="figure" id="id107" style="text-align: center"><p><img src="../Images/14805061572e2ea7a08e10338a022ff8.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-966439ccf02ecbf0381106d45a0dcbcfb8a1412d.png"/></p>
<p><span class="caption-number">Fig. 221 </span><span class="caption-text">Sharing the bottleneck link between different inputs</span></p>
</div></div></blockquote>
<p>In general, the links have a non-zero delay. This is illustrated in the figure below where a delay has been added on the link between <cite>R</cite> and <cite>C</cite>.</p>
<blockquote>
<div><div class="figure" id="id108" style="text-align: center"><p><img src="../Images/515eaae674689b5424d8fbcf3dfe9c0e.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-db6942ecfbc5147fe81ef57fed06977ad34ff651.png"/></p>
<p><span class="caption-number">Fig. 222 </span><span class="caption-text">Sharing the bottleneck link between different inputs</span></p>
</div></div></blockquote>
</section>
<section id="tcpcongestion">
<span id="id54"/><h3>Congestion control<a class="headerlink" href="#tcpcongestion" title="Link to this heading">#</a></h3>
<p>In an internetwork, i.e. a networking composed of different types of networks (such as the Internet), congestion control could be implemented either in the network layer or the transport layer. The congestion problem was clearly identified in the later 1980s and the researchers who developed techniques to solve the problem opted for a solution in the transport layer. Adding congestion control to the transport layer makes sense since this layer provides a reliable data transfer and avoiding congestion is a factor in this reliable delivery. The transport layer already deals with heterogeneous networks thanks to its <cite>self-clocking</cite> property that we have already described. In this section, we explain how congestion control has been added to TCP and how this mechanism could be improved in the future.</p>
<p>The TCP congestion control scheme was initially proposed by <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id55"><span>[Jacobson1988]</span></a>. The current specification may be found in <span class="target" id="index-43"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc5681.html"><strong>RFC 5681</strong></a>. TCP relies on <cite>Additive Increase and Multiplicative Decrease (AIMD)</cite>. To implement <a class="reference internal" href="../glossary.html#term-AIMD"><span class="xref std std-term">AIMD</span></a>, a TCP host must be able to control its transmission rate. A first approach would be to use timers and adjust their expiration times in function of the rate imposed by <a class="reference internal" href="../glossary.html#term-AIMD"><span class="xref std std-term">AIMD</span></a>. Unfortunately, maintaining such timers for a large number of TCP connections can be difficult. Instead, <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> noted that the rate of TCP congestion can be artificially controlled by constraining its sending window. A TCP connection cannot send data faster than <span class="math notranslate nohighlight">\(\frac{window}{rtt}\)</span> where <span class="math notranslate nohighlight">\(window\)</span> is the minimum between the host’s sending window and the window advertised by the receiver.</p>
<p>TCP’s congestion control scheme is based on a <cite>congestion window</cite>. The current value of the congestion window (<cite>cwnd</cite>) is stored in the TCB of each TCP connection and the window that can be used by the sender is constrained by <span class="math notranslate nohighlight">\(\min(cwnd,rwin,swin)\)</span> where <span class="math notranslate nohighlight">\(swin\)</span> is the current sending window and <span class="math notranslate nohighlight">\(rwin\)</span> the last received receive window. The <cite>Additive Increase</cite> part of the TCP congestion control increments the congestion window by <a class="reference internal" href="../glossary.html#term-MSS"><span class="xref std std-term">MSS</span></a> bytes every round-trip-time. In the TCP literature, this phase is often called the <cite>congestion avoidance</cite> phase. The <cite>Multiplicative Decrease</cite> part of the TCP congestion control divides the current value of the congestion window once congestion has been detected.</p>
<p>When a TCP connection begins, the sending host does not know whether the part of the network that it uses to reach the destination is congested or not. To avoid causing too much congestion, it must start with a small congestion window. <a class="reference internal" href="../bibliography.html#jacobson1988" id="id56"><span>[Jacobson1988]</span></a> recommends an initial window of MSS bytes. As the additive increase part of the TCP congestion control scheme increments the congestion window by MSS bytes every round-trip-time, the TCP connection may have to wait many round-trip-times before being able to efficiently use the available bandwidth. This is especially important in environments where the <span class="math notranslate nohighlight">\(bandwidth \times rtt\)</span> product is high. To avoid waiting too many round-trip-times before reaching a congestion window that is large enough to efficiently utilize the network, the TCP congestion control scheme includes the <cite>slow-start</cite> algorithm. The objective of the TCP <cite>slow-start</cite> phase is to quickly reach an acceptable value for the <cite>cwnd</cite>. During <cite>slow-start</cite>, the congestion window is doubled every round-trip-time. The <cite>slow-start</cite> algorithm uses an additional variable in the TCB : <cite>ssthresh</cite> (<cite>slow-start threshold</cite>). The <cite>ssthresh</cite> is an estimation of the last value of the <cite>cwnd</cite> that did not cause congestion. It is initialized at the sending window and is updated after each congestion event.</p>
<p>A key question that must be answered by any congestion control scheme is how congestion is detected. The first implementations of the TCP congestion control scheme opted for a simple and pragmatic approach : packet losses indicate congestion. If the network is congested, router buffers are full and packets are discarded. In wired networks, packet losses are mainly caused by congestion. In wireless networks, packets can be lost due to transmission errors and for other reasons that are independent of congestion. TCP already detects segment losses to ensure a reliable delivery. The TCP congestion control scheme distinguishes between two types of congestion :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>mild congestion</cite>. TCP considers that the network is lightly congested if it receives three duplicate acknowledgments and performs a fast retransmit. If the fast retransmit is successful, this implies that only one segment has been lost. In this case, TCP performs multiplicative decrease and the congestion window is divided by <cite>2</cite>. The slow-start threshold is set to the new value of the congestion window.</p></li>
<li><p><cite>severe congestion</cite>. TCP considers that the network is severely congested when its retransmission timer expires. In this case, TCP retransmits the first segment, sets the slow-start threshold to 50% of the congestion window. The congestion window is reset to its initial value and TCP performs a slow-start.</p></li>
</ul>
</div></blockquote>
<p>The figure below illustrates the evolution of the congestion window when there is severe congestion. At the beginning of the connection, the sender performs <cite>slow-start</cite> until the first segments are lost and the retransmission timer expires. At this time, the <cite>ssthresh</cite> is set to half of the current congestion window and the congestion window is reset at one segment. The lost segments are retransmitted as the sender again performs slow-start until the congestion window reaches the <cite>sshtresh</cite>. It then switches to congestion avoidance and the congestion window increases linearly until segments are lost and the retransmission timer expires.</p>
<figure class="align-center" id="id109">
<a class="reference internal image-reference" href="../_images/tcp-congestion-severe.png"><img alt="../_images/tcp-congestion-severe.png" src="../Images/8bb9ad90fd07bc82d9cba2b40a1481a6.png" style="width: 461.99999999999994px; height: 186.89999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-severe.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 223 </span><span class="caption-text">Evaluation of the TCP congestion window with severe congestion</span><a class="headerlink" href="#id109" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The figure below illustrates the evolution of the congestion window when the network is lightly congested and all lost segments can be retransmitted using fast retransmit. The sender begins with a slow-start. A segment is lost but successfully retransmitted by a fast retransmit. The congestion window is divided by 2 and the sender immediately enters congestion avoidance as this was a mild congestion.</p>
<figure class="align-center" id="id110">
<a class="reference internal image-reference" href="../_images/tcp-congestion-mild.png"><img alt="../_images/tcp-congestion-mild.png" src="../Images/118024671401b1ade2c8daa21527e64f.png" style="width: 454.99999999999994px; height: 178.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-mild.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 224 </span><span class="caption-text">Evaluation of the TCP congestion window when the network is lightly congested</span><a class="headerlink" href="#id110" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Most TCP implementations update the congestion window when they receive an acknowledgment. If we assume that the receiver acknowledges each received segment and the sender only sends MSS sized segments, the TCP congestion control scheme can be implemented using the simplified pseudo-code <a class="footnote-reference brackets" href="#fwrap" id="id57" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> below. This pseudocode includes the optimization proposed in <span class="target" id="index-44"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3042.html"><strong>RFC 3042</strong></a> that allows a sender to send new unsent data upon reception of the first or second duplicate acknowledgment. The reception of each of these acknowledgments indicates that one segment has left the network and thus additional data can be sent without causing more congestion. Note that the congestion window is <em>not</em> increased upon reception of these first duplicate acknowledgments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># Initialization</span>
<span class="n">cwnd</span> <span class="o">=</span> <span class="n">MSS</span>  <span class="c1"># congestion window in bytes</span>
<span class="n">ssthresh</span><span class="o">=</span> <span class="n">swin</span> <span class="c1"># in bytes</span>

<span class="c1"># Ack arrival</span>
<span class="k">if</span> <span class="n">tcp</span><span class="o">.</span><span class="n">ack</span> <span class="o">&gt;</span> <span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">:</span>  <span class="c1"># new ack, no congestion</span>
    <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not currently recovering from loss</span>
        <span class="k">if</span> <span class="n">cwnd</span> <span class="o">&lt;</span> <span class="n">ssthresh</span><span class="p">:</span>
            <span class="c1"># slow-start : quickly increase cwnd</span>
            <span class="c1"># double cwnd every rtt</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># congestion avoidance : slowly increase cwnd</span>
            <span class="c1"># increase cwnd by one mss every rtt</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span> <span class="o">*</span> <span class="p">(</span><span class="n">MSS</span> <span class="o">/</span> <span class="n">cwnd</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># recovering from loss</span>
        <span class="n">cwnd</span> <span class="o">=</span> <span class="n">ssthresh</span>  <span class="c1"># deflate cwnd RFC5681</span>
        <span class="n">dupacks</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># duplicate or old ack</span>
    <span class="k">if</span> <span class="n">tcp</span><span class="o">.</span><span class="n">ack</span> <span class="o">==</span> <span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">:</span>  <span class="c1"># duplicate acknowledgment</span>
        <span class="n">dupacks</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">send_next_unacked_segment</span>  <span class="c1"># RFC3042</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">retransmitsegment</span><span class="p">(</span><span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">)</span>
            <span class="n">ssthresh</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">cwnd</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">MSS</span><span class="p">)</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">ssthresh</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># RFC5681</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span>  <span class="c1"># inflate cwnd</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># ack for old segment, ignored</span>
        <span class="k">pass</span>

<span class="n">Expiration</span> <span class="n">of</span> <span class="n">the</span> <span class="n">retransmission</span> <span class="n">timer</span><span class="p">:</span>
    <span class="n">send</span><span class="p">(</span><span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">)</span>  <span class="c1"># retransmit first lost segment</span>
    <span class="n">sshtresh</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">cwnd</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">MSS</span><span class="p">)</span>
    <span class="n">cwnd</span> <span class="o">=</span> <span class="n">MSS</span>
</pre></div>
</div>
<p>Furthermore when a TCP connection has been idle for more than its current retransmission timer, it should reset its congestion window to the congestion window size that it uses when the connection begins, as it no longer knows the current congestion state of the network.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Initial congestion window</p>
<p>The original TCP congestion control mechanism proposed in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id58"><span>[Jacobson1988]</span></a> recommended that each TCP connection should begin by setting <span class="math notranslate nohighlight">\(cwnd=MSS\)</span>. However, in today’s higher bandwidth networks, using such a small initial congestion window severely affects the performance for short TCP connections, such as those used by web servers. In 2002, <span class="target" id="index-45"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3390.html"><strong>RFC 3390</strong></a> allowed an initial congestion window of about 4 KBytes, which corresponds to 3 segments in many environments. Recently, researchers from Google proposed to further increase the initial window up to 15 KBytes <a class="reference internal" href="../bibliography.html#drc-2010" id="id59"><span>[DRC+2010]</span></a>. The measurements that they collected show that this increase would not significantly increase congestion but would significantly reduce the latency of short HTTP responses. Unsurprisingly, the chosen initial window corresponds to the average size of an HTTP response from a search engine. This proposed modification has been adopted in <span class="target" id="index-46"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc6928.html"><strong>RFC 6928</strong></a> and TCP implementations support it.</p>
</div>
<section id="controlling-congestion-without-losing-data">
<h4>Controlling congestion without losing data<a class="headerlink" href="#controlling-congestion-without-losing-data" title="Link to this heading">#</a></h4>
<p>In today’s Internet, congestion is controlled by regularly sending packets at a higher rate than the network capacity. These packets fill the buffers of the routers and are eventually discarded. But shortly after, TCP senders retransmit packets containing exactly the same data. This is potentially a waste of resources since these successive retransmissions consume resources upstream of the router that discards the packets. Packet losses are not the only signal to detect congestion inside the network. An alternative is to allow routers to explicitly indicate their current level of congestion when forwarding packets. This approach was proposed in the late 1980s <a class="reference internal" href="../bibliography.html#rj1995" id="id60"><span>[RJ1995]</span></a> and used in some networks. Unfortunately, it took almost a decade before the Internet community agreed to consider this approach. In the mean time, a large number of TCP implementations and routers were deployed on the Internet.</p>
<p>As explained earlier, Explicit Congestion Notification <span class="target" id="index-47"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> improves the detection of congestion by allowing routers to explicitly mark packets when they are lightly congested. In theory, a single bit in the packet header <a class="reference internal" href="../bibliography.html#rj1995" id="id61"><span>[RJ1995]</span></a> is sufficient to support this congestion control scheme. When a host receives a marked packet, it returns the congestion information to the source that adapts its transmission rate accordingly. Although the idea is relatively simple, deploying it on the entire Internet has proven to be challenging <a class="reference internal" href="../bibliography.html#knt2013" id="id62"><span>[KNT2013]</span></a>. It is interesting to analyze the different factors that have hindered the deployment of this technique.</p>
<p>The first difficulty in adding Explicit Congestion Notification (ECN) in TCP/IP network was to modify the format of the network packet and transport segment headers to carry the required information. In the network layer, one bit was required to allow the routers to mark the packets they forward during congestion periods. In the IP network layer, this bit is called the <cite>Congestion Experienced</cite> (<cite>CE</cite>) bit and is part of the packet header. However, using a single bit to mark packets is not sufficient. Consider a simple scenario with two sources, one congested router and one destination. Assume that the first sender and the destination support ECN, but not the second sender. If the router is congested it will mark packets from both senders. The first sender will react to the packet markings by reducing its transmission rate. However since the second sender does not support ECN, it will not react to the markings. Furthermore, this sender could continue to increase its transmission rate, which would lead to more packets being marked and the first source would decrease again its transmission rate, … In the end, the sources that implement ECN are penalized compared to the sources that do not implement it. This unfairness issue is a major hurdle to widely deploy ECN on the public Internet <a class="footnote-reference brackets" href="#fprivate" id="id63" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>. The solution proposed in <span class="target" id="index-48"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> to deal with this problem is to use a second bit in the network packet header. This bit, called the <cite>ECN-capable transport</cite> (ECT) bit, indicates whether the packet contains a segment produced by a transport protocol that supports ECN or not. Transport protocols that support ECN set the ECT bit in all packets. When a router is congested, it first verifies whether the ECT bit is set. In this case, the CE bit of the packet is set to indicate congestion. Otherwise, the packet is discarded. This eases the deployment of ECN <a class="footnote-reference brackets" href="#fecnnonce" id="id64" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>.</p>
<p>The second difficulty is how to allow the receiver to inform the sender of the reception of network packets marked with the <cite>CE</cite> bit. In reliable transport protocols like TCP and SCTP, the acknowledgments can be used to provide this feedback. For TCP, two options were possible : change some bits in the TCP segment header or define a new TCP option to carry this information. The designers of ECN opted for reusing spare bits in the TCP header. More precisely, two TCP flags have been added in the TCP header to support ECN. The <cite>ECN-Echo</cite> (ECE) is set in the acknowledgments when the <cite>CE</cite> was set in packets received on the forward path.</p>
<figure class="align-default" id="id111">
<a class="reference internal image-reference" href="../_images/tcp-enc.svg"><img alt="../_images/tcp-enc.svg" src="../Images/0b92b95f1c0dadde63905a28574e87cb.png" style="width: 664.8px; height: 115.19999999999999px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-enc.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 225 </span><span class="caption-text">The TCP flags</span><a class="headerlink" href="#id111" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The third difficulty is to allow an ECN-capable sender to detect whether the remote host also supports ECN. This is a classical negotiation of extensions to a transport protocol. In TCP, this could have been solved by defining a new TCP option used during the three-way handshake. To avoid wasting space in the TCP options, the designers of ECN opted in <span class="target" id="index-49"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> for using the <cite>ECN-Echo</cite> and <cite>CWR</cite> bits in the TCP header to perform this negotiation. In the end, the result is the same with fewer bits exchanged.</p>
<p>Thanks to the <cite>ECT</cite>, <cite>CE</cite> and <cite>ECE</cite>, routers can mark packets during congestion and receivers can return the congestion information back to the TCP senders. However, these three bits are not sufficient to allow a server to reliably send the <cite>ECE</cite> bit to a TCP sender. TCP acknowledgments are not sent reliably. A TCP acknowledgment always contains the next expected sequence number. Since TCP acknowledgments are cumulative, the loss of one acknowledgment is recovered by the correct reception of a subsequent acknowledgment.</p>
<p>If TCP acknowledgments are overloaded to carry the <cite>ECE</cite> bit, the situation is different. Consider the example shown in the figure below. A client sends packets to a server through a router. In the example below, the first packet is marked. The server returns an acknowledgment with the <cite>ECE</cite> bit set. Unfortunately, this acknowledgment is lost and never reaches the client. Shortly after, the server sends a data segment that also carries a cumulative acknowledgment. This acknowledgment confirms the reception of the data to the client, but it did not receive the congestion information through the <cite>ECE</cite> bit.</p>
<blockquote>
<div><p class="mscgen">
<img src="../Images/d2ed1589241adeb9e699082587d1916d.png" alt="msc {&#10;client [label=&quot;client&quot;, linecolour=black],&#10;router [label=&quot;router&quot;, linecolour=black],&#10;server [label=&quot;server&quot;, linecolour=black];&#10;&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;server=&gt;router [ label = &quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;router -x client [label=&quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;|||;&#10;server=&gt;router [ label = &quot;data[seq=x,ack=2,ECE=0,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;client [ label = &quot;data[seq=x,ack=2,ECE=0,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client-&gt;server [linecolour=white];&#10;}" usemap="#700c52eed1dd99ea5abd5216ffd2e044e6fee931" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/mscgen-700c52eed1dd99ea5abd5216ffd2e044e6fee931.png"/>
<map id="700c52eed1dd99ea5abd5216ffd2e044e6fee931" name="700c52eed1dd99ea5abd5216ffd2e044e6fee931"/></p>
</div></blockquote>
<p>To solve this problem, <span class="target" id="index-50"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> uses an additional bit in the TCP header : the <cite>Congestion Window Reduced</cite> (CWR) bit.</p>
<blockquote>
<div><p class="mscgen">
<img src="../Images/36533b7c4d47ba50cac29ebf6ebcc1f1.png" alt="msc {&#10;client [label=&quot;client&quot;, linecolour=black],&#10;router [label=&quot;router&quot;, linecolour=black],&#10;server [label=&quot;server&quot;, linecolour=black];&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;server=&gt;router [ label = &quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;router -x client [label=&quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;|||;&#10;server=&gt;router [ label = &quot;data[seq=x,ack=2,ECE=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;client [ label = &quot;data[seq=x,ack=2,ECE=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0,CWR=1]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1,CWR=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client-&gt;server [linecolour=white];&#10;}" usemap="#d60c580a7379dbdd26f90fd2eead54f832ee6ad6" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/mscgen-d60c580a7379dbdd26f90fd2eead54f832ee6ad6.png"/>
<map id="d60c580a7379dbdd26f90fd2eead54f832ee6ad6" name="d60c580a7379dbdd26f90fd2eead54f832ee6ad6"/></p>
</div></blockquote>
<p>The <cite>CWR</cite> bit of the TCP header provides some form of acknowledgment for the <cite>ECE</cite> bit. When a TCP receiver detects a packet marked with the <cite>CE</cite> bit, it sets the <cite>ECE</cite> bit in all segments that it returns to the sender. Upon reception of an acknowledgment with the <cite>ECE</cite> bit set, the sender reduces its congestion window to reflect a mild congestion and sets the <cite>CWR</cite> bit. This bit remains set as long as the segments received contained the <cite>ECE</cite> bit set. A sender should only react once per round-trip-time to marked packets.</p>
<p>The last point that needs to be discussed about Explicit Congestion Notification is the algorithm that is used by routers to detect congestion. On a router, congestion manifests itself by the number of packets that are stored inside the router buffers. As explained earlier, we need to distinguish between two types of routers :</p>
<blockquote>
<div><ul class="simple">
<li><p>routers that have a single FIFO queue</p></li>
<li><p>routers that have several queues served by a round-robin scheduler</p></li>
</ul>
</div></blockquote>
<p>Routers that use a single queue measure their buffer occupancy as the number of bytes of packets stored in the queue <a class="footnote-reference brackets" href="#fslot" id="id65" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. A first method to detect congestion is to measure the instantaneous buffer occupancy and consider the router to be congested as soon as this occupancy is above a threshold. Typical values of the threshold could be 40% of the total buffer. Measuring the instantaneous buffer occupancy is simple since it only requires one counter. However, this value is fragile from a control viewpoint since it changes frequently. A better solution is to measure the <em>average</em> buffer occupancy and consider the router to be congested when this average occupancy is too high. Random Early Detection (RED) <a class="reference internal" href="../bibliography.html#fj1993" id="id66"><span>[FJ1993]</span></a> is an algorithm that was designed to support Explicit Congestion Notification. In addition to measuring the average buffer occupancy, it also uses probabilistic marking. When the router is congested, the arriving packets are marked with a probability that increases with the average buffer occupancy. The main advantage of using probabilistic marking instead of marking all arriving packets is that flows will be marked in proportion of the number of packets that they transmit. If the router marks 10% of the arriving packets when congested, then a large flow that sends hundred packets per second will be marked 10 times while a flow that only sends one packet per second will not be marked. This probabilistic marking allows marking packets in proportion of their usage of the network resources.</p>
<p>If the router uses several queues served by a scheduler, the situation is different. If a large and a small flow are competing for bandwidth, the scheduler will already favor the small flow that is not using its fair share of the bandwidth. The queue for the small flow will be almost empty while the queue for the large flow will build up. On routers using such schedulers, a good way of marking the packets is to set a threshold on the occupancy of each queue and mark the packets that arrive in a particular queue as soon as its occupancy is above the configured threshold.</p>
</section>
<section id="modeling-tcp-congestion-control">
<h4>Modeling TCP congestion control<a class="headerlink" href="#modeling-tcp-congestion-control" title="Link to this heading">#</a></h4>
<p>Thanks to its congestion control scheme, TCP adapts its transmission rate to the losses that occur in the network. Intuitively, the TCP transmission rate decreases when the percentage of losses increases. Researchers have proposed detailed models that allow the prediction of the throughput of a TCP connection when losses occur <a class="reference internal" href="../bibliography.html#msmo1997" id="id67"><span>[MSMO1997]</span></a> . To have some intuition about the factors that affect the performance of TCP, let us consider a very simple model. Its assumptions are not completely realistic, but it gives us good intuition without requiring complex mathematics.</p>
<p>This model considers a hypothetical TCP connection that suffers from equally spaced segment losses. If <span class="math notranslate nohighlight">\(p\)</span> is the segment loss ratio, then the TCP connection successfully transfers <span class="math notranslate nohighlight">\(\frac{1}{p}-1\)</span> segments and the next segment is lost. If we ignore the slow-start at the beginning of the connection, TCP in this environment is always in congestion avoidance as there are only isolated losses that can be recovered by using fast retransmit. The evolution of the congestion window is thus as shown in the figure below. Note that the <cite>x-axis</cite> of this figure represents time measured in units of one round-trip-time, which is supposed to be constant in the model, and the <cite>y-axis</cite> represents the size of the congestion window measured in MSS-sized segments.</p>
<figure class="align-center" id="id112">
<a class="reference internal image-reference" href="../_images/tcp-congestion-regular.png"><img alt="../_images/tcp-congestion-regular.png" src="../Images/aba5ecafe63f482ae07b7a280f8f3275.png" style="width: 419.29999999999995px; height: 128.79999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-regular.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 226 </span><span class="caption-text">Evolution of the congestion window with regular losses</span><a class="headerlink" href="#id112" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As the losses are equally spaced, the congestion window always starts at some value (<span class="math notranslate nohighlight">\(\frac{W}{2}\)</span>), and is incremented by one MSS every round-trip-time until it reaches twice this value (<cite>W</cite>). At this point, a segment is retransmitted and the cycle starts again. If the congestion window is measured in MSS-sized segments, a cycle lasts <span class="math notranslate nohighlight">\(\frac{W}{2}\)</span> round-trip-times. The bandwidth of the TCP connection is the number of bytes that have been transmitted during a given period of time. During a cycle, the number of segments that are sent on the TCP connection is equal to the area of the yellow trapeze in the figure. Its area is thus :</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(area=(\frac{W}{2})^2 + \frac{1}{2} \times (\frac{W}{2})^2 = \frac{3 \times W^2}{8}\)</span></p>
</div></blockquote>
<p>However, given the regular losses that we consider, the number of segments that are sent between two losses (i.e. during a cycle) is by definition equal to <span class="math notranslate nohighlight">\(\frac{1}{p}\)</span>. Thus, <span class="math notranslate nohighlight">\(W=\sqrt{\frac{8}{3 \times p}}=\frac{k}{\sqrt{p}}\)</span>. The throughput (in bytes per second) of the TCP connection is equal to the number of segments transmitted divided by the duration of the cycle :</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Throughput=\frac{area \times MSS}{time} = \frac{ \frac{3 \times W^2}{8}}{\frac{W}{2} \times rtt}\)</span>
or, after having eliminated <cite>W</cite>, <span class="math notranslate nohighlight">\(Throughput=\sqrt{\frac{3}{2}} \times \frac{MSS}{rtt \times \sqrt{p}}\)</span></p>
</div></blockquote>
<p>More detailed models and the analysis of simulations have shown that a first order model of the TCP throughput when losses occur was <span class="math notranslate nohighlight">\(Throughput \approx \frac{k \times MSS}{rtt \times \sqrt{p}}\)</span>. This is an important result which shows that :</p>
<blockquote>
<div><ul class="simple">
<li><p>TCP connections with a small round-trip-time can achieve a higher throughput than TCP connections having a longer round-trip-time when losses occur. This implies that the TCP congestion control scheme is not completely fair since it favors the connections that have the shorter round-trip-times.</p></li>
<li><p>TCP connections that use a large MSS can achieve a higher throughput that the TCP connections that use a shorter MSS. This creates another source of unfairness between TCP connections. However, it should be noted that today most hosts are using almost the same MSS, roughly 1460 bytes.</p></li>
</ul>
</div></blockquote>
<p>In general, the maximum throughput that can be achieved by a TCP connection depends on its maximum window size and the round-trip-time if there are no losses. If there are losses, it depends on the MSS, the round-trip-time and the loss ratio.</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Throughput&lt;\min(\frac{window}{rtt},\frac{k \times MSS}{rtt \times \sqrt{p}})\)</span></p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The TCP congestion control zoo</p>
<p>The first TCP congestion control scheme was proposed by <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id68"><span>[Jacobson1988]</span></a>. In addition to writing the scientific paper, <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> also implemented the slow-start and congestion avoidance schemes in release 4.3 <cite>Tahoe</cite> of the BSD Unix distributed by the University of Berkeley. Later, he improved the congestion control by adding the fast retransmit and the fast recovery mechanisms in the <cite>Reno</cite> release of 4.3 BSD Unix. Since then, many researchers have proposed, simulated and implemented modifications to the TCP congestion control scheme. Some of these modifications are still used today, e.g. :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>NewReno</cite> (<span class="target" id="index-51"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3782.html"><strong>RFC 3782</strong></a>), which was proposed as an improvement of the fast recovery mechanism in the <cite>Reno</cite> implementation.</p></li>
<li><p><cite>TCP Vegas</cite>, which uses changes in the round-trip-time to estimate congestion in order to avoid it <a class="reference internal" href="../bibliography.html#bop1994" id="id69"><span>[BOP1994]</span></a>. This is one of the examples of the delay-based congestion control algorithms. A Vegas sender continuously measures the evolution of the round-trip-time and slows down when the round-trip-time increases significantly. This enables Vegas to prevent congestion when used alone. Unfortunately, if Vegas senders compete with more aggressive TCP congestion control schemes that only react to losses, Vegas senders may have difficulties to use their fair share of the available bandwidth.</p></li>
<li><p><cite>CUBIC</cite>, which was designed for high bandwidth links and is the default congestion control scheme in Linux since the Linux 2.6.19 kernel <a class="reference internal" href="../bibliography.html#hrx2008" id="id70"><span>[HRX2008]</span></a>. It is now used by several operating systems and is becoming the default congestion control scheme <span class="target" id="index-52"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc8312.html"><strong>RFC 8312</strong></a>. A key difference between CUBIC and the TCP congestion control scheme described in this chapter is that CUBIC is much more aggressive when probing the network. Instead of relying on additive increase after a fast recovery, a CUBIC sender adjusts its congestion by using a cubic function. Thanks to this function, the congestion windows grows faster. This is particularly important in high-bandwidth delay networks.</p></li>
<li><p><cite>BBR</cite>, which is being developed by Google researchers and is included in recent Linux kernels <a class="reference internal" href="../bibliography.html#ccg-2016" id="id71"><span>[CCG+2016]</span></a>. BBR periodically estimates the available bandwidth and the round-trip-times. To adapt to changes in network conditions, BBR regularly tries to send at 1.25 times the current bandwidth. This enables BBR senders to probe the network, but can also cause large amount of losses. Recent scientific articles indicate that BBR is unfair to other congestion control schemes in specific conditions <a class="reference internal" href="../bibliography.html#wmss2019" id="id72"><span>[WMSS2019]</span></a>.</p></li>
</ul>
</div></blockquote>
<p>A wide range of congestion control schemes have been proposed in the scientific literature and several of them have been widely deployed. A detailed comparison of these congestion control schemes is outside the scope of this chapter. A recent survey paper describing many of the implemented TCP congestion control schemes may be found in <a class="reference internal" href="../bibliography.html#tku2019" id="id73"><span>[TKU2019]</span></a>.</p>
</div>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fbufferbloat" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>There are still some vendors that try to put as many buffers as possible on their routers. A recent example is the buffer bloat problem that plagues some low-end Internet routers <a class="reference internal" href="../bibliography.html#gn2011" id="id74"><span>[GN2011]</span></a>.</p>
</aside>
<aside class="footnote brackets" id="fpps" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>Some examples of the performance of various types of commercial networks nodes (routers and switches) may be found in <a class="reference external" href="http://www.cisco.com/web/partners/downloads/765/tools/quickreference/routerperformance.pdf">http://www.cisco.com/web/partners/downloads/765/tools/quickreference/routerperformance.pdf</a> and <a class="reference external" href="http://www.cisco.com/web/partners/downloads/765/tools/quickreference/switchperformance.pdf">http://www.cisco.com/web/partners/downloads/765/tools/quickreference/switchperformance.pdf</a></p>
</aside>
<aside class="footnote brackets" id="fadjust" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>Some networking technologies allow to adjust dynamically the bandwidth of links. For example, some devices can reduce their bandwidth to preserve energy. We ignore these technologies in this basic course and assume that all links used inside the network have a fixed bandwidth.</p>
</aside>
<aside class="footnote brackets" id="fslottime" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">4</a><span class="fn-bracket">]</span></span>
<p>This name should not be confused with the duration of a transmission slot in slotted ALOHA. In CSMA/CD networks, the slot time is the time during which a collision can occur at the beginning of the transmission of a frame. In slotted ALOHA, the duration of a slot is the transmission time of an entire fixed-size frame.</p>
</aside>
<aside class="footnote brackets" id="fcredit" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id42">5</a><span class="fn-bracket">]</span></span>
<p>In this section, we focus on congestion control mechanisms that regulate the transmission rate of the hosts. Other types of mechanisms have been proposed in the literature. For example, <cite>credit-based</cite> flow-control has been proposed to avoid congestion in ATM networks <a class="reference internal" href="../bibliography.html#kr1995" id="id75"><span>[KR1995]</span></a>. With a credit-based mechanism, hosts can only send packets once they have received credits from the routers and the credits depend on the occupancy of the router’s buffers.</p>
</aside>
<aside class="footnote brackets" id="fflowslink" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id45">6</a><span class="fn-bracket">]</span></span>
<p>For example, the measurements performed in the Sprint network in 2004 reported more than 10k active TCP connections on a link, see <a class="reference external" href="https://research.sprintlabs.com/packstat/packetoverview.php">https://research.sprintlabs.com/packstat/packetoverview.php</a>. More recent information about backbone links may be obtained from <a class="reference external" href="https://www.caida.org">caida</a> ‘s real-time measurements, see e.g.  <a class="reference external" href="http://www.caida.org/data/realtime/passive/">http://www.caida.org/data/realtime/passive/</a></p>
</aside>
</aside>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fwrap" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id57">7</a><span class="fn-bracket">]</span></span>
<p>In this pseudo-code, we assume that TCP uses unlimited sequence and acknowledgment numbers. Furthermore, we do not detail how the <cite>cwnd</cite> is adjusted after the retransmission of the lost segment by fast retransmit. Additional details may be found in <span class="target" id="index-53"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc5681.html"><strong>RFC 5681</strong></a>.</p>
</aside>
<aside class="footnote brackets" id="fprivate" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id63">8</a><span class="fn-bracket">]</span></span>
<p>In enterprise networks or datacenters, the situation is different since a single company typically controls all the sources and all the routers. In such networks it is possible to ensure that all hosts and routers have been upgraded before turning on ECN on the routers.</p>
</aside>
<aside class="footnote brackets" id="fecnnonce" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id64">9</a><span class="fn-bracket">]</span></span>
<p>With the ECT bit, the deployment issue with ECN is solved provided that all sources cooperate. If some sources do not support ECN but still set the ECT bit in the packets that they sent, they will have an unfair advantage over the sources that correctly react to packet markings. Several solutions have been proposed to deal with this problem <span class="target" id="index-54"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3540.html"><strong>RFC 3540</strong></a>, but they are outside the scope of this book.</p>
</aside>
<aside class="footnote brackets" id="fslot" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id65">10</a><span class="fn-bracket">]</span></span>
<p>The buffers of a router can be implemented as variable or fixed-length slots. If the router uses variable length slots to store the queued packets, then the occupancy is usually measured in bytes. Some routers have use fixed-length slots with each slot large enough to store a maximum-length packet. In this case, the buffer occupancy is measured in packets.</p>
</aside>
</aside>
</section>
</section>
&#13;

<h3>Congestion control with a window-based transport protocol<a class="headerlink" href="#congestion-control-with-a-window-based-transport-protocol" title="Link to this heading">#</a></h3>
<p>AIMD controls congestion by adjusting the transmission rate of the sources in reaction to the current congestion level. If the network is not congested, the transmission rate increases. If congestion is detected, the transmission rate is multiplicatively decreased. In practice, directly adjusting the transmission rate can be difficult since it requires the utilization of fine grained timers. In reliable transport protocols, an alternative is to dynamically adjust the sending window. This is the solution chosen for protocols like TCP and SCTP that will be described in more details later. To understand how window-based protocols can adjust their transmission rate, let us consider the very simple scenario of a reliable transport protocol that uses <cite>go-back-n</cite>. Consider the very simple scenario shown in figure <a class="reference internal" href="#fig-bottleneck"><span class="std std-numref">Fig. 215</span></a>.</p>
<blockquote>
<div><div class="figure" id="id101" style="text-align: center">
<span id="fig-bottleneck"/><p><img src="../Images/fba8d6b7380280ee70f3550fb599fb9f.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-c571e39a73d7fa60a9759c2a5d1d690b0ec7f12a.png"/></p>
<p><span class="caption-number">Fig. 215 </span><span class="caption-text">A simple network with hosts sharing a bottleneck link</span></p>
</div></div></blockquote>
<p>The links between the hosts and the routers have a bandwidth of 1 Mbps while the link between the two routers has a bandwidth of 500 Kbps. There is no significant propagation delay in this network. For simplicity, assume that hosts <cite>A</cite> and <cite>B</cite> send 1000 bits packets. The transmission of such a packet on a <cite>host-router</cite> (resp. <cite>router-router</cite> ) link requires 1 msec (resp. 2 msec). If there is no traffic in the network, the round-trip-time measured by host <cite>A</cite> to reach <cite>D</cite> is slightly larger than 4 msec. Let us observe the flow of packets with different window sizes to understand the relationship between sending window and transmission rate.</p>
<p>Consider first a window of one segment. This segment takes 4 msec to reach host <cite>D</cite>. The destination replies with an acknowledgment and the next segment can be transmitted. With such a sending window, the transmission rate is roughly 250 segments per second or 250 Kbps. This is illustrated in figure <a class="reference internal" href="#fig-gbn-win-1"><span class="std std-numref">Fig. 216</span></a> where each square of the grid corresponds to one millisecond.</p>
<blockquote>
<div><div class="figure" id="id102" style="text-align: center">
<span id="fig-gbn-win-1"/><p><img src="../Images/fb17e125cf960286b8258b6251471572.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-5d002e7b389ca36e47cf6c9f674938a0d08593cb.png"/></p>
<p><span class="caption-number">Fig. 216 </span><span class="caption-text">Go-back-n transfer from A to D, window of one segment</span></p>
</div></div></blockquote>
<p>Consider now a window of two segments. Host <cite>A</cite> can send two segments within 2 msec on its 1 Mbps link. If the first segment is sent at time <span class="math notranslate nohighlight">\(t_{0}\)</span>, it reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. Host <cite>D</cite> replies with an acknowledgment that opens the sending window on host <cite>A</cite> and enables it to transmit a new segment. In the meantime, the second segment was buffered by router <cite>R1</cite>. It reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+6\)</span> and an acknowledgment is returned. With a window of two segments, host <cite>A</cite> transmits at roughly 500 Kbps, i.e. the transmission rate of the bottleneck link.</p>
<blockquote>
<div><div class="figure" id="id103" style="text-align: center"><p><img src="../Images/972780cbffde5435003f04c4edfabc56.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-d13039c2482f96c3496fdd84e458374e31963a74.png"/></p>
<p><span class="caption-number">Fig. 217 </span><span class="caption-text">Go-back-n transfer from A to D, window of two segments</span></p>
</div></div></blockquote>
<p>Our last example is a window of four segments. These segments are sent at <span class="math notranslate nohighlight">\(t_{0}\)</span>, <span class="math notranslate nohighlight">\(t_{0}+1\)</span>, <span class="math notranslate nohighlight">\(t_{0}+2\)</span> and <span class="math notranslate nohighlight">\(t_{0}+3\)</span>. The first segment reaches host <cite>D</cite> at <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. Host <cite>D</cite> replies to this segment by sending an acknowledgment that enables host <cite>A</cite> to transmit its fifth segment. This segment reaches router <cite>R1</cite> at <span class="math notranslate nohighlight">\(t_{0}+5\)</span>. At that time, router <cite>R1</cite> is transmitting the third segment to router <cite>R2</cite> and the fourth segment is still in its buffers. At time <span class="math notranslate nohighlight">\(t_{0}+6\)</span>, host <cite>D</cite> receives the second segment and returns the corresponding acknowledgment. This acknowledgment enables host <cite>A</cite> to send its sixth segment. This segment reaches router <cite>R1</cite> at roughly <span class="math notranslate nohighlight">\(t_{0}+7\)</span>. At that time, the router starts to transmit the fourth segment to router <cite>R2</cite>. Since link <cite>R1-R2</cite> can only sustain 500 Kbps, packets will accumulate in the buffers of <cite>R1</cite>. On average, there will be two packets waiting in the buffers of <cite>R1</cite>. The presence of these two packets will induce an increase of the round-trip-time as measured by the transport protocol. While the first segment was acknowledged within 4 msec, the fifth segment (<cite>data(4)</cite>) that was transmitted at time <span class="math notranslate nohighlight">\(t_{0}+4\)</span> is only acknowledged at time <span class="math notranslate nohighlight">\(t_{0}+11\)</span>. On average, the sender transmits at 500 Kbps, but the utilization of a large window induces a longer delay through the network.</p>
<blockquote>
<div><div class="figure" id="id104" style="text-align: center"><p><img src="../Images/6fac88cb38e8b98800e49c18ffa34a0c.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-b9493099e8639557dc24065f10c48391e80c82fd.png"/></p>
<p><span class="caption-number">Fig. 218 </span><span class="caption-text">Go-back-n transfer from A to D, window of four segments</span></p>
</div></div></blockquote>
<p id="index-42">From the above example, we can adjust the transmission rate by adjusting the sending window of a reliable transport protocol. A reliable transport protocol cannot send data faster than <span class="math notranslate nohighlight">\(\frac{window}{rtt}\)</span> segments per second where <span class="math notranslate nohighlight">\(window\)</span> is the current sending window. To control the transmission rate, we introduce a <cite>congestion window</cite>. This congestion window limits the sending window. At any time, the sending window is restricted to <span class="math notranslate nohighlight">\(\min(swin,cwin)\)</span>, where <cite>swin</cite> is the sending window and <cite>cwin</cite> the current <cite>congestion window</cite>. Of course, the window is further constrained by the receive window advertised by the remote peer. With the utilization of a congestion window, a simple reliable transport protocol that uses fixed size segments could implement <cite>AIMD</cite> as follows.</p>
<p>For the <cite>Additive Increase</cite> part our simple protocol would simply increase its <cite>congestion window</cite> by one segment every round-trip-time. The
<cite>Multiplicative Decrease</cite> part of <cite>AIMD</cite> could be implemented by halving the congestion window when congestion is detected. For simplicity, we assume that congestion is detected thanks to a binary feedback and that no segments are lost. We will discuss in more details how losses affect a real transport protocol like TCP in later sections.</p>
<p>A congestion control scheme for our simple transport protocol could be implemented as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># Initialisation</span>
<span class="n">cwin</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># congestion window measured in segments</span>

<span class="c1"># Ack arrival</span>
<span class="k">if</span> <span class="n">ack_received</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">newack</span><span class="p">:</span>  <span class="c1"># new ack, no congestion</span>
        <span class="c1"># increase cwin by one every rtt</span>
        <span class="n">cwin</span> <span class="o">=</span> <span class="n">cwin</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">cwin</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># no increase</span>

<span class="k">if</span> <span class="n">congestion_detected</span><span class="p">:</span>
    <span class="n">cwin</span> <span class="o">=</span> <span class="n">cwin</span> <span class="o">/</span> <span class="mi">2</span> <span class="c1"># only once per rtt</span>
</pre></div>
</div>
<p>In the above pseudocode, <cite>cwin</cite> contains the congestion window stored as a real number of segments. This congestion window is updated upon the arrival of each acknowledgment and when congestion is detected. For simplicity, we assume that <cite>cwin</cite> is stored as a floating point number but only full segments can be transmitted.</p>
<p>As an illustration, let us consider the network scenario above and assume that the router implements the DECBit binary feedback scheme <a class="reference internal" href="../bibliography.html#rj1995" id="id53"><span>[RJ1995]</span></a>. This scheme uses a form of Forward Explicit Congestion Notification and a router marks the congestion bit in arriving packets when its buffer contains one or more packets. In figure <a class="reference internal" href="#fig-gbn-decbit"><span class="std std-numref">Fig. 219</span></a>, we use a <cite>*</cite> to indicate a marked packet.</p>
<blockquote>
<div><div class="figure" id="id105" style="text-align: center">
<span id="fig-gbn-decbit"/><p><img src="../Images/2b9761249f9ab8572a0fc4b3416b6752.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-c2a62409e962fdfb2fe7754a330fce96bb7f3bd0.png"/></p>
<p><span class="caption-number">Fig. 219 </span><span class="caption-text">Go-back-n transfer from A to D, with AIMD congestion control and DecBit binary feedback scheme</span></p>
</div></div></blockquote>
<p>When the connection starts, its congestion window is set to one segment. Segment <cite>S0</cite> is sent an acknowledgment at roughly <span class="math notranslate nohighlight">\(t_{0}+4\)</span>. The congestion window is increased by one segment and <cite>S1</cite> and <cite>S2</cite> are transmitted at time <span class="math notranslate nohighlight">\(t_{0}+4\)</span> and <span class="math notranslate nohighlight">\(t_{0}+5\)</span>. The corresponding acknowledgments are received at times <span class="math notranslate nohighlight">\(t_{0}+8\)</span> and <span class="math notranslate nohighlight">\(t_{0}+10\)</span>. Upon reception of this last acknowledgment, the congestion window reaches <cite>3</cite> and segments can be sent (<cite>S4</cite> and <cite>S5</cite>). When segment <cite>S6</cite> reaches router <cite>R1</cite>, its buffers already contain <cite>S5</cite>. The packet containing <cite>S6</cite> is thus marked to inform the sender of the congestion. Note that the sender will only notice the congestion once it receives the corresponding acknowledgment at <span class="math notranslate nohighlight">\(t_{0}+18\)</span>. In the meantime, the congestion window continues to increase. At <span class="math notranslate nohighlight">\(t_{0}+16\)</span>, upon reception of the acknowledgment for <cite>S5</cite>, it reaches <cite>4</cite>. When congestion is detected, the congestion window is decreased down to <cite>2</cite>. This explains the idle time between the reception of the acknowledgment for <cite>S*6</cite> and the transmission of <cite>S10</cite>.</p>
<p>In practice, a router is connected to multiple input links. Figure <a class="reference internal" href="#fig-2hosts-bottleneck"><span class="std std-numref">Fig. 220</span></a> shows an example with two hosts.</p>
<blockquote>
<div><div class="figure" id="id106" style="text-align: center">
<span id="fig-2hosts-bottleneck"/><p><img src="../Images/54044b2beb542c9ff304fe654e7cbe2e.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-699cdd177204f453f241862f3b959ea743b24e53.png"/></p>
<p><span class="caption-number">Fig. 220 </span><span class="caption-text">A simple network with hosts sharing a bottleneck</span></p>
</div><div class="figure" id="id107" style="text-align: center"><p><img src="../Images/14805061572e2ea7a08e10338a022ff8.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-966439ccf02ecbf0381106d45a0dcbcfb8a1412d.png"/></p>
<p><span class="caption-number">Fig. 221 </span><span class="caption-text">Sharing the bottleneck link between different inputs</span></p>
</div></div></blockquote>
<p>In general, the links have a non-zero delay. This is illustrated in the figure below where a delay has been added on the link between <cite>R</cite> and <cite>C</cite>.</p>
<blockquote>
<div><div class="figure" id="id108" style="text-align: center"><p><img src="../Images/515eaae674689b5424d8fbcf3dfe9c0e.png" alt="Figure made with TikZ" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tikz-db6942ecfbc5147fe81ef57fed06977ad34ff651.png"/></p>
<p><span class="caption-number">Fig. 222 </span><span class="caption-text">Sharing the bottleneck link between different inputs</span></p>
</div></div></blockquote>
&#13;

<span id="id54"/><h3>Congestion control<a class="headerlink" href="#tcpcongestion" title="Link to this heading">#</a></h3>
<p>In an internetwork, i.e. a networking composed of different types of networks (such as the Internet), congestion control could be implemented either in the network layer or the transport layer. The congestion problem was clearly identified in the later 1980s and the researchers who developed techniques to solve the problem opted for a solution in the transport layer. Adding congestion control to the transport layer makes sense since this layer provides a reliable data transfer and avoiding congestion is a factor in this reliable delivery. The transport layer already deals with heterogeneous networks thanks to its <cite>self-clocking</cite> property that we have already described. In this section, we explain how congestion control has been added to TCP and how this mechanism could be improved in the future.</p>
<p>The TCP congestion control scheme was initially proposed by <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id55"><span>[Jacobson1988]</span></a>. The current specification may be found in <span class="target" id="index-43"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc5681.html"><strong>RFC 5681</strong></a>. TCP relies on <cite>Additive Increase and Multiplicative Decrease (AIMD)</cite>. To implement <a class="reference internal" href="../glossary.html#term-AIMD"><span class="xref std std-term">AIMD</span></a>, a TCP host must be able to control its transmission rate. A first approach would be to use timers and adjust their expiration times in function of the rate imposed by <a class="reference internal" href="../glossary.html#term-AIMD"><span class="xref std std-term">AIMD</span></a>. Unfortunately, maintaining such timers for a large number of TCP connections can be difficult. Instead, <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> noted that the rate of TCP congestion can be artificially controlled by constraining its sending window. A TCP connection cannot send data faster than <span class="math notranslate nohighlight">\(\frac{window}{rtt}\)</span> where <span class="math notranslate nohighlight">\(window\)</span> is the minimum between the host’s sending window and the window advertised by the receiver.</p>
<p>TCP’s congestion control scheme is based on a <cite>congestion window</cite>. The current value of the congestion window (<cite>cwnd</cite>) is stored in the TCB of each TCP connection and the window that can be used by the sender is constrained by <span class="math notranslate nohighlight">\(\min(cwnd,rwin,swin)\)</span> where <span class="math notranslate nohighlight">\(swin\)</span> is the current sending window and <span class="math notranslate nohighlight">\(rwin\)</span> the last received receive window. The <cite>Additive Increase</cite> part of the TCP congestion control increments the congestion window by <a class="reference internal" href="../glossary.html#term-MSS"><span class="xref std std-term">MSS</span></a> bytes every round-trip-time. In the TCP literature, this phase is often called the <cite>congestion avoidance</cite> phase. The <cite>Multiplicative Decrease</cite> part of the TCP congestion control divides the current value of the congestion window once congestion has been detected.</p>
<p>When a TCP connection begins, the sending host does not know whether the part of the network that it uses to reach the destination is congested or not. To avoid causing too much congestion, it must start with a small congestion window. <a class="reference internal" href="../bibliography.html#jacobson1988" id="id56"><span>[Jacobson1988]</span></a> recommends an initial window of MSS bytes. As the additive increase part of the TCP congestion control scheme increments the congestion window by MSS bytes every round-trip-time, the TCP connection may have to wait many round-trip-times before being able to efficiently use the available bandwidth. This is especially important in environments where the <span class="math notranslate nohighlight">\(bandwidth \times rtt\)</span> product is high. To avoid waiting too many round-trip-times before reaching a congestion window that is large enough to efficiently utilize the network, the TCP congestion control scheme includes the <cite>slow-start</cite> algorithm. The objective of the TCP <cite>slow-start</cite> phase is to quickly reach an acceptable value for the <cite>cwnd</cite>. During <cite>slow-start</cite>, the congestion window is doubled every round-trip-time. The <cite>slow-start</cite> algorithm uses an additional variable in the TCB : <cite>ssthresh</cite> (<cite>slow-start threshold</cite>). The <cite>ssthresh</cite> is an estimation of the last value of the <cite>cwnd</cite> that did not cause congestion. It is initialized at the sending window and is updated after each congestion event.</p>
<p>A key question that must be answered by any congestion control scheme is how congestion is detected. The first implementations of the TCP congestion control scheme opted for a simple and pragmatic approach : packet losses indicate congestion. If the network is congested, router buffers are full and packets are discarded. In wired networks, packet losses are mainly caused by congestion. In wireless networks, packets can be lost due to transmission errors and for other reasons that are independent of congestion. TCP already detects segment losses to ensure a reliable delivery. The TCP congestion control scheme distinguishes between two types of congestion :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>mild congestion</cite>. TCP considers that the network is lightly congested if it receives three duplicate acknowledgments and performs a fast retransmit. If the fast retransmit is successful, this implies that only one segment has been lost. In this case, TCP performs multiplicative decrease and the congestion window is divided by <cite>2</cite>. The slow-start threshold is set to the new value of the congestion window.</p></li>
<li><p><cite>severe congestion</cite>. TCP considers that the network is severely congested when its retransmission timer expires. In this case, TCP retransmits the first segment, sets the slow-start threshold to 50% of the congestion window. The congestion window is reset to its initial value and TCP performs a slow-start.</p></li>
</ul>
</div></blockquote>
<p>The figure below illustrates the evolution of the congestion window when there is severe congestion. At the beginning of the connection, the sender performs <cite>slow-start</cite> until the first segments are lost and the retransmission timer expires. At this time, the <cite>ssthresh</cite> is set to half of the current congestion window and the congestion window is reset at one segment. The lost segments are retransmitted as the sender again performs slow-start until the congestion window reaches the <cite>sshtresh</cite>. It then switches to congestion avoidance and the congestion window increases linearly until segments are lost and the retransmission timer expires.</p>
<figure class="align-center" id="id109">
<a class="reference internal image-reference" href="../_images/tcp-congestion-severe.png"><img alt="../_images/tcp-congestion-severe.png" src="../Images/8bb9ad90fd07bc82d9cba2b40a1481a6.png" style="width: 461.99999999999994px; height: 186.89999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-severe.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 223 </span><span class="caption-text">Evaluation of the TCP congestion window with severe congestion</span><a class="headerlink" href="#id109" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The figure below illustrates the evolution of the congestion window when the network is lightly congested and all lost segments can be retransmitted using fast retransmit. The sender begins with a slow-start. A segment is lost but successfully retransmitted by a fast retransmit. The congestion window is divided by 2 and the sender immediately enters congestion avoidance as this was a mild congestion.</p>
<figure class="align-center" id="id110">
<a class="reference internal image-reference" href="../_images/tcp-congestion-mild.png"><img alt="../_images/tcp-congestion-mild.png" src="../Images/118024671401b1ade2c8daa21527e64f.png" style="width: 454.99999999999994px; height: 178.5px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-mild.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 224 </span><span class="caption-text">Evaluation of the TCP congestion window when the network is lightly congested</span><a class="headerlink" href="#id110" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Most TCP implementations update the congestion window when they receive an acknowledgment. If we assume that the receiver acknowledges each received segment and the sender only sends MSS sized segments, the TCP congestion control scheme can be implemented using the simplified pseudo-code <a class="footnote-reference brackets" href="#fwrap" id="id57" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> below. This pseudocode includes the optimization proposed in <span class="target" id="index-44"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3042.html"><strong>RFC 3042</strong></a> that allows a sender to send new unsent data upon reception of the first or second duplicate acknowledgment. The reception of each of these acknowledgments indicates that one segment has left the network and thus additional data can be sent without causing more congestion. Note that the congestion window is <em>not</em> increased upon reception of these first duplicate acknowledgments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="c1"># Initialization</span>
<span class="n">cwnd</span> <span class="o">=</span> <span class="n">MSS</span>  <span class="c1"># congestion window in bytes</span>
<span class="n">ssthresh</span><span class="o">=</span> <span class="n">swin</span> <span class="c1"># in bytes</span>

<span class="c1"># Ack arrival</span>
<span class="k">if</span> <span class="n">tcp</span><span class="o">.</span><span class="n">ack</span> <span class="o">&gt;</span> <span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">:</span>  <span class="c1"># new ack, no congestion</span>
    <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not currently recovering from loss</span>
        <span class="k">if</span> <span class="n">cwnd</span> <span class="o">&lt;</span> <span class="n">ssthresh</span><span class="p">:</span>
            <span class="c1"># slow-start : quickly increase cwnd</span>
            <span class="c1"># double cwnd every rtt</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># congestion avoidance : slowly increase cwnd</span>
            <span class="c1"># increase cwnd by one mss every rtt</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span> <span class="o">*</span> <span class="p">(</span><span class="n">MSS</span> <span class="o">/</span> <span class="n">cwnd</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># recovering from loss</span>
        <span class="n">cwnd</span> <span class="o">=</span> <span class="n">ssthresh</span>  <span class="c1"># deflate cwnd RFC5681</span>
        <span class="n">dupacks</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># duplicate or old ack</span>
    <span class="k">if</span> <span class="n">tcp</span><span class="o">.</span><span class="n">ack</span> <span class="o">==</span> <span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">:</span>  <span class="c1"># duplicate acknowledgment</span>
        <span class="n">dupacks</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">send_next_unacked_segment</span>  <span class="c1"># RFC3042</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">retransmitsegment</span><span class="p">(</span><span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">)</span>
            <span class="n">ssthresh</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">cwnd</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">MSS</span><span class="p">)</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">ssthresh</span>
        <span class="k">if</span> <span class="n">dupacks</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># RFC5681</span>
            <span class="n">cwnd</span> <span class="o">=</span> <span class="n">cwnd</span> <span class="o">+</span> <span class="n">MSS</span>  <span class="c1"># inflate cwnd</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># ack for old segment, ignored</span>
        <span class="k">pass</span>

<span class="n">Expiration</span> <span class="n">of</span> <span class="n">the</span> <span class="n">retransmission</span> <span class="n">timer</span><span class="p">:</span>
    <span class="n">send</span><span class="p">(</span><span class="n">snd</span><span class="o">.</span><span class="n">una</span><span class="p">)</span>  <span class="c1"># retransmit first lost segment</span>
    <span class="n">sshtresh</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">cwnd</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">MSS</span><span class="p">)</span>
    <span class="n">cwnd</span> <span class="o">=</span> <span class="n">MSS</span>
</pre></div>
</div>
<p>Furthermore when a TCP connection has been idle for more than its current retransmission timer, it should reset its congestion window to the congestion window size that it uses when the connection begins, as it no longer knows the current congestion state of the network.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Initial congestion window</p>
<p>The original TCP congestion control mechanism proposed in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id58"><span>[Jacobson1988]</span></a> recommended that each TCP connection should begin by setting <span class="math notranslate nohighlight">\(cwnd=MSS\)</span>. However, in today’s higher bandwidth networks, using such a small initial congestion window severely affects the performance for short TCP connections, such as those used by web servers. In 2002, <span class="target" id="index-45"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3390.html"><strong>RFC 3390</strong></a> allowed an initial congestion window of about 4 KBytes, which corresponds to 3 segments in many environments. Recently, researchers from Google proposed to further increase the initial window up to 15 KBytes <a class="reference internal" href="../bibliography.html#drc-2010" id="id59"><span>[DRC+2010]</span></a>. The measurements that they collected show that this increase would not significantly increase congestion but would significantly reduce the latency of short HTTP responses. Unsurprisingly, the chosen initial window corresponds to the average size of an HTTP response from a search engine. This proposed modification has been adopted in <span class="target" id="index-46"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc6928.html"><strong>RFC 6928</strong></a> and TCP implementations support it.</p>
</div>
<section id="controlling-congestion-without-losing-data">
<h4>Controlling congestion without losing data<a class="headerlink" href="#controlling-congestion-without-losing-data" title="Link to this heading">#</a></h4>
<p>In today’s Internet, congestion is controlled by regularly sending packets at a higher rate than the network capacity. These packets fill the buffers of the routers and are eventually discarded. But shortly after, TCP senders retransmit packets containing exactly the same data. This is potentially a waste of resources since these successive retransmissions consume resources upstream of the router that discards the packets. Packet losses are not the only signal to detect congestion inside the network. An alternative is to allow routers to explicitly indicate their current level of congestion when forwarding packets. This approach was proposed in the late 1980s <a class="reference internal" href="../bibliography.html#rj1995" id="id60"><span>[RJ1995]</span></a> and used in some networks. Unfortunately, it took almost a decade before the Internet community agreed to consider this approach. In the mean time, a large number of TCP implementations and routers were deployed on the Internet.</p>
<p>As explained earlier, Explicit Congestion Notification <span class="target" id="index-47"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> improves the detection of congestion by allowing routers to explicitly mark packets when they are lightly congested. In theory, a single bit in the packet header <a class="reference internal" href="../bibliography.html#rj1995" id="id61"><span>[RJ1995]</span></a> is sufficient to support this congestion control scheme. When a host receives a marked packet, it returns the congestion information to the source that adapts its transmission rate accordingly. Although the idea is relatively simple, deploying it on the entire Internet has proven to be challenging <a class="reference internal" href="../bibliography.html#knt2013" id="id62"><span>[KNT2013]</span></a>. It is interesting to analyze the different factors that have hindered the deployment of this technique.</p>
<p>The first difficulty in adding Explicit Congestion Notification (ECN) in TCP/IP network was to modify the format of the network packet and transport segment headers to carry the required information. In the network layer, one bit was required to allow the routers to mark the packets they forward during congestion periods. In the IP network layer, this bit is called the <cite>Congestion Experienced</cite> (<cite>CE</cite>) bit and is part of the packet header. However, using a single bit to mark packets is not sufficient. Consider a simple scenario with two sources, one congested router and one destination. Assume that the first sender and the destination support ECN, but not the second sender. If the router is congested it will mark packets from both senders. The first sender will react to the packet markings by reducing its transmission rate. However since the second sender does not support ECN, it will not react to the markings. Furthermore, this sender could continue to increase its transmission rate, which would lead to more packets being marked and the first source would decrease again its transmission rate, … In the end, the sources that implement ECN are penalized compared to the sources that do not implement it. This unfairness issue is a major hurdle to widely deploy ECN on the public Internet <a class="footnote-reference brackets" href="#fprivate" id="id63" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>. The solution proposed in <span class="target" id="index-48"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> to deal with this problem is to use a second bit in the network packet header. This bit, called the <cite>ECN-capable transport</cite> (ECT) bit, indicates whether the packet contains a segment produced by a transport protocol that supports ECN or not. Transport protocols that support ECN set the ECT bit in all packets. When a router is congested, it first verifies whether the ECT bit is set. In this case, the CE bit of the packet is set to indicate congestion. Otherwise, the packet is discarded. This eases the deployment of ECN <a class="footnote-reference brackets" href="#fecnnonce" id="id64" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>.</p>
<p>The second difficulty is how to allow the receiver to inform the sender of the reception of network packets marked with the <cite>CE</cite> bit. In reliable transport protocols like TCP and SCTP, the acknowledgments can be used to provide this feedback. For TCP, two options were possible : change some bits in the TCP segment header or define a new TCP option to carry this information. The designers of ECN opted for reusing spare bits in the TCP header. More precisely, two TCP flags have been added in the TCP header to support ECN. The <cite>ECN-Echo</cite> (ECE) is set in the acknowledgments when the <cite>CE</cite> was set in packets received on the forward path.</p>
<figure class="align-default" id="id111">
<a class="reference internal image-reference" href="../_images/tcp-enc.svg"><img alt="../_images/tcp-enc.svg" src="../Images/0b92b95f1c0dadde63905a28574e87cb.png" style="width: 664.8px; height: 115.19999999999999px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-enc.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 225 </span><span class="caption-text">The TCP flags</span><a class="headerlink" href="#id111" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The third difficulty is to allow an ECN-capable sender to detect whether the remote host also supports ECN. This is a classical negotiation of extensions to a transport protocol. In TCP, this could have been solved by defining a new TCP option used during the three-way handshake. To avoid wasting space in the TCP options, the designers of ECN opted in <span class="target" id="index-49"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> for using the <cite>ECN-Echo</cite> and <cite>CWR</cite> bits in the TCP header to perform this negotiation. In the end, the result is the same with fewer bits exchanged.</p>
<p>Thanks to the <cite>ECT</cite>, <cite>CE</cite> and <cite>ECE</cite>, routers can mark packets during congestion and receivers can return the congestion information back to the TCP senders. However, these three bits are not sufficient to allow a server to reliably send the <cite>ECE</cite> bit to a TCP sender. TCP acknowledgments are not sent reliably. A TCP acknowledgment always contains the next expected sequence number. Since TCP acknowledgments are cumulative, the loss of one acknowledgment is recovered by the correct reception of a subsequent acknowledgment.</p>
<p>If TCP acknowledgments are overloaded to carry the <cite>ECE</cite> bit, the situation is different. Consider the example shown in the figure below. A client sends packets to a server through a router. In the example below, the first packet is marked. The server returns an acknowledgment with the <cite>ECE</cite> bit set. Unfortunately, this acknowledgment is lost and never reaches the client. Shortly after, the server sends a data segment that also carries a cumulative acknowledgment. This acknowledgment confirms the reception of the data to the client, but it did not receive the congestion information through the <cite>ECE</cite> bit.</p>
<blockquote>
<div><p class="mscgen">
<img src="../Images/d2ed1589241adeb9e699082587d1916d.png" alt="msc {&#10;client [label=&quot;client&quot;, linecolour=black],&#10;router [label=&quot;router&quot;, linecolour=black],&#10;server [label=&quot;server&quot;, linecolour=black];&#10;&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;server=&gt;router [ label = &quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;router -x client [label=&quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;|||;&#10;server=&gt;router [ label = &quot;data[seq=x,ack=2,ECE=0,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;client [ label = &quot;data[seq=x,ack=2,ECE=0,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client-&gt;server [linecolour=white];&#10;}" usemap="#700c52eed1dd99ea5abd5216ffd2e044e6fee931" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/mscgen-700c52eed1dd99ea5abd5216ffd2e044e6fee931.png"/>
<map id="700c52eed1dd99ea5abd5216ffd2e044e6fee931" name="700c52eed1dd99ea5abd5216ffd2e044e6fee931"/></p>
</div></blockquote>
<p>To solve this problem, <span class="target" id="index-50"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> uses an additional bit in the TCP header : the <cite>Congestion Window Reduced</cite> (CWR) bit.</p>
<blockquote>
<div><p class="mscgen">
<img src="../Images/36533b7c4d47ba50cac29ebf6ebcc1f1.png" alt="msc {&#10;client [label=&quot;client&quot;, linecolour=black],&#10;router [label=&quot;router&quot;, linecolour=black],&#10;server [label=&quot;server&quot;, linecolour=black];&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;server=&gt;router [ label = &quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;router -x client [label=&quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;|||;&#10;server=&gt;router [ label = &quot;data[seq=x,ack=2,ECE=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;client [ label = &quot;data[seq=x,ack=2,ECE=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0,CWR=1]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1,CWR=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client-&gt;server [linecolour=white];&#10;}" usemap="#d60c580a7379dbdd26f90fd2eead54f832ee6ad6" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/mscgen-d60c580a7379dbdd26f90fd2eead54f832ee6ad6.png"/>
<map id="d60c580a7379dbdd26f90fd2eead54f832ee6ad6" name="d60c580a7379dbdd26f90fd2eead54f832ee6ad6"/></p>
</div></blockquote>
<p>The <cite>CWR</cite> bit of the TCP header provides some form of acknowledgment for the <cite>ECE</cite> bit. When a TCP receiver detects a packet marked with the <cite>CE</cite> bit, it sets the <cite>ECE</cite> bit in all segments that it returns to the sender. Upon reception of an acknowledgment with the <cite>ECE</cite> bit set, the sender reduces its congestion window to reflect a mild congestion and sets the <cite>CWR</cite> bit. This bit remains set as long as the segments received contained the <cite>ECE</cite> bit set. A sender should only react once per round-trip-time to marked packets.</p>
<p>The last point that needs to be discussed about Explicit Congestion Notification is the algorithm that is used by routers to detect congestion. On a router, congestion manifests itself by the number of packets that are stored inside the router buffers. As explained earlier, we need to distinguish between two types of routers :</p>
<blockquote>
<div><ul class="simple">
<li><p>routers that have a single FIFO queue</p></li>
<li><p>routers that have several queues served by a round-robin scheduler</p></li>
</ul>
</div></blockquote>
<p>Routers that use a single queue measure their buffer occupancy as the number of bytes of packets stored in the queue <a class="footnote-reference brackets" href="#fslot" id="id65" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. A first method to detect congestion is to measure the instantaneous buffer occupancy and consider the router to be congested as soon as this occupancy is above a threshold. Typical values of the threshold could be 40% of the total buffer. Measuring the instantaneous buffer occupancy is simple since it only requires one counter. However, this value is fragile from a control viewpoint since it changes frequently. A better solution is to measure the <em>average</em> buffer occupancy and consider the router to be congested when this average occupancy is too high. Random Early Detection (RED) <a class="reference internal" href="../bibliography.html#fj1993" id="id66"><span>[FJ1993]</span></a> is an algorithm that was designed to support Explicit Congestion Notification. In addition to measuring the average buffer occupancy, it also uses probabilistic marking. When the router is congested, the arriving packets are marked with a probability that increases with the average buffer occupancy. The main advantage of using probabilistic marking instead of marking all arriving packets is that flows will be marked in proportion of the number of packets that they transmit. If the router marks 10% of the arriving packets when congested, then a large flow that sends hundred packets per second will be marked 10 times while a flow that only sends one packet per second will not be marked. This probabilistic marking allows marking packets in proportion of their usage of the network resources.</p>
<p>If the router uses several queues served by a scheduler, the situation is different. If a large and a small flow are competing for bandwidth, the scheduler will already favor the small flow that is not using its fair share of the bandwidth. The queue for the small flow will be almost empty while the queue for the large flow will build up. On routers using such schedulers, a good way of marking the packets is to set a threshold on the occupancy of each queue and mark the packets that arrive in a particular queue as soon as its occupancy is above the configured threshold.</p>
</section>
<section id="modeling-tcp-congestion-control">
<h4>Modeling TCP congestion control<a class="headerlink" href="#modeling-tcp-congestion-control" title="Link to this heading">#</a></h4>
<p>Thanks to its congestion control scheme, TCP adapts its transmission rate to the losses that occur in the network. Intuitively, the TCP transmission rate decreases when the percentage of losses increases. Researchers have proposed detailed models that allow the prediction of the throughput of a TCP connection when losses occur <a class="reference internal" href="../bibliography.html#msmo1997" id="id67"><span>[MSMO1997]</span></a> . To have some intuition about the factors that affect the performance of TCP, let us consider a very simple model. Its assumptions are not completely realistic, but it gives us good intuition without requiring complex mathematics.</p>
<p>This model considers a hypothetical TCP connection that suffers from equally spaced segment losses. If <span class="math notranslate nohighlight">\(p\)</span> is the segment loss ratio, then the TCP connection successfully transfers <span class="math notranslate nohighlight">\(\frac{1}{p}-1\)</span> segments and the next segment is lost. If we ignore the slow-start at the beginning of the connection, TCP in this environment is always in congestion avoidance as there are only isolated losses that can be recovered by using fast retransmit. The evolution of the congestion window is thus as shown in the figure below. Note that the <cite>x-axis</cite> of this figure represents time measured in units of one round-trip-time, which is supposed to be constant in the model, and the <cite>y-axis</cite> represents the size of the congestion window measured in MSS-sized segments.</p>
<figure class="align-center" id="id112">
<a class="reference internal image-reference" href="../_images/tcp-congestion-regular.png"><img alt="../_images/tcp-congestion-regular.png" src="../Images/aba5ecafe63f482ae07b7a280f8f3275.png" style="width: 419.29999999999995px; height: 128.79999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-regular.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 226 </span><span class="caption-text">Evolution of the congestion window with regular losses</span><a class="headerlink" href="#id112" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As the losses are equally spaced, the congestion window always starts at some value (<span class="math notranslate nohighlight">\(\frac{W}{2}\)</span>), and is incremented by one MSS every round-trip-time until it reaches twice this value (<cite>W</cite>). At this point, a segment is retransmitted and the cycle starts again. If the congestion window is measured in MSS-sized segments, a cycle lasts <span class="math notranslate nohighlight">\(\frac{W}{2}\)</span> round-trip-times. The bandwidth of the TCP connection is the number of bytes that have been transmitted during a given period of time. During a cycle, the number of segments that are sent on the TCP connection is equal to the area of the yellow trapeze in the figure. Its area is thus :</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(area=(\frac{W}{2})^2 + \frac{1}{2} \times (\frac{W}{2})^2 = \frac{3 \times W^2}{8}\)</span></p>
</div></blockquote>
<p>However, given the regular losses that we consider, the number of segments that are sent between two losses (i.e. during a cycle) is by definition equal to <span class="math notranslate nohighlight">\(\frac{1}{p}\)</span>. Thus, <span class="math notranslate nohighlight">\(W=\sqrt{\frac{8}{3 \times p}}=\frac{k}{\sqrt{p}}\)</span>. The throughput (in bytes per second) of the TCP connection is equal to the number of segments transmitted divided by the duration of the cycle :</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Throughput=\frac{area \times MSS}{time} = \frac{ \frac{3 \times W^2}{8}}{\frac{W}{2} \times rtt}\)</span>
or, after having eliminated <cite>W</cite>, <span class="math notranslate nohighlight">\(Throughput=\sqrt{\frac{3}{2}} \times \frac{MSS}{rtt \times \sqrt{p}}\)</span></p>
</div></blockquote>
<p>More detailed models and the analysis of simulations have shown that a first order model of the TCP throughput when losses occur was <span class="math notranslate nohighlight">\(Throughput \approx \frac{k \times MSS}{rtt \times \sqrt{p}}\)</span>. This is an important result which shows that :</p>
<blockquote>
<div><ul class="simple">
<li><p>TCP connections with a small round-trip-time can achieve a higher throughput than TCP connections having a longer round-trip-time when losses occur. This implies that the TCP congestion control scheme is not completely fair since it favors the connections that have the shorter round-trip-times.</p></li>
<li><p>TCP connections that use a large MSS can achieve a higher throughput that the TCP connections that use a shorter MSS. This creates another source of unfairness between TCP connections. However, it should be noted that today most hosts are using almost the same MSS, roughly 1460 bytes.</p></li>
</ul>
</div></blockquote>
<p>In general, the maximum throughput that can be achieved by a TCP connection depends on its maximum window size and the round-trip-time if there are no losses. If there are losses, it depends on the MSS, the round-trip-time and the loss ratio.</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Throughput&lt;\min(\frac{window}{rtt},\frac{k \times MSS}{rtt \times \sqrt{p}})\)</span></p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The TCP congestion control zoo</p>
<p>The first TCP congestion control scheme was proposed by <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id68"><span>[Jacobson1988]</span></a>. In addition to writing the scientific paper, <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> also implemented the slow-start and congestion avoidance schemes in release 4.3 <cite>Tahoe</cite> of the BSD Unix distributed by the University of Berkeley. Later, he improved the congestion control by adding the fast retransmit and the fast recovery mechanisms in the <cite>Reno</cite> release of 4.3 BSD Unix. Since then, many researchers have proposed, simulated and implemented modifications to the TCP congestion control scheme. Some of these modifications are still used today, e.g. :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>NewReno</cite> (<span class="target" id="index-51"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3782.html"><strong>RFC 3782</strong></a>), which was proposed as an improvement of the fast recovery mechanism in the <cite>Reno</cite> implementation.</p></li>
<li><p><cite>TCP Vegas</cite>, which uses changes in the round-trip-time to estimate congestion in order to avoid it <a class="reference internal" href="../bibliography.html#bop1994" id="id69"><span>[BOP1994]</span></a>. This is one of the examples of the delay-based congestion control algorithms. A Vegas sender continuously measures the evolution of the round-trip-time and slows down when the round-trip-time increases significantly. This enables Vegas to prevent congestion when used alone. Unfortunately, if Vegas senders compete with more aggressive TCP congestion control schemes that only react to losses, Vegas senders may have difficulties to use their fair share of the available bandwidth.</p></li>
<li><p><cite>CUBIC</cite>, which was designed for high bandwidth links and is the default congestion control scheme in Linux since the Linux 2.6.19 kernel <a class="reference internal" href="../bibliography.html#hrx2008" id="id70"><span>[HRX2008]</span></a>. It is now used by several operating systems and is becoming the default congestion control scheme <span class="target" id="index-52"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc8312.html"><strong>RFC 8312</strong></a>. A key difference between CUBIC and the TCP congestion control scheme described in this chapter is that CUBIC is much more aggressive when probing the network. Instead of relying on additive increase after a fast recovery, a CUBIC sender adjusts its congestion by using a cubic function. Thanks to this function, the congestion windows grows faster. This is particularly important in high-bandwidth delay networks.</p></li>
<li><p><cite>BBR</cite>, which is being developed by Google researchers and is included in recent Linux kernels <a class="reference internal" href="../bibliography.html#ccg-2016" id="id71"><span>[CCG+2016]</span></a>. BBR periodically estimates the available bandwidth and the round-trip-times. To adapt to changes in network conditions, BBR regularly tries to send at 1.25 times the current bandwidth. This enables BBR senders to probe the network, but can also cause large amount of losses. Recent scientific articles indicate that BBR is unfair to other congestion control schemes in specific conditions <a class="reference internal" href="../bibliography.html#wmss2019" id="id72"><span>[WMSS2019]</span></a>.</p></li>
</ul>
</div></blockquote>
<p>A wide range of congestion control schemes have been proposed in the scientific literature and several of them have been widely deployed. A detailed comparison of these congestion control schemes is outside the scope of this chapter. A recent survey paper describing many of the implemented TCP congestion control schemes may be found in <a class="reference internal" href="../bibliography.html#tku2019" id="id73"><span>[TKU2019]</span></a>.</p>
</div>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fbufferbloat" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>There are still some vendors that try to put as many buffers as possible on their routers. A recent example is the buffer bloat problem that plagues some low-end Internet routers <a class="reference internal" href="../bibliography.html#gn2011" id="id74"><span>[GN2011]</span></a>.</p>
</aside>
<aside class="footnote brackets" id="fpps" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>Some examples of the performance of various types of commercial networks nodes (routers and switches) may be found in <a class="reference external" href="http://www.cisco.com/web/partners/downloads/765/tools/quickreference/routerperformance.pdf">http://www.cisco.com/web/partners/downloads/765/tools/quickreference/routerperformance.pdf</a> and <a class="reference external" href="http://www.cisco.com/web/partners/downloads/765/tools/quickreference/switchperformance.pdf">http://www.cisco.com/web/partners/downloads/765/tools/quickreference/switchperformance.pdf</a></p>
</aside>
<aside class="footnote brackets" id="fadjust" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>Some networking technologies allow to adjust dynamically the bandwidth of links. For example, some devices can reduce their bandwidth to preserve energy. We ignore these technologies in this basic course and assume that all links used inside the network have a fixed bandwidth.</p>
</aside>
<aside class="footnote brackets" id="fslottime" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">4</a><span class="fn-bracket">]</span></span>
<p>This name should not be confused with the duration of a transmission slot in slotted ALOHA. In CSMA/CD networks, the slot time is the time during which a collision can occur at the beginning of the transmission of a frame. In slotted ALOHA, the duration of a slot is the transmission time of an entire fixed-size frame.</p>
</aside>
<aside class="footnote brackets" id="fcredit" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id42">5</a><span class="fn-bracket">]</span></span>
<p>In this section, we focus on congestion control mechanisms that regulate the transmission rate of the hosts. Other types of mechanisms have been proposed in the literature. For example, <cite>credit-based</cite> flow-control has been proposed to avoid congestion in ATM networks <a class="reference internal" href="../bibliography.html#kr1995" id="id75"><span>[KR1995]</span></a>. With a credit-based mechanism, hosts can only send packets once they have received credits from the routers and the credits depend on the occupancy of the router’s buffers.</p>
</aside>
<aside class="footnote brackets" id="fflowslink" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id45">6</a><span class="fn-bracket">]</span></span>
<p>For example, the measurements performed in the Sprint network in 2004 reported more than 10k active TCP connections on a link, see <a class="reference external" href="https://research.sprintlabs.com/packstat/packetoverview.php">https://research.sprintlabs.com/packstat/packetoverview.php</a>. More recent information about backbone links may be obtained from <a class="reference external" href="https://www.caida.org">caida</a> ‘s real-time measurements, see e.g.  <a class="reference external" href="http://www.caida.org/data/realtime/passive/">http://www.caida.org/data/realtime/passive/</a></p>
</aside>
</aside>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fwrap" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id57">7</a><span class="fn-bracket">]</span></span>
<p>In this pseudo-code, we assume that TCP uses unlimited sequence and acknowledgment numbers. Furthermore, we do not detail how the <cite>cwnd</cite> is adjusted after the retransmission of the lost segment by fast retransmit. Additional details may be found in <span class="target" id="index-53"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc5681.html"><strong>RFC 5681</strong></a>.</p>
</aside>
<aside class="footnote brackets" id="fprivate" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id63">8</a><span class="fn-bracket">]</span></span>
<p>In enterprise networks or datacenters, the situation is different since a single company typically controls all the sources and all the routers. In such networks it is possible to ensure that all hosts and routers have been upgraded before turning on ECN on the routers.</p>
</aside>
<aside class="footnote brackets" id="fecnnonce" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id64">9</a><span class="fn-bracket">]</span></span>
<p>With the ECT bit, the deployment issue with ECN is solved provided that all sources cooperate. If some sources do not support ECN but still set the ECT bit in the packets that they sent, they will have an unfair advantage over the sources that correctly react to packet markings. Several solutions have been proposed to deal with this problem <span class="target" id="index-54"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3540.html"><strong>RFC 3540</strong></a>, but they are outside the scope of this book.</p>
</aside>
<aside class="footnote brackets" id="fslot" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id65">10</a><span class="fn-bracket">]</span></span>
<p>The buffers of a router can be implemented as variable or fixed-length slots. If the router uses variable length slots to store the queued packets, then the occupancy is usually measured in bytes. Some routers have use fixed-length slots with each slot large enough to store a maximum-length packet. In this case, the buffer occupancy is measured in packets.</p>
</aside>
</aside>
</section>
&#13;

<h4>Controlling congestion without losing data<a class="headerlink" href="#controlling-congestion-without-losing-data" title="Link to this heading">#</a></h4>
<p>In today’s Internet, congestion is controlled by regularly sending packets at a higher rate than the network capacity. These packets fill the buffers of the routers and are eventually discarded. But shortly after, TCP senders retransmit packets containing exactly the same data. This is potentially a waste of resources since these successive retransmissions consume resources upstream of the router that discards the packets. Packet losses are not the only signal to detect congestion inside the network. An alternative is to allow routers to explicitly indicate their current level of congestion when forwarding packets. This approach was proposed in the late 1980s <a class="reference internal" href="../bibliography.html#rj1995" id="id60"><span>[RJ1995]</span></a> and used in some networks. Unfortunately, it took almost a decade before the Internet community agreed to consider this approach. In the mean time, a large number of TCP implementations and routers were deployed on the Internet.</p>
<p>As explained earlier, Explicit Congestion Notification <span class="target" id="index-47"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> improves the detection of congestion by allowing routers to explicitly mark packets when they are lightly congested. In theory, a single bit in the packet header <a class="reference internal" href="../bibliography.html#rj1995" id="id61"><span>[RJ1995]</span></a> is sufficient to support this congestion control scheme. When a host receives a marked packet, it returns the congestion information to the source that adapts its transmission rate accordingly. Although the idea is relatively simple, deploying it on the entire Internet has proven to be challenging <a class="reference internal" href="../bibliography.html#knt2013" id="id62"><span>[KNT2013]</span></a>. It is interesting to analyze the different factors that have hindered the deployment of this technique.</p>
<p>The first difficulty in adding Explicit Congestion Notification (ECN) in TCP/IP network was to modify the format of the network packet and transport segment headers to carry the required information. In the network layer, one bit was required to allow the routers to mark the packets they forward during congestion periods. In the IP network layer, this bit is called the <cite>Congestion Experienced</cite> (<cite>CE</cite>) bit and is part of the packet header. However, using a single bit to mark packets is not sufficient. Consider a simple scenario with two sources, one congested router and one destination. Assume that the first sender and the destination support ECN, but not the second sender. If the router is congested it will mark packets from both senders. The first sender will react to the packet markings by reducing its transmission rate. However since the second sender does not support ECN, it will not react to the markings. Furthermore, this sender could continue to increase its transmission rate, which would lead to more packets being marked and the first source would decrease again its transmission rate, … In the end, the sources that implement ECN are penalized compared to the sources that do not implement it. This unfairness issue is a major hurdle to widely deploy ECN on the public Internet <a class="footnote-reference brackets" href="#fprivate" id="id63" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>. The solution proposed in <span class="target" id="index-48"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> to deal with this problem is to use a second bit in the network packet header. This bit, called the <cite>ECN-capable transport</cite> (ECT) bit, indicates whether the packet contains a segment produced by a transport protocol that supports ECN or not. Transport protocols that support ECN set the ECT bit in all packets. When a router is congested, it first verifies whether the ECT bit is set. In this case, the CE bit of the packet is set to indicate congestion. Otherwise, the packet is discarded. This eases the deployment of ECN <a class="footnote-reference brackets" href="#fecnnonce" id="id64" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>.</p>
<p>The second difficulty is how to allow the receiver to inform the sender of the reception of network packets marked with the <cite>CE</cite> bit. In reliable transport protocols like TCP and SCTP, the acknowledgments can be used to provide this feedback. For TCP, two options were possible : change some bits in the TCP segment header or define a new TCP option to carry this information. The designers of ECN opted for reusing spare bits in the TCP header. More precisely, two TCP flags have been added in the TCP header to support ECN. The <cite>ECN-Echo</cite> (ECE) is set in the acknowledgments when the <cite>CE</cite> was set in packets received on the forward path.</p>
<figure class="align-default" id="id111">
<a class="reference internal image-reference" href="../_images/tcp-enc.svg"><img alt="../_images/tcp-enc.svg" src="../Images/0b92b95f1c0dadde63905a28574e87cb.png" style="width: 664.8px; height: 115.19999999999999px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-enc.svg"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 225 </span><span class="caption-text">The TCP flags</span><a class="headerlink" href="#id111" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The third difficulty is to allow an ECN-capable sender to detect whether the remote host also supports ECN. This is a classical negotiation of extensions to a transport protocol. In TCP, this could have been solved by defining a new TCP option used during the three-way handshake. To avoid wasting space in the TCP options, the designers of ECN opted in <span class="target" id="index-49"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> for using the <cite>ECN-Echo</cite> and <cite>CWR</cite> bits in the TCP header to perform this negotiation. In the end, the result is the same with fewer bits exchanged.</p>
<p>Thanks to the <cite>ECT</cite>, <cite>CE</cite> and <cite>ECE</cite>, routers can mark packets during congestion and receivers can return the congestion information back to the TCP senders. However, these three bits are not sufficient to allow a server to reliably send the <cite>ECE</cite> bit to a TCP sender. TCP acknowledgments are not sent reliably. A TCP acknowledgment always contains the next expected sequence number. Since TCP acknowledgments are cumulative, the loss of one acknowledgment is recovered by the correct reception of a subsequent acknowledgment.</p>
<p>If TCP acknowledgments are overloaded to carry the <cite>ECE</cite> bit, the situation is different. Consider the example shown in the figure below. A client sends packets to a server through a router. In the example below, the first packet is marked. The server returns an acknowledgment with the <cite>ECE</cite> bit set. Unfortunately, this acknowledgment is lost and never reaches the client. Shortly after, the server sends a data segment that also carries a cumulative acknowledgment. This acknowledgment confirms the reception of the data to the client, but it did not receive the congestion information through the <cite>ECE</cite> bit.</p>
<blockquote>
<div><p class="mscgen">
<img src="../Images/d2ed1589241adeb9e699082587d1916d.png" alt="msc {&#10;client [label=&quot;client&quot;, linecolour=black],&#10;router [label=&quot;router&quot;, linecolour=black],&#10;server [label=&quot;server&quot;, linecolour=black];&#10;&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;server=&gt;router [ label = &quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;router -x client [label=&quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;|||;&#10;server=&gt;router [ label = &quot;data[seq=x,ack=2,ECE=0,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;client [ label = &quot;data[seq=x,ack=2,ECE=0,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client-&gt;server [linecolour=white];&#10;}" usemap="#700c52eed1dd99ea5abd5216ffd2e044e6fee931" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/mscgen-700c52eed1dd99ea5abd5216ffd2e044e6fee931.png"/>
<map id="700c52eed1dd99ea5abd5216ffd2e044e6fee931" name="700c52eed1dd99ea5abd5216ffd2e044e6fee931"/></p>
</div></blockquote>
<p>To solve this problem, <span class="target" id="index-50"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3168.html"><strong>RFC 3168</strong></a> uses an additional bit in the TCP header : the <cite>Congestion Window Reduced</cite> (CWR) bit.</p>
<blockquote>
<div><p class="mscgen">
<img src="../Images/36533b7c4d47ba50cac29ebf6ebcc1f1.png" alt="msc {&#10;client [label=&quot;client&quot;, linecolour=black],&#10;router [label=&quot;router&quot;, linecolour=black],&#10;server [label=&quot;server&quot;, linecolour=black];&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;server=&gt;router [ label = &quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;router -x client [label=&quot;ack=2,ECE=1&quot;, arcskip=&quot;1&quot; ];&#10;|||;&#10;server=&gt;router [ label = &quot;data[seq=x,ack=2,ECE=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;client [ label = &quot;data[seq=x,ack=2,ECE=1,ECT=1,CE=0]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client=&gt;router [ label = &quot;data[seq=1,ECT=1,CE=0,CWR=1]&quot;, arcskip=&quot;1&quot; ];&#10;router=&gt;server [ label = &quot;data[seq=1,ECT=1,CE=1,CWR=1]&quot;, arcskip=&quot;1&quot;];&#10;|||;&#10;client-&gt;server [linecolour=white];&#10;}" usemap="#d60c580a7379dbdd26f90fd2eead54f832ee6ad6" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/mscgen-d60c580a7379dbdd26f90fd2eead54f832ee6ad6.png"/>
<map id="d60c580a7379dbdd26f90fd2eead54f832ee6ad6" name="d60c580a7379dbdd26f90fd2eead54f832ee6ad6"/></p>
</div></blockquote>
<p>The <cite>CWR</cite> bit of the TCP header provides some form of acknowledgment for the <cite>ECE</cite> bit. When a TCP receiver detects a packet marked with the <cite>CE</cite> bit, it sets the <cite>ECE</cite> bit in all segments that it returns to the sender. Upon reception of an acknowledgment with the <cite>ECE</cite> bit set, the sender reduces its congestion window to reflect a mild congestion and sets the <cite>CWR</cite> bit. This bit remains set as long as the segments received contained the <cite>ECE</cite> bit set. A sender should only react once per round-trip-time to marked packets.</p>
<p>The last point that needs to be discussed about Explicit Congestion Notification is the algorithm that is used by routers to detect congestion. On a router, congestion manifests itself by the number of packets that are stored inside the router buffers. As explained earlier, we need to distinguish between two types of routers :</p>
<blockquote>
<div><ul class="simple">
<li><p>routers that have a single FIFO queue</p></li>
<li><p>routers that have several queues served by a round-robin scheduler</p></li>
</ul>
</div></blockquote>
<p>Routers that use a single queue measure their buffer occupancy as the number of bytes of packets stored in the queue <a class="footnote-reference brackets" href="#fslot" id="id65" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. A first method to detect congestion is to measure the instantaneous buffer occupancy and consider the router to be congested as soon as this occupancy is above a threshold. Typical values of the threshold could be 40% of the total buffer. Measuring the instantaneous buffer occupancy is simple since it only requires one counter. However, this value is fragile from a control viewpoint since it changes frequently. A better solution is to measure the <em>average</em> buffer occupancy and consider the router to be congested when this average occupancy is too high. Random Early Detection (RED) <a class="reference internal" href="../bibliography.html#fj1993" id="id66"><span>[FJ1993]</span></a> is an algorithm that was designed to support Explicit Congestion Notification. In addition to measuring the average buffer occupancy, it also uses probabilistic marking. When the router is congested, the arriving packets are marked with a probability that increases with the average buffer occupancy. The main advantage of using probabilistic marking instead of marking all arriving packets is that flows will be marked in proportion of the number of packets that they transmit. If the router marks 10% of the arriving packets when congested, then a large flow that sends hundred packets per second will be marked 10 times while a flow that only sends one packet per second will not be marked. This probabilistic marking allows marking packets in proportion of their usage of the network resources.</p>
<p>If the router uses several queues served by a scheduler, the situation is different. If a large and a small flow are competing for bandwidth, the scheduler will already favor the small flow that is not using its fair share of the bandwidth. The queue for the small flow will be almost empty while the queue for the large flow will build up. On routers using such schedulers, a good way of marking the packets is to set a threshold on the occupancy of each queue and mark the packets that arrive in a particular queue as soon as its occupancy is above the configured threshold.</p>
&#13;

<h4>Modeling TCP congestion control<a class="headerlink" href="#modeling-tcp-congestion-control" title="Link to this heading">#</a></h4>
<p>Thanks to its congestion control scheme, TCP adapts its transmission rate to the losses that occur in the network. Intuitively, the TCP transmission rate decreases when the percentage of losses increases. Researchers have proposed detailed models that allow the prediction of the throughput of a TCP connection when losses occur <a class="reference internal" href="../bibliography.html#msmo1997" id="id67"><span>[MSMO1997]</span></a> . To have some intuition about the factors that affect the performance of TCP, let us consider a very simple model. Its assumptions are not completely realistic, but it gives us good intuition without requiring complex mathematics.</p>
<p>This model considers a hypothetical TCP connection that suffers from equally spaced segment losses. If <span class="math notranslate nohighlight">\(p\)</span> is the segment loss ratio, then the TCP connection successfully transfers <span class="math notranslate nohighlight">\(\frac{1}{p}-1\)</span> segments and the next segment is lost. If we ignore the slow-start at the beginning of the connection, TCP in this environment is always in congestion avoidance as there are only isolated losses that can be recovered by using fast retransmit. The evolution of the congestion window is thus as shown in the figure below. Note that the <cite>x-axis</cite> of this figure represents time measured in units of one round-trip-time, which is supposed to be constant in the model, and the <cite>y-axis</cite> represents the size of the congestion window measured in MSS-sized segments.</p>
<figure class="align-center" id="id112">
<a class="reference internal image-reference" href="../_images/tcp-congestion-regular.png"><img alt="../_images/tcp-congestion-regular.png" src="../Images/aba5ecafe63f482ae07b7a280f8f3275.png" style="width: 419.29999999999995px; height: 128.79999999999998px;" data-original-src="https://4ed.computer-networking.info/syllabus/default/_images/tcp-congestion-regular.png"/>
</a>
<figcaption>
<p><span class="caption-number">Fig. 226 </span><span class="caption-text">Evolution of the congestion window with regular losses</span><a class="headerlink" href="#id112" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As the losses are equally spaced, the congestion window always starts at some value (<span class="math notranslate nohighlight">\(\frac{W}{2}\)</span>), and is incremented by one MSS every round-trip-time until it reaches twice this value (<cite>W</cite>). At this point, a segment is retransmitted and the cycle starts again. If the congestion window is measured in MSS-sized segments, a cycle lasts <span class="math notranslate nohighlight">\(\frac{W}{2}\)</span> round-trip-times. The bandwidth of the TCP connection is the number of bytes that have been transmitted during a given period of time. During a cycle, the number of segments that are sent on the TCP connection is equal to the area of the yellow trapeze in the figure. Its area is thus :</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(area=(\frac{W}{2})^2 + \frac{1}{2} \times (\frac{W}{2})^2 = \frac{3 \times W^2}{8}\)</span></p>
</div></blockquote>
<p>However, given the regular losses that we consider, the number of segments that are sent between two losses (i.e. during a cycle) is by definition equal to <span class="math notranslate nohighlight">\(\frac{1}{p}\)</span>. Thus, <span class="math notranslate nohighlight">\(W=\sqrt{\frac{8}{3 \times p}}=\frac{k}{\sqrt{p}}\)</span>. The throughput (in bytes per second) of the TCP connection is equal to the number of segments transmitted divided by the duration of the cycle :</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Throughput=\frac{area \times MSS}{time} = \frac{ \frac{3 \times W^2}{8}}{\frac{W}{2} \times rtt}\)</span>
or, after having eliminated <cite>W</cite>, <span class="math notranslate nohighlight">\(Throughput=\sqrt{\frac{3}{2}} \times \frac{MSS}{rtt \times \sqrt{p}}\)</span></p>
</div></blockquote>
<p>More detailed models and the analysis of simulations have shown that a first order model of the TCP throughput when losses occur was <span class="math notranslate nohighlight">\(Throughput \approx \frac{k \times MSS}{rtt \times \sqrt{p}}\)</span>. This is an important result which shows that :</p>
<blockquote>
<div><ul class="simple">
<li><p>TCP connections with a small round-trip-time can achieve a higher throughput than TCP connections having a longer round-trip-time when losses occur. This implies that the TCP congestion control scheme is not completely fair since it favors the connections that have the shorter round-trip-times.</p></li>
<li><p>TCP connections that use a large MSS can achieve a higher throughput that the TCP connections that use a shorter MSS. This creates another source of unfairness between TCP connections. However, it should be noted that today most hosts are using almost the same MSS, roughly 1460 bytes.</p></li>
</ul>
</div></blockquote>
<p>In general, the maximum throughput that can be achieved by a TCP connection depends on its maximum window size and the round-trip-time if there are no losses. If there are losses, it depends on the MSS, the round-trip-time and the loss ratio.</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Throughput&lt;\min(\frac{window}{rtt},\frac{k \times MSS}{rtt \times \sqrt{p}})\)</span></p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The TCP congestion control zoo</p>
<p>The first TCP congestion control scheme was proposed by <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> in <a class="reference internal" href="../bibliography.html#jacobson1988" id="id68"><span>[Jacobson1988]</span></a>. In addition to writing the scientific paper, <a class="reference external" href="https://en.wikipedia.org/wiki/Van_Jacobson">Van Jacobson</a> also implemented the slow-start and congestion avoidance schemes in release 4.3 <cite>Tahoe</cite> of the BSD Unix distributed by the University of Berkeley. Later, he improved the congestion control by adding the fast retransmit and the fast recovery mechanisms in the <cite>Reno</cite> release of 4.3 BSD Unix. Since then, many researchers have proposed, simulated and implemented modifications to the TCP congestion control scheme. Some of these modifications are still used today, e.g. :</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>NewReno</cite> (<span class="target" id="index-51"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3782.html"><strong>RFC 3782</strong></a>), which was proposed as an improvement of the fast recovery mechanism in the <cite>Reno</cite> implementation.</p></li>
<li><p><cite>TCP Vegas</cite>, which uses changes in the round-trip-time to estimate congestion in order to avoid it <a class="reference internal" href="../bibliography.html#bop1994" id="id69"><span>[BOP1994]</span></a>. This is one of the examples of the delay-based congestion control algorithms. A Vegas sender continuously measures the evolution of the round-trip-time and slows down when the round-trip-time increases significantly. This enables Vegas to prevent congestion when used alone. Unfortunately, if Vegas senders compete with more aggressive TCP congestion control schemes that only react to losses, Vegas senders may have difficulties to use their fair share of the available bandwidth.</p></li>
<li><p><cite>CUBIC</cite>, which was designed for high bandwidth links and is the default congestion control scheme in Linux since the Linux 2.6.19 kernel <a class="reference internal" href="../bibliography.html#hrx2008" id="id70"><span>[HRX2008]</span></a>. It is now used by several operating systems and is becoming the default congestion control scheme <span class="target" id="index-52"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc8312.html"><strong>RFC 8312</strong></a>. A key difference between CUBIC and the TCP congestion control scheme described in this chapter is that CUBIC is much more aggressive when probing the network. Instead of relying on additive increase after a fast recovery, a CUBIC sender adjusts its congestion by using a cubic function. Thanks to this function, the congestion windows grows faster. This is particularly important in high-bandwidth delay networks.</p></li>
<li><p><cite>BBR</cite>, which is being developed by Google researchers and is included in recent Linux kernels <a class="reference internal" href="../bibliography.html#ccg-2016" id="id71"><span>[CCG+2016]</span></a>. BBR periodically estimates the available bandwidth and the round-trip-times. To adapt to changes in network conditions, BBR regularly tries to send at 1.25 times the current bandwidth. This enables BBR senders to probe the network, but can also cause large amount of losses. Recent scientific articles indicate that BBR is unfair to other congestion control schemes in specific conditions <a class="reference internal" href="../bibliography.html#wmss2019" id="id72"><span>[WMSS2019]</span></a>.</p></li>
</ul>
</div></blockquote>
<p>A wide range of congestion control schemes have been proposed in the scientific literature and several of them have been widely deployed. A detailed comparison of these congestion control schemes is outside the scope of this chapter. A recent survey paper describing many of the implemented TCP congestion control schemes may be found in <a class="reference internal" href="../bibliography.html#tku2019" id="id73"><span>[TKU2019]</span></a>.</p>
</div>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fbufferbloat" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>There are still some vendors that try to put as many buffers as possible on their routers. A recent example is the buffer bloat problem that plagues some low-end Internet routers <a class="reference internal" href="../bibliography.html#gn2011" id="id74"><span>[GN2011]</span></a>.</p>
</aside>
<aside class="footnote brackets" id="fpps" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>Some examples of the performance of various types of commercial networks nodes (routers and switches) may be found in <a class="reference external" href="http://www.cisco.com/web/partners/downloads/765/tools/quickreference/routerperformance.pdf">http://www.cisco.com/web/partners/downloads/765/tools/quickreference/routerperformance.pdf</a> and <a class="reference external" href="http://www.cisco.com/web/partners/downloads/765/tools/quickreference/switchperformance.pdf">http://www.cisco.com/web/partners/downloads/765/tools/quickreference/switchperformance.pdf</a></p>
</aside>
<aside class="footnote brackets" id="fadjust" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>Some networking technologies allow to adjust dynamically the bandwidth of links. For example, some devices can reduce their bandwidth to preserve energy. We ignore these technologies in this basic course and assume that all links used inside the network have a fixed bandwidth.</p>
</aside>
<aside class="footnote brackets" id="fslottime" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">4</a><span class="fn-bracket">]</span></span>
<p>This name should not be confused with the duration of a transmission slot in slotted ALOHA. In CSMA/CD networks, the slot time is the time during which a collision can occur at the beginning of the transmission of a frame. In slotted ALOHA, the duration of a slot is the transmission time of an entire fixed-size frame.</p>
</aside>
<aside class="footnote brackets" id="fcredit" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id42">5</a><span class="fn-bracket">]</span></span>
<p>In this section, we focus on congestion control mechanisms that regulate the transmission rate of the hosts. Other types of mechanisms have been proposed in the literature. For example, <cite>credit-based</cite> flow-control has been proposed to avoid congestion in ATM networks <a class="reference internal" href="../bibliography.html#kr1995" id="id75"><span>[KR1995]</span></a>. With a credit-based mechanism, hosts can only send packets once they have received credits from the routers and the credits depend on the occupancy of the router’s buffers.</p>
</aside>
<aside class="footnote brackets" id="fflowslink" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id45">6</a><span class="fn-bracket">]</span></span>
<p>For example, the measurements performed in the Sprint network in 2004 reported more than 10k active TCP connections on a link, see <a class="reference external" href="https://research.sprintlabs.com/packstat/packetoverview.php">https://research.sprintlabs.com/packstat/packetoverview.php</a>. More recent information about backbone links may be obtained from <a class="reference external" href="https://www.caida.org">caida</a> ‘s real-time measurements, see e.g.  <a class="reference external" href="http://www.caida.org/data/realtime/passive/">http://www.caida.org/data/realtime/passive/</a></p>
</aside>
</aside>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fwrap" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id57">7</a><span class="fn-bracket">]</span></span>
<p>In this pseudo-code, we assume that TCP uses unlimited sequence and acknowledgment numbers. Furthermore, we do not detail how the <cite>cwnd</cite> is adjusted after the retransmission of the lost segment by fast retransmit. Additional details may be found in <span class="target" id="index-53"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc5681.html"><strong>RFC 5681</strong></a>.</p>
</aside>
<aside class="footnote brackets" id="fprivate" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id63">8</a><span class="fn-bracket">]</span></span>
<p>In enterprise networks or datacenters, the situation is different since a single company typically controls all the sources and all the routers. In such networks it is possible to ensure that all hosts and routers have been upgraded before turning on ECN on the routers.</p>
</aside>
<aside class="footnote brackets" id="fecnnonce" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id64">9</a><span class="fn-bracket">]</span></span>
<p>With the ECT bit, the deployment issue with ECN is solved provided that all sources cooperate. If some sources do not support ECN but still set the ECT bit in the packets that they sent, they will have an unfair advantage over the sources that correctly react to packet markings. Several solutions have been proposed to deal with this problem <span class="target" id="index-54"/><a class="rfc reference external" href="https://datatracker.ietf.org/doc/html/rfc3540.html"><strong>RFC 3540</strong></a>, but they are outside the scope of this book.</p>
</aside>
<aside class="footnote brackets" id="fslot" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id65">10</a><span class="fn-bracket">]</span></span>
<p>The buffers of a router can be implemented as variable or fixed-length slots. If the router uses variable length slots to store the queued packets, then the occupancy is usually measured in bytes. Some routers have use fixed-length slots with each slot large enough to store a maximum-length packet. In this case, the buffer occupancy is measured in packets.</p>
</aside>
</aside>
    
</body>
</html>